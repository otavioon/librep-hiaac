{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f1fa0e-165b-4e4f-b7a4-86eb02c6a790",
   "metadata": {},
   "source": [
    "# Testing Loaders and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1957de22-0cb6-4d3f-a216-0816b802ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19d0f05a-e2fb-43c4-a0dc-f90ff5ae207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librep.datasets.har.loaders import (\n",
    "    KuHarResampledView20HZ,\n",
    "    MotionSenseResampledView20HZ,\n",
    "    CHARMUnbalancedView,\n",
    "    WISDMInterpolatedUnbalancedView,\n",
    "    UCIHARUnbalancedView,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cfaa4e-3d5f-4ebb-bd55-bc6980374ff7",
   "metadata": {},
   "source": [
    "## KuHar Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134c35c5-f7ce-45b0-ae29-8b8128796eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Resampled to 20Hz KuHar View\n",
       "\n",
       "This view contains train, validation and test subsets in the following proportions:\n",
       "- Train: 70% of samples\n",
       "- Validation: 10% of samples\n",
       "- Test: 20% of samples\n",
       "\n",
       "After splits, the datasets were balanced in relation to the activity code column, that is, each subset have the same number of activitiy samples.\n",
       "\n",
       "## Activities:\n",
       "- Stand: 0 (185 train, 6 validation, 21 test)\n",
       "- Sit: 1 (185 train, 6 validation, 21 test)\n",
       "- Talk-sit: 2 (185 train, 6 validation, 21 test)\n",
       "- Talk-stand: 3 (185 train, 6 validation, 21 test)\n",
       "- Stand-sit: 4 (185 train, 6 validation, 21 test)\n",
       "- Lay: 5 (185 train, 6 validation, 21 test)\n",
       "- Lay-stand: 6 (185 train, 6 validation, 21 test)\n",
       "- Pick: 7 (185 train, 6 validation, 21 test)\n",
       "- Jump: 8 (185 train, 6 validation, 21 test)\n",
       "- Push-up: 9 (185 train, 6 validation, 21 test)\n",
       "- Sit-up: 10 (185 train, 6 validation, 21 test)\n",
       "- Walk: 11 (185 train, 6 validation, 21 test)\n",
       "- Walk-backwards: 12 (185 train, 6 validation, 21 test)\n",
       "- Walk-circle: 13 (185 train, 6 validation, 21 test)\n",
       "- Run: 14 (185 train, 6 validation, 21 test)\n",
       "- Stair-up: 15 (185 train, 6 validation, 21 test)\n",
       "- Stair-down: 16 (185 train, 6 validation, 21 test)\n",
       "- Table-tennis: 17 (185 train, 6 validation, 21 test)\n",
       "\n",
       "## Users\n",
       "- 62 users train dataset: 1003 (29 samples), 1004 (58 samples), 1005 (25 samples), 1008 (71 samples), 1011 (24 samples), 1013 (54 samples), 1014 (120 samples), 1015 (56 samples), 1016 (39 samples), 1017 (24 samples), 1018 (35 samples), 1020 (32 samples), 1021 (39 samples), 1022 (102 samples), 1023 (63 samples), 1024 (117 samples), 1025 (39 samples), 1026 (89 samples), 1027 (64 samples), 1029 (39 samples), 1031 (42 samples), 1032 (21 samples), 1033 (18 samples), 1034 (138 samples), 1035 (7 samples), 1037 (67 samples), 1038 (48 samples), 1039 (103 samples), 1040 (92 samples), 1041 (96 samples), 1042 (85 samples), 1043 (87 samples), 1046 (82 samples), 1047 (37 samples), 1048 (38 samples), 1049 (36 samples), 1051 (28 samples), 1053 (29 samples), 1054 (8 samples), 1055 (36 samples), 1058 (29 samples), 1060 (31 samples), 1061 (33 samples), 1063 (27 samples), 1064 (19 samples), 1067 (16 samples), 1068 (32 samples), 1069 (25 samples), 1070 (33 samples), 1073 (15 samples), 1074 (14 samples), 1075 (17 samples), 1076 (31 samples), 1078 (20 samples), 1079 (26 samples), 1081 (51 samples), 1083 (30 samples), 1084 (29 samples), 1085 (29 samples), 1087 (32 samples), 1090 (42 samples), 1101 (532 samples).\n",
       "- 9 users validation dataset: 1002 (58 samples), 1006 (5 samples), 1019 (6 samples), 1062 (6 samples), 1065 (3 samples), 1071 (13 samples), 1072 (1 samples), 1082 (10 samples), 1086 (6 samples).\n",
       "- 18 users test dataset: 1001 (12 samples), 1007 (19 samples), 1009 (8 samples), 1010 (6 samples), 1028 (10 samples), 1030 (29 samples), 1036 (45 samples), 1044 (66 samples), 1045 (58 samples), 1050 (10 samples), 1052 (14 samples), 1056 (22 samples), 1057 (10 samples), 1066 (15 samples), 1077 (23 samples), 1080 (10 samples), 1088 (10 samples), 1089 (11 samples).\n",
       "\n",
       "**NOTE**: Each subset contain samples from distinct users, that is, samples of one user belongs exclusivelly to one of three subsets.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = KuHarResampledView20HZ(\"../data/views/KuHar/resampled_view_20Hz\", download=False)\n",
    "loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b369d1dc-f946-4242-a17f-6195429cf9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=3438, features=360, no. window=6,\n",
       " PandasMultiModalDataset: samples=378, features=360, no. window=6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val, test = loader.load(concat_train_validation=True)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf4636-5819-431e-8936-a45930cff558",
   "metadata": {},
   "source": [
    "## MotionSense Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2071f3ff-2f42-49ba-8d9f-46ac67a155ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Resampled to 20Hz MotionSense View\n",
       "\n",
       "This view contains train, validation and test subsets in the following proportions:\n",
       "- Train: 70% of samples\n",
       "- Validation: 10% of samples\n",
       "- Test: 20% of samples\n",
       "\n",
       "After splits, the datasets were balanced in relation to the activity code column, that is, each subset have the same number of activitiy samples.\n",
       "\n",
       "## Activities:\n",
       "- dws: 0 (569 train, 101 validation, 170 test)\n",
       "- ups: 1 (569 train, 101 validation, 170 test)\n",
       "- sit: 2 (569 train, 101 validation, 170 test)\n",
       "- std: 3 (569 train, 101 validation, 170 test)\n",
       "- wlk: 4 (569 train, 101 validation, 170 test)\n",
       "- jog: 5 (569 train, 101 validation, 170 test)\n",
       "\n",
       "## Users\n",
       "- 16 users train dataset: 1 (218 samples), 2 (219 samples), 5 (185 samples), 6 (218 samples), 8 (233 samples), 9 (202 samples), 10 (218 samples), 11 (211 samples), 12 (197 samples), 13 (183 samples), 15 (208 samples), 16 (246 samples), 17 (209 samples), 21 (254 samples), 22 (200 samples), 23 (213 samples).\n",
       "- 3 users validation dataset: 4 (190 samples), 7 (211 samples), 20 (205 samples).\n",
       "- 5 users test dataset: 3 (222 samples), 14 (183 samples), 18 (223 samples), 19 (233 samples), 24 (159 samples).\n",
       "\n",
       "**NOTE**: Each subset contain samples from distinct users, that is, samples of one user belongs exclusivelly to one of three subsets.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = MotionSenseResampledView20HZ(\"../data/views/MotionSense/resampled_view_20Hz\", download=False)\n",
    "loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffcf566e-b491-43e7-8934-1a713ce1bd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=4020, features=360, no. window=6,\n",
       " PandasMultiModalDataset: samples=1020, features=360, no. window=6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val, test = loader.load(concat_train_validation=True)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96ba465-5077-4ec9-9db6-6cf5e7adcfa9",
   "metadata": {},
   "source": [
    "## CHARM Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab345dae-99a9-4eec-a381-168381277990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Unbalanced CHARM View (a.k.a. V0)\n",
       "\n",
       "This view contain only the train and test files for [CHARM dataset](https://zenodo.org/record/4642560) (70% samples train and 30% test).\n",
       "The dataset was sampled at 20Hz.\n",
       "\n",
       "## Activities:\n",
       "\n",
       "- 0: Sitting in a Chair  (258 train, 105 test)\n",
       "- 1: Sitting in a Couch. (260 train, 105 test)\n",
       "- 2: Standing (105 train, 36 test)\n",
       "- 3: Lying up (237 train, 85 test)\n",
       "- 4: Lying side (232 train, 87 test)\n",
       "- 5: Device on surface (155 train, 65 test)\n",
       "- 6: Walking (226 train, 64 test)\n",
       "- 7: Running (237 train, 84 test)\n",
       "- 8: Walking Upstairs (113 train, 33 test)\n",
       "- 9: Walking Downstairs (229 train, 83 test)\n",
       "\n",
       "## Users\n",
       "\n",
       "There are 30 users in total. Each sample is from a single user.\n",
       "\n",
       "- Samples from user 0-19 are in train file\n",
       "- Samples from user 21-30 are in test file\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = CHARMUnbalancedView(\"../data/views/CHARM/unbalanced_view_train_test-v1\", download=False)\n",
    "loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694c90ac-3814-4783-8d49-1ef116da3698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=2052, features=360, no. window=6,\n",
       " PandasMultiModalDataset: samples=747, features=360, no. window=6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val, test = loader.load(concat_train_validation=True)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fe7f6-d803-403a-ab22-0ec37c77d929",
   "metadata": {},
   "source": [
    "## WISDM Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "603a982a-5c27-479f-a900-b0c4429adb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Interpolated unbalanced WISDM View (a.k.a. V2)\n",
       "\n",
       "This view contain only the train and test files for [WISDM dataset](https://archive.ics.uci.edu/ml/datasets/WISDM+Smartphone+and+Smartwatch+Activity+and+Biometrics+Dataset) (70% samples train and 30% test).\n",
       "The dataset was sampled at 20Hz and interpolated using the cubic spline method due to non stable sampling.\n",
       "\n",
       "## Activities:\n",
       "\n",
       "0: Walking (2188 train, 886 test)\n",
       "1: Jogging (2070 train, 887 test)\n",
       "2: Stairs (2187 train, 827 test)\n",
       "3: Sitting (2189 train, 886 test)\n",
       "4: Standing (2189 train, 887 test)\n",
       "\n",
       "\n",
       "## Users\n",
       "\n",
       "There are 51 users in total. Each sample is from a single user.\n",
       "\n",
       "- Samples from user 1600-1635 are in train file\n",
       "- Samples from user 1631-1650 are in test file\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = WISDMInterpolatedUnbalancedView(\"../data/views/WISDM/interpolated_unbalanced_view_train_test-v1\", download=False)\n",
    "loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf1c249c-32e3-4ffd-8eab-442ab480e9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=10823, features=360, no. window=6,\n",
       " PandasMultiModalDataset: samples=4373, features=360, no. window=6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val, test = loader.load(concat_train_validation=True)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18fcb09-2b15-4a5a-8fb8-de9ccf8abe35",
   "metadata": {},
   "source": [
    "## UCI-HAR Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6244e5b0-57e4-4678-aa6a-15b11206a59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Unbalanced UCI-HAR View (a.k.a. V0)\n",
       "\n",
       "This view contain only the train and test files for [UCI-HAR dataset](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones#) (70% samples train and 30% test).\n",
       "The dataset was sampled at 50Hz.\n",
       "\n",
       "## Activities:\n",
       "\n",
       "1: Walking (506 train, 204 test)\n",
       "2: Walking Upstairs (439 train, 189 test)\n",
       "3: Walking Downstairs (395 train, 173 test)\n",
       "4: Sitting (544 train, 204 test)\n",
       "5: Standing (575 train, 227 test)\n",
       "6: Laying (590 train, 227 test)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = UCIHARUnbalancedView(\"../data/views/UCI-HAR/unbalanced_view_train_test-v1\", download=False)\n",
    "loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82cbb3d9-4a7a-4223-beaa-e38aaba05b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=3049, features=900, no. window=6,\n",
       " PandasMultiModalDataset: samples=1224, features=900, no. window=6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val, test = loader.load(concat_train_validation=True)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f3842-1d0a-4f5a-b7ed-08ceb8f792b8",
   "metadata": {},
   "source": [
    "# Performing Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4227ec0-ee7f-4e71-bb81-44c1282a8ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
