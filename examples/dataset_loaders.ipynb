{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea54f721-8136-400a-8a35-d517ec967d31",
   "metadata": {},
   "source": [
    "# Testing Dataset Loaders\n",
    "\n",
    "Example of loading the following datasets (classes):\n",
    "\n",
    "- KuHarResampledView20HZ\n",
    "- MotionSenseResampledView20HZ\n",
    "- CHARMUnbalancedView\n",
    "- WISDMInterpolatedUnbalancedView\n",
    "- UCIHARUnbalancedView\n",
    "\n",
    "To load the datasets, you must:\n",
    "\n",
    "- Wrap the dataset path arround one of the above classes (`root_dir` argument). You may want to download the dataset setting `download` argument to `True`\n",
    "- Use `load` function. It will create `PandasMultiModalDataset` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b1204d2-2b93-42b1-b5db-249bad888dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d6799d-9de9-4678-a9bf-a889005f6748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 18:32:28.717493: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-28 18:32:28.787333: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from librep.datasets.har.loaders import (\n",
    "    KuHarResampledView20HZ,\n",
    "    MotionSenseResampledView20HZ,\n",
    "    CHARMUnbalancedView,\n",
    "    WISDMInterpolatedUnbalancedView,\n",
    "    UCIHARUnbalancedView,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682955b-faf0-4d91-aeca-79762f6441f6",
   "metadata": {},
   "source": [
    "## KuHar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8898cfbc-4cef-4996-b5ae-0e1ed7670353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Balanced KuHar View Resampled to 20Hz\n",
       "\n",
       "This view contains train, validation and test subsets in the following proportions:\n",
       "- Train: 70% of samples\n",
       "- Validation: 10% of samples\n",
       "- Test: 20% of samples\n",
       "\n",
       "After splits, the datasets were balanced in relation to the activity code column, that is, each subset have the same number of activitiy samples.\n",
       "\n",
       "## Activities:\n",
       "- 0: Sit (185 train, 6 validation, 21 test)\n",
       "- 1: Stand (185 train, 6 validation, 21 test)\n",
       "- 2: Walk (185 train, 6 validation, 21 test)\n",
       "- 3: Stair-up (185 train, 6 validation, 21 test)\n",
       "- 4: Stair-down (185 train, 6 validation, 21 test)\n",
       "- 5: Run (185 train, 6 validation, 21 test)\n",
       "- 6: Talk-sit (185 train, 6 validation, 21 test)\n",
       "- 7: Talk-stand (185 train, 6 validation, 21 test)\n",
       "- 8: Stand-sit (185 train, 6 validation, 21 test)\n",
       "- 9: Lay (185 train, 6 validation, 21 test)\n",
       "- 10: Lay-stand (185 train, 6 validation, 21 test)\n",
       "- 11: Pick (185 train, 6 validation, 21 test)\n",
       "- 12: Jump (185 train, 6 validation, 21 test)\n",
       "- 13: Push-up (185 train, 6 validation, 21 test)\n",
       "- 14: Sit-up (185 train, 6 validation, 21 test)\n",
       "- 15: Walk-backwards (185 train, 6 validation, 21 test)\n",
       "- 16: Walk-circle (185 train, 6 validation, 21 test)\n",
       "- 17: Table-tennis (185 train, 6 validation, 21 test)\n",
       "\n",
       "## Users\n",
       "- 62 users train dataset: 1003 (29 samples), 1004 (58 samples), 1005 (25 samples), 1008 (71 samples), 1011 (24 samples), 1013 (54 samples), 1014 (120 samples), 1015 (56 samples), 1016 (39 samples), 1017 (24 samples), 1018 (35 samples), 1020 (32 samples), 1021 (39 samples), 1022 (102 samples), 1023 (63 samples), 1024 (117 samples), 1025 (39 samples), 1026 (89 samples), 1027 (64 samples), 1029 (39 samples), 1031 (42 samples), 1032 (21 samples), 1033 (18 samples), 1034 (138 samples), 1035 (7 samples), 1037 (67 samples), 1038 (48 samples), 1039 (103 samples), 1040 (92 samples), 1041 (96 samples), 1042 (85 samples), 1043 (87 samples), 1046 (82 samples), 1047 (37 samples), 1048 (38 samples), 1049 (36 samples), 1051 (28 samples), 1053 (29 samples), 1054 (8 samples), 1055 (36 samples), 1058 (29 samples), 1060 (31 samples), 1061 (33 samples), 1063 (27 samples), 1064 (19 samples), 1067 (16 samples), 1068 (32 samples), 1069 (25 samples), 1070 (33 samples), 1073 (15 samples), 1074 (14 samples), 1075 (17 samples), 1076 (31 samples), 1078 (20 samples), 1079 (26 samples), 1081 (51 samples), 1083 (30 samples), 1084 (29 samples), 1085 (29 samples), 1087 (32 samples), 1090 (42 samples), 1101 (532 samples).\n",
       "- 9 users validation dataset: 1002 (58 samples), 1006 (5 samples), 1019 (6 samples), 1062 (6 samples), 1065 (3 samples), 1071 (13 samples), 1072 (1 samples), 1082 (10 samples), 1086 (6 samples).\n",
       "- 18 users test dataset: 1001 (12 samples), 1007 (19 samples), 1009 (8 samples), 1010 (6 samples), 1028 (10 samples), 1030 (29 samples), 1036 (45 samples), 1044 (66 samples), 1045 (58 samples), 1050 (10 samples), 1052 (14 samples), 1056 (22 samples), 1057 (10 samples), 1066 (15 samples), 1077 (23 samples), 1080 (10 samples), 1088 (10 samples), 1089 (11 samples).\n",
       "\n",
       "**NOTE**: Each subset contain samples from distinct users, that is, samples of one user belongs exclusivelly to one of three subsets.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# KuHar Loader\n",
    "loader = KuHarResampledView20HZ(\n",
    "    \"../data/views/KuHar/resampled_view_20Hz\", download=False\n",
    ")\n",
    "\n",
    "# Print the readme (optional)\n",
    "loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dadff9bf-fc0b-46e1-b07b-8c4f28044c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=3438, features=360, no. window=6,\n",
       " PandasMultiModalDataset: samples=378, features=360, no. window=6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# If concat_train_validation is true, return a tuple (train+validation, test)\n",
    "train_val, test = loader.load(concat_train_validation=True)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315bb023-a286-4e13-9db1-6d2f88f34d72",
   "metadata": {},
   "source": [
    "## MotionSense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4064765f-567c-4559-b472-90c0269075b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Resampled to 20Hz MotionSense View\n",
       "\n",
       "This view contains train, validation and test subsets in the following proportions:\n",
       "- Train: 70% of samples\n",
       "- Validation: 10% of samples\n",
       "- Test: 20% of samples\n",
       "\n",
       "After splits, the datasets were balanced in relation to the activity code column, that is, each subset have the same number of activitiy samples.\n",
       "\n",
       "## Activities:\n",
       "- sit: 0 (569 train, 101 validation, 170 test)\n",
       "- std: 1 (569 train, 101 validation, 170 test)\n",
       "- wlk: 2 (569 train, 101 validation, 170 test)\n",
       "- ups: 3 (569 train, 101 validation, 170 test)\n",
       "- dws: 4 (569 train, 101 validation, 170 test)\n",
       "- jog: 5 (569 train, 101 validation, 170 test)\n",
       "\n",
       "## Users\n",
       "- 16 users train dataset: 1 (218 samples), 2 (219 samples), 5 (185 samples), 6 (218 samples), 8 (233 samples), 9 (202 samples), 10 (218 samples), 11 (211 samples), 12 (197 samples), 13 (183 samples), 15 (208 samples), 16 (246 samples), 17 (209 samples), 21 (254 samples), 22 (200 samples), 23 (213 samples).\n",
       "- 3 users validation dataset: 4 (190 samples), 7 (211 samples), 20 (205 samples).\n",
       "- 5 users test dataset: 3 (222 samples), 14 (183 samples), 18 (223 samples), 19 (233 samples), 24 (159 samples).\n",
       "\n",
       "**NOTE**: Each subset contain samples from distinct users, that is, samples of one user belongs exclusivelly to one of three subsets.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MotionSense Loader\n",
    "loader = MotionSenseResampledView20HZ(\n",
    "    \"../data/views/MotionSense/resampled_view_20Hz\", download=False\n",
    ")\n",
    "\n",
    "# Print the readme (optional)\n",
    "loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c101ae-623f-424e-b835-0aab8631f92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=4020, features=360, no. window=6,\n",
       " PandasMultiModalDataset: samples=1020, features=360, no. window=6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# If concat_train_validation is true, return a tuple (train+validation, test)\n",
    "train_val, test = loader.load(concat_train_validation=True)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd43785-5056-4015-824a-bae3e64a491a",
   "metadata": {},
   "source": [
    "## CHARM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a892e6cf-f646-4272-9bef-949c12eeabbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Unbalanced CHARM View (a.k.a. V0)\n",
       "\n",
       "This view contain only the train and test files for [CHARM dataset](https://zenodo.org/record/4642560) (70% samples train and 30% test).\n",
       "The dataset was sampled at 20Hz.\n",
       "\n",
       "## Activities:\n",
       "\n",
       "- 0: Sitting in a Chair  (258 train, 105 test)\n",
       "- 1: Sitting in a Couch. (260 train, 105 test)\n",
       "- 2: Standing (105 train, 36 test)\n",
       "- 3: Lying up (237 train, 85 test)\n",
       "- 4: Lying side (232 train, 87 test)\n",
       "- 5: Device on surface (155 train, 65 test)\n",
       "- 6: Walking (226 train, 64 test)\n",
       "- 7: Running (237 train, 84 test)\n",
       "- 8: Walking Upstairs (113 train, 33 test)\n",
       "- 9: Walking Downstairs (229 train, 83 test)\n",
       "\n",
       "## Users\n",
       "\n",
       "There are 30 users in total. Each sample is from a single user.\n",
       "\n",
       "- Samples from user 0-19 are in train file\n",
       "- Samples from user 21-30 are in test file\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CHARM Loader\n",
    "loader = CHARMUnbalancedView(\n",
    "    \"../data/views/CHARM/unbalanced_view_train_test-v1\", download=False\n",
    ")\n",
    "\n",
    "# Print the readme (optional)\n",
    "loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28810b09-a54b-4cfb-a459-132f85af8f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=2052, features=360, no. window=6,\n",
       " PandasMultiModalDataset: samples=747, features=360, no. window=6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# If concat_train_validation is true, return a tuple (train+validation, test)\n",
    "train_val, test = loader.load(concat_train_validation=True)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61fdde-4be5-48a7-bbe0-c0b70fc5d2c2",
   "metadata": {},
   "source": [
    "## WISDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bcb3962-ff94-4b78-b67c-83614efcff04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Interpolated unbalanced WISDM View (a.k.a. V2)\n",
       "\n",
       "This view contain only the train and test files for [WISDM dataset](https://archive.ics.uci.edu/ml/datasets/WISDM+Smartphone+and+Smartwatch+Activity+and+Biometrics+Dataset) (70% samples train and 30% test).\n",
       "The dataset was sampled at 20Hz and interpolated using the cubic spline method due to non stable sampling.\n",
       "\n",
       "## Activities:\n",
       "\n",
       "0: Walking (2188 train, 886 test)\n",
       "1: Jogging (2070 train, 887 test)\n",
       "2: Stairs (2187 train, 827 test)\n",
       "3: Sitting (2189 train, 886 test)\n",
       "4: Standing (2189 train, 887 test)\n",
       "\n",
       "\n",
       "## Users\n",
       "\n",
       "There are 51 users in total. Each sample is from a single user.\n",
       "\n",
       "- Samples from user 1600-1635 are in train file\n",
       "- Samples from user 1631-1650 are in test file\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WISDM Loader\n",
    "loader = WISDMInterpolatedUnbalancedView(\n",
    "    \"../data/views/WISDM/interpolated_unbalanced_view_train_test-v1\", download=False\n",
    ")\n",
    "\n",
    "# Print the readme (optional)\n",
    "loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "353d211f-5753-47f4-b107-7d5253d1c926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=10823, features=360, no. window=6,\n",
       " PandasMultiModalDataset: samples=4373, features=360, no. window=6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# If concat_train_validation is true, return a tuple (train+validation, test)\n",
    "train_val, test = loader.load(concat_train_validation=True)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7af93-2aef-43c5-8fe9-2a64d9a2f02f",
   "metadata": {},
   "source": [
    "## UCI-HAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a611aa11-6ef3-4453-bb4d-20908e0316c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Unbalanced UCI-HAR View (a.k.a. V0)\n",
       "\n",
       "This view contain only the train and test files for [UCI-HAR dataset](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones#) (70% samples train and 30% test).\n",
       "The dataset was sampled at 50Hz.\n",
       "\n",
       "## Activities:\n",
       "\n",
       "1: Walking (506 train, 204 test)\n",
       "2: Walking Upstairs (439 train, 189 test)\n",
       "3: Walking Downstairs (395 train, 173 test)\n",
       "4: Sitting (544 train, 204 test)\n",
       "5: Standing (575 train, 227 test)\n",
       "6: Laying (590 train, 227 test)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# UCI-HAR Loader\n",
    "loader = UCIHARUnbalancedView(\n",
    "    \"../data/views/UCI-HAR/unbalanced_view_train_test-v1\", download=False\n",
    ")\n",
    "\n",
    "# Print the readme (optional)\n",
    "loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02250e4f-d2d7-4a8c-8f60-ca359b35dfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=3049, features=900, no. window=6,\n",
       " PandasMultiModalDataset: samples=1224, features=900, no. window=6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# If concat_train_validation is true, return a tuple (train+validation, test)\n",
    "train_val, test = loader.load(concat_train_validation=True)\n",
    "train_val, test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
