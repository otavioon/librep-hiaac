{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46de2ac5-1944-44fc-9230-f3e33c1100fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29206a2e-4b50-42d5-9eeb-7c23a49e2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e4076acd-7480-4ba1-8ff7-9b7164f5a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from librep.transforms import TSNE\n",
    "from librep.transforms import UMAP\n",
    "from librep.datasets.multimodal import TransformMultiModalDataset, ArrayMultiModalDataset\n",
    "from librep.metrics.dimred_evaluator import DimensionalityReductionQualityReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cb6479e-7492-470c-a914-e78dd2b09bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "(train_x, train_y), (test_x, test_y) = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4366994e-7901-44f0-815d-be7975f1d553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_X_REORDERED (60000, 784)\n",
      "TEST_X_REORDERED (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_x_reordered = train_x.reshape((60000,-1))\n",
    "print('TRAIN_X_REORDERED', train_x_reordered.shape)\n",
    "\n",
    "test_x_reordered = test_x.reshape((10000,-1))\n",
    "print('TEST_X_REORDERED', test_x_reordered.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ba19d17-98fc-4692-99e9-d17c7503ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset_train = ArrayMultiModalDataset(X=train_x_reordered, y=train_y, window_slices=[(0, 28*28)], \n",
    "                                             window_names=[\"px\"])\n",
    "mnist_dataset_test = ArrayMultiModalDataset(X=test_x_reordered, y=test_y, window_slices=[(0, 28*28)], \n",
    "                                             window_names=[\"px\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a5984f3-50e4-4447-a2fb-394c44183a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform_tsne = TSNE()\n",
    "transformer = TransformMultiModalDataset(transforms=[transform_tsne])\n",
    "train_applied_tsne = transformer(mnist_dataset_train)\n",
    "test_applied_tsne = transformer(mnist_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cf6f4020-a41d-4c08-8d50-a4faced7defc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_5650/3781300316.py\", line 2, in <cell line: 2>\n",
      "    metrics_train_applied_tsne = metrics_reporter.evaluate([mnist_dataset_train, train_applied_tsne])\n",
      "  File \"/home/hubert/librep-hiaac/experiments/../librep/metrics/dimred_evaluator.py\", line 70, in evaluate\n",
      "    drms.append(DRMetrics(X_highdim[i[0]:i[1]], X_lowdim[i[0]:i[1]]))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyDRMetrics/pyDRMetrics.py\", line 45, in __init__\n",
      "    self.Dz = pd.DataFrame(pairwise_distances(dfz.values)).values\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/pairwise.py\", line 2022, in pairwise_distances\n",
      "    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/pairwise.py\", line 1563, in _parallel_pairwise\n",
      "    return func(X, Y, **kwds)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/pairwise.py\", line 328, in euclidean_distances\n",
      "    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/pairwise.py\", line 366, in _euclidean_distances\n",
      "    distances = _euclidean_distances_upcast(X, XX, Y, YY)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/metrics/pairwise.py\", line 517, in _euclidean_distances_upcast\n",
      "    distances = np.empty((n_samples_X, n_samples_Y), dtype=np.float32)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 13.4 GiB for an array with shape (60000, 60000) and data type float32\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/core.py\", line 699, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/core.py\", line 647, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/core.py\", line 626, in executing_piece\n",
      "    return only(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "metrics_reporter = DimensionalityReductionQualityReport(sampling_threshold=60000)\n",
    "metrics_train_applied_tsne = metrics_reporter.evaluate([mnist_dataset_train, train_applied_tsne])\n",
    "print(metrics_train_applied_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58049bcf-7e9e-4233-91d0-2d8b9ad8f704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'residual variance (pearson)': 0.8301890626247299, 'residual variance (spearman)': 0.8775358629453783, 'trustworthiness': 0.8225352120810003, 'continuity': 0.8062537115864876, 'co k nearest neighbor size': 0.4635446369478756, 'local continuity meta criterion': 0.33646833802563925, 'local property': 0.46590098965171045, 'global property': 0.651624257710338}\n"
     ]
    }
   ],
   "source": [
    "metrics_reporter = DimensionalityReductionQualityReport(sampling_threshold=128)\n",
    "metrics_train_applied_tsne = metrics_reporter.evaluate([mnist_dataset_train, train_applied_tsne])\n",
    "print(metrics_train_applied_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36edbf21-dec6-438f-a7a6-0ed98ec92f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'residual variance (pearson)': 0.8628833319928074, 'residual variance (spearman)': 0.8932985862170509, 'trustworthiness': 0.9176937877653166, 'continuity': 0.8874884473916779, 'co k nearest neighbor size': 0.49067092518370076, 'local continuity meta criterion': 0.45854241112747585, 'local property': 0.4432237923261308, 'global property': 0.6448800761210081}\n"
     ]
    }
   ],
   "source": [
    "metrics_reporter = DimensionalityReductionQualityReport(sampling_threshold=500)\n",
    "metrics_train_applied_tsne = metrics_reporter.evaluate([mnist_dataset_train, train_applied_tsne])\n",
    "print(metrics_train_applied_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcc3a06b-eb6c-49c9-a510-95f61631fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'residual variance (pearson)': 0.8711973356490541, 'residual variance (spearman)': 0.8974326725581581, 'trustworthiness': 0.9404433765742684, 'continuity': 0.9142095596783625, 'co k nearest neighbor size': 0.4543731231231231, 'local continuity meta criterion': 0.4383410589948666, 'local property': 0.4350409710732697, 'global property': 0.6426242381124041}\n"
     ]
    }
   ],
   "source": [
    "metrics_reporter = DimensionalityReductionQualityReport(sampling_threshold=1000)\n",
    "metrics_train_applied_tsne = metrics_reporter.evaluate([mnist_dataset_train, train_applied_tsne])\n",
    "print(metrics_train_applied_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b831fb-6607-4974-8f29-48742093647e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5024a8e7-0e5a-4d54-8e76-27f353981ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_umap = UMAP()\n",
    "transformer = TransformMultiModalDataset(transforms=[transform_umap])\n",
    "train_applied_umap = transformer(mnist_dataset_train)\n",
    "test_applied_umap = transformer(mnist_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "459fa8df-5c54-4877-b0a1-a40fceda0d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'residual variance (pearson)': 0.8789082633392178, 'residual variance (spearman)': 0.9321168529678, 'trustworthiness': 0.780808962260786, 'continuity': 0.7810681328944254, 'co k nearest neighbor size': 0.41886090093019596, 'local continuity meta criterion': 0.2917846020079596, 'local property': 0.4768176073417678, 'global property': 0.630489444423573}\n"
     ]
    }
   ],
   "source": [
    "metrics_reporter = DimensionalityReductionQualityReport(sampling_threshold=128)\n",
    "metrics_train_applied_umap = metrics_reporter.evaluate([mnist_dataset_train, train_applied_umap])\n",
    "print(metrics_train_applied_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0d9134cd-26dc-4832-968e-d594366774c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'residual variance (pearson)': 0.9120860777989777, 'residual variance (spearman)': 0.9455947185002721, 'trustworthiness': 0.9261186710895912, 'continuity': 0.8660834047321697, 'co k nearest neighbor size': 0.5160091015364061, 'local continuity meta criterion': 0.48388058748018126, 'local property': 0.447971114794818, 'global property': 0.6236250835603652}\n"
     ]
    }
   ],
   "source": [
    "metrics_reporter = DimensionalityReductionQualityReport(sampling_threshold=500)\n",
    "metrics_train_applied_umap = metrics_reporter.evaluate([mnist_dataset_train, train_applied_umap])\n",
    "print(metrics_train_applied_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e3ee9e8-e752-474a-b3f4-fd0e99bfbbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'residual variance (pearson)': 0.920167565088619, 'residual variance (spearman)': 0.9489185806764261, 'trustworthiness': 0.9429731560570406, 'continuity': 0.8963074648487076, 'co k nearest neighbor size': 0.4733702452452453, 'local continuity meta criterion': 0.4573381811169888, 'local property': 0.4360080097788009, 'global property': 0.6212713223190469}\n"
     ]
    }
   ],
   "source": [
    "metrics_reporter = DimensionalityReductionQualityReport(sampling_threshold=1000)\n",
    "metrics_train_applied_umap = metrics_reporter.evaluate([mnist_dataset_train, train_applied_umap])\n",
    "print(metrics_train_applied_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42d5c8-8fcd-48a9-b36f-0ef94b66fe57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23835e-ffc8-4067-9384-e975806f80f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab96ed1f-36af-4fff-9850-6092d6cbf4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librep.transforms.topo_ae import TopologicalDimensionalityReduction\n",
    "from librep.estimators.ae.torch.models.topological_ae.topological_ae import TopologicallyRegularizedAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cfcb74a0-d953-4012-8421-34b82946407e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using python to compute signatures\n",
      "IMG torch.Size([128, 28, 28])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [157]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# reshaped = \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIMG\u001b[39m\u001b[38;5;124m'\u001b[39m, img\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "topo_transformer = TopologicallyRegularizedAutoencoder()\n",
    "optimizer = torch.optim.Adam(topo_transformer.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "# topo_transformer.fit(train_x)\n",
    "# autoencoder_model='DeepAE'\n",
    "data_loader = torch.utils.data.DataLoader(dataset=train_x, batch_size=128, shuffle=True)\n",
    "for img in data_loader:\n",
    "    orig = img\n",
    "    reshaped = \n",
    "    print('IMG', img.shape)\n",
    "    assert 1==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c07417a7-4c59-4e94-8c46-8482f0241a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:6598.7891\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [162]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m reshaped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(img, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m))\n\u001b[1;32m     16\u001b[0m reshaped \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(reshaped)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 17\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtopo_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreshaped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/estimators/ae/torch/models/topological_ae/topological_ae.py:42\u001b[0m, in \u001b[0;36mTopologicallyRegularizedAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"Compute the loss of the Topologically regularized autoencoder.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    x: Input data\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    Tuple of final_loss, (...loss components...)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[0;32m---> 42\u001b[0m x_distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_distance_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dimensions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# If we have an image dataset, normalize using theoretical maximum\u001b[39;00m\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/estimators/ae/torch/models/topological_ae/topological_ae.py:30\u001b[0m, in \u001b[0;36mTopologicallyRegularizedAutoencoder._compute_distance_matrix\u001b[0;34m(x, p)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_distance_matrix\u001b[39m(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     29\u001b[0m     x_flat \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     distances \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mx_flat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_flat\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, p\u001b[38;5;241m=\u001b[39mp)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m distances\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms as torch_transforms\n",
    "import numpy as np\n",
    "torch_transformer = torch_transforms.ToTensor()\n",
    "X = train_x\n",
    "patience = 10\n",
    "max_loss = 1000\n",
    "num_epochs = 1000\n",
    "batch = 128\n",
    "for epoch in range(num_epochs):\n",
    "    topo_transformer.train()\n",
    "    for img in data_loader:\n",
    "        # print(img)\n",
    "        # assert 1==0\n",
    "        reshaped = np.reshape(img, (-1,1,28,28))\n",
    "        reshaped = torch.Tensor(reshaped).float()\n",
    "        loss, _ = topo_transformer(reshaped)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    if max_loss < loss.item():\n",
    "        if patience == 0:\n",
    "            break\n",
    "        patience -= 1\n",
    "    else:\n",
    "        max_loss = loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2da4b-e86b-474c-a7ad-9f0203cd4b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:6032.2686\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [159]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(reshaped)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# sample = torch_transformer(reshaped)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# torch.squeeze(sample)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtopo_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/estimators/ae/torch/models/topological_ae/topological_ae.py:42\u001b[0m, in \u001b[0;36mTopologicallyRegularizedAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"Compute the loss of the Topologically regularized autoencoder.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    x: Input data\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    Tuple of final_loss, (...loss components...)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[0;32m---> 42\u001b[0m x_distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_distance_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dimensions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# If we have an image dataset, normalize using theoretical maximum\u001b[39;00m\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/estimators/ae/torch/models/topological_ae/topological_ae.py:30\u001b[0m, in \u001b[0;36mTopologicallyRegularizedAutoencoder._compute_distance_matrix\u001b[0;34m(x, p)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_distance_matrix\u001b[39m(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     29\u001b[0m     x_flat \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_flat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m distances\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/functional.py:1507\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1507\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdim\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28minput\u001b[39m, p, _dim, keepdim\u001b[38;5;241m=\u001b[39mkeepdim, dtype\u001b[38;5;241m=\u001b[39mdtype)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms as torch_transforms\n",
    "import numpy as np\n",
    "torch_transformer = torch_transforms.ToTensor()\n",
    "X = train_x\n",
    "patience = 10\n",
    "max_loss = 1000\n",
    "num_epochs = 1000\n",
    "batch = 128\n",
    "for epoch in range(num_epochs):\n",
    "    topo_transformer.train()\n",
    "    for i in range(0,len(X), batch):\n",
    "        # row = np.array([X[i]])\n",
    "        # print('row',row.shape)\n",
    "        orig_value = np.array(X[i:i+batch])\n",
    "        # print('ORIG_VAL', orig_value.shape)\n",
    "        reshaped = np.reshape(orig_value, (-1,1, 28, 28))\n",
    "        # print('RESHAPED', reshaped.shape)\n",
    "        # print(np.array(X[20*i:20*i + 200]).shape)\n",
    "        sample = torch.Tensor(reshaped)\n",
    "        # sample = torch_transformer(reshaped)\n",
    "        # torch.squeeze(sample)\n",
    "        loss, _ = topo_transformer(sample)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    if max_loss < loss.item():\n",
    "        if patience == 0:\n",
    "            break\n",
    "        patience -= 1\n",
    "    else:\n",
    "        max_loss = loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b6c11b02-b0f1-4965-b782-409fd72113ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using python to compute signatures\n",
      "Epoch:1, Loss:7108.2979\n",
      "Epoch:2, Loss:7092.2686\n",
      "Epoch:3, Loss:7153.6641\n",
      "Epoch:4, Loss:7176.3931\n",
      "Epoch:5, Loss:7584.6494\n",
      "Epoch:6, Loss:7030.2422\n",
      "Epoch:7, Loss:7398.6040\n",
      "Epoch:8, Loss:7235.0044\n",
      "Epoch:9, Loss:7495.4717\n",
      "Epoch:10, Loss:7051.5610\n",
      "Epoch:11, Loss:7236.7314\n",
      "Epoch:12, Loss:6753.2441\n",
      "Epoch:13, Loss:7045.3691\n",
      "Epoch:14, Loss:7645.3496\n",
      "Epoch:15, Loss:7203.8330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<librep.transforms.topo_ae.TopologicalDimensionalityReduction at 0x7f89d7cefa30>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ae_model='DeepAE'\n",
    "transform_topoae = TopologicalDimensionalityReduction()\n",
    "transform_topoae.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "54a67f24-81c7-460d-8eae-f690cceb64db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:7023.1533\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [166]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransform_topoae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmnist_dataset_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/transforms/topo_ae.py:33\u001b[0m, in \u001b[0;36mTopologicalDimensionalityReduction.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m reshaped_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape)\n\u001b[1;32m     32\u001b[0m in_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(reshaped_data)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 33\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/estimators/ae/torch/models/topological_ae/topological_ae.py:42\u001b[0m, in \u001b[0;36mTopologicallyRegularizedAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"Compute the loss of the Topologically regularized autoencoder.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    x: Input data\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    Tuple of final_loss, (...loss components...)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[0;32m---> 42\u001b[0m x_distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_distance_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dimensions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# If we have an image dataset, normalize using theoretical maximum\u001b[39;00m\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/estimators/ae/torch/models/topological_ae/topological_ae.py:30\u001b[0m, in \u001b[0;36mTopologicallyRegularizedAutoencoder._compute_distance_matrix\u001b[0;34m(x, p)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_distance_matrix\u001b[39m(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     29\u001b[0m     x_flat \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     distances \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mx_flat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_flat\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, p\u001b[38;5;241m=\u001b[39mp)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m distances\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transform_topoae.fit(mnist_dataset_train.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "852ad244-a72f-40ad-9a12-4cd5be878fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 6.76202   ,  6.76202   ],\n",
       "         [ 2.9179964 ,  3.2432091 ]],\n",
       "\n",
       "        [[42.01605   , 42.01605   ],\n",
       "         [ 1.1252633 ,  3.9875753 ]],\n",
       "\n",
       "        [[ 9.8636265 , 13.577072  ],\n",
       "         [ 8.03079   ,  8.03079   ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[11.331206  ,  5.5345616 ],\n",
       "         [ 3.228788  ,  0.        ]],\n",
       "\n",
       "        [[12.323138  ,  0.38622716],\n",
       "         [12.323138  ,  2.823908  ]],\n",
       "\n",
       "        [[27.669516  , 27.669516  ],\n",
       "         [ 5.7188435 ,  5.500233  ]]],\n",
       "\n",
       "\n",
       "       [[[ 6.6657114 , 18.82138   ],\n",
       "         [ 0.        , 18.82138   ]],\n",
       "\n",
       "        [[13.873988  , 18.49059   ],\n",
       "         [ 8.47049   , 18.49059   ]],\n",
       "\n",
       "        [[22.105213  , 12.938071  ],\n",
       "         [22.105213  ,  1.5794593 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 6.8279905 ,  1.345787  ],\n",
       "         [ 6.293439  ,  1.345787  ]],\n",
       "\n",
       "        [[16.840761  ,  6.886689  ],\n",
       "         [26.06776   , 26.06776   ]],\n",
       "\n",
       "        [[23.972586  , 11.059387  ],\n",
       "         [23.972586  , 11.059387  ]]],\n",
       "\n",
       "\n",
       "       [[[ 8.110809  ,  8.429537  ],\n",
       "         [ 0.45890024,  8.429537  ]],\n",
       "\n",
       "        [[ 8.673986  , 14.933697  ],\n",
       "         [ 0.7771793 ,  0.9182837 ]],\n",
       "\n",
       "        [[11.89309   , 14.957458  ],\n",
       "         [ 0.9083942 ,  0.6847334 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 8.966683  ,  8.966683  ],\n",
       "         [ 0.42672014,  0.25346145]],\n",
       "\n",
       "        [[ 0.5938343 ,  9.517898  ],\n",
       "         [12.443568  , 12.443568  ]],\n",
       "\n",
       "        [[13.010844  , 13.010844  ],\n",
       "         [ 0.51215434, 11.169661  ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[14.460899  , 14.460899  ],\n",
       "         [13.236033  , 13.626799  ]],\n",
       "\n",
       "        [[29.744358  , 29.744358  ],\n",
       "         [ 2.1291199 ,  5.71668   ]],\n",
       "\n",
       "        [[29.256905  , 16.146734  ],\n",
       "         [29.256905  , 16.146734  ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[29.42378   , 29.42378   ],\n",
       "         [ 9.235598  ,  0.        ]],\n",
       "\n",
       "        [[27.410128  ,  8.583766  ],\n",
       "         [27.410128  ,  0.7398156 ]],\n",
       "\n",
       "        [[30.456884  , 30.456884  ],\n",
       "         [ 1.9142518 ,  0.7996002 ]]],\n",
       "\n",
       "\n",
       "       [[[18.693995  ,  9.926     ],\n",
       "         [18.693995  ,  2.9586399 ]],\n",
       "\n",
       "        [[26.955482  , 26.955482  ],\n",
       "         [24.326956  ,  0.98631227]],\n",
       "\n",
       "        [[18.92961   , 15.875454  ],\n",
       "         [18.92961   ,  0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[26.405249  ,  7.1201234 ],\n",
       "         [26.405249  ,  0.        ]],\n",
       "\n",
       "        [[ 3.090207  ,  0.06970857],\n",
       "         [19.690569  , 11.361324  ]],\n",
       "\n",
       "        [[36.807655  , 21.413507  ],\n",
       "         [36.807655  ,  6.362498  ]]],\n",
       "\n",
       "\n",
       "       [[[19.909573  , 19.909573  ],\n",
       "         [19.735054  , 12.098241  ]],\n",
       "\n",
       "        [[19.154478  , 27.273832  ],\n",
       "         [19.154478  , 27.273832  ]],\n",
       "\n",
       "        [[30.13283   , 25.193275  ],\n",
       "         [30.13283   , 25.193275  ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[28.918438  , 21.379477  ],\n",
       "         [28.918438  ,  0.        ]],\n",
       "\n",
       "        [[ 7.7317405 ,  0.        ],\n",
       "         [24.352364  , 24.352364  ]],\n",
       "\n",
       "        [[24.835884  , 15.744816  ],\n",
       "         [24.835884  , 20.406155  ]]]], dtype=float32)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_topoae.transform(mnist_dataset_test.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a1432add-de95-4739-ab63-18fa37846cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:7181.7002\n",
      "Epoch:2, Loss:7269.8188\n",
      "Epoch:3, Loss:7055.5576\n",
      "Epoch:4, Loss:6776.0493\n",
      "Epoch:5, Loss:7176.3638\n",
      "Epoch:6, Loss:6871.8525\n",
      "Epoch:7, Loss:7178.8257\n",
      "Epoch:8, Loss:6921.0229\n",
      "Epoch:9, Loss:6966.6348\n",
      "Epoch:10, Loss:7577.4990\n",
      "Epoch:11, Loss:7870.6362\n",
      "Epoch:12, Loss:7291.6445\n",
      "Epoch:13, Loss:7430.4556\n",
      "Epoch:14, Loss:7559.8550\n",
      "TRANSFORM [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [164]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m transformer \u001b[38;5;241m=\u001b[39m TransformMultiModalDataset(transforms\u001b[38;5;241m=\u001b[39m[transform_topoae])\n\u001b[0;32m----> 2\u001b[0m train_applied_topoae \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmnist_dataset_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_applied_topoae \u001b[38;5;241m=\u001b[39m transformer(mnist_dataset_test)\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/datasets/multimodal/transformer.py:52\u001b[0m, in \u001b[0;36mTransformMultiModalDataset.__call__\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     50\u001b[0m X \u001b[38;5;241m=\u001b[39m new_dataset[:][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m y \u001b[38;5;241m=\u001b[39m new_dataset[:][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 52\u001b[0m new_X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__transform_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_slices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m new_y \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Calculate new slices\u001b[39;00m\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/datasets/multimodal/transformer.py:42\u001b[0m, in \u001b[0;36mTransformMultiModalDataset.__transform_sample\u001b[0;34m(self, transform, X, y, slices)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__transform_sample\u001b[39m(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     37\u001b[0m     transform: Transform,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     slices: List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]],\n\u001b[1;32m     41\u001b[0m ):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     43\u001b[0m         transform\u001b[38;5;241m.\u001b[39mfit_transform(X[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, start:end], y)\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m slices\n\u001b[1;32m     45\u001b[0m     ]\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/datasets/multimodal/transformer.py:43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__transform_sample\u001b[39m(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     37\u001b[0m     transform: Transform,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     slices: List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]],\n\u001b[1;32m     41\u001b[0m ):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m---> 43\u001b[0m         \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m slices\n\u001b[1;32m     45\u001b[0m     ]\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/base/transform.py:54\u001b[0m, in \u001b[0;36mTransform.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: ArrayLike, y: ArrayLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/transforms/topo_ae.py:50\u001b[0m, in \u001b[0;36mTopologicalDimensionalityReduction.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRANSFORM\u001b[39m\u001b[38;5;124m'\u001b[39m, X)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/estimators/ae/torch/models/topological_ae/topological_ae.py:82\u001b[0m, in \u001b[0;36mTopologicallyRegularizedAutoencoder.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/../librep/estimators/ae/torch/models/topological_ae/model_submodules.py:40\u001b[0m, in \u001b[0;36mConvolutionalAutoencoder.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute latent representation using convolutional autoencoder.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "transformer = TransformMultiModalDataset(transforms=[transform_topoae])\n",
    "train_applied_topoae = transformer(mnist_dataset_train)\n",
    "test_applied_topoae = transformer(mnist_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61cb0c8-34ad-4a31-8133-c63e83225323",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_by_y = [[], [], [], [], [], [], [], [], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354fef4-9fc6-422e-8dc4-abc7ff82d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD TUTORIAL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from librep.base.data import SimpleDataset\n",
    "from librep.transforms import UMAP\n",
    "from librep.datasets.multimodal import TransformMultiModalDataset, ArrayMultiModalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c801b1c4-9d2f-4cf6-ba44-c21b0d3ba00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset_train = ArrayMultiModalDataset(X=train_x_reordered, y=train_y, window_slices=[(0, 28*28)], \n",
    "                                             window_names=[\"px\"])\n",
    "mnist_dataset_test = SimpleDataset(X=test_x_reordered, y=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c47b87-e259-4696-8c6e-dbe71de088fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_umap = UMAP()\n",
    "transformer = TransformMultiModalDataset(transforms=[transform_umap])\n",
    "transformed_dataset = transformer(mnist_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ca674-4762-4c39-bc96-d9fa9b62b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformed_dataset.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0dc75-b1b3-4451-ac58-4ccf948a5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformMultiModalDataset(transforms=[transform_umap])\n",
    "transformed_dataset = transformer(mnist_dataset_train.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fb4a9-7db2-47cf-b05f-0eb7fb4d6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset_train.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ccf11-e3b8-46cc-ad4e-9b82123e5b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b909d-af7a-48a5-9154-d6ba6c8e3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_umap = UMAP()\n",
    "transformed_dataset = transform_umap.fit_transform(train_x_reordered)\n",
    "# transformer = TransformMultiModalDataset(transforms=[transform_umap])\n",
    "# transformed_dataset = transformer(mnist_dataset_train)\n",
    "print(transformed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b916d-90db-4f0e-8235-ecde853449e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, X: list, y: list):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index], self.y[index])\n",
    "\n",
    "    # (optional) Simple interface to describe the object as string\n",
    "    def __str__(self) -> str:\n",
    "        return f\"MNISTDataset: {len(self)} samples; {len(set(self.y))} classes\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a305e959-89ed-4416-9e63-941e4a5a46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [[1, 2, 3], [3, 2, 1], [0.5, 0.5, 0.5], [0, 0, 0]]\n",
    "labels = [0, 0, 1, 1]\n",
    "\n",
    "dataset = MNISTDataset(X=samples, y=labels)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb03c1f-082c-47c7-9e16-07511db917ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be420576-799b-4d04-8baf-b6a53b46ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = dataset\n",
    "trains_by_y = [[], [], [], [], [], [], [], [], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc775d-613e-48eb-824b-fd75c8e64453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "tsne_reducer = TSNE()\n",
    "np_train_x = np.array(train_x)\n",
    "print(np_train_x.shape)\n",
    "reordered = np_train_x.reshape((60000,-1))\n",
    "print(reordered.shape)\n",
    "train_x_2d = tsne_reducer.fit_transform(np.array(reordered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf1325-dd4b-47ab-a3ba-5544eb65c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(train_y)):\n",
    "    y_val = train_y[index]\n",
    "    trains_by_y[y_val].append(train_x_2d[index])\n",
    "# print(trains_by_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f347ac0-cf81-4a31-8004-411b455179e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(10, 10), dpi=80)\n",
    "for i in range(len(trains_by_y)):\n",
    "    data = trains_by_y[i]\n",
    "    plt.plot(np.array(data)[:,0], np.array(data)[:,1], '.', markersize=10, label=str(i))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08219e-493d-4c84-914c-9604b0a35ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # For defining dataset Paths\n",
    "import sys                # For include librep package\n",
    "\n",
    "# This must be done if librep is not installed via pip,\n",
    "# as this directory (examples) is appart from librep package root\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Third party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librep imports\n",
    "from librep.utils.dataset import PandasDatasetsIO          # For quick load train, test and validation CSVs\n",
    "from librep.datasets.multimodal import PandasMultiModalDataset # Wrap CSVs to librep's `Dataset` interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ea03d-6350-4c7a-a08b-a31082802f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for KuHar balanced view with the same activities (and labels numbers) as MotionSense\n",
    "# It is assumed that the directory will contain (train.csv, test.csv and validation.csv)\n",
    "kuhar_dataset_path = Path(\"../data/views/KuHar/balanced_motionsense_equivalent_view\")\n",
    "\n",
    "# Path for Motionsese balanced view\n",
    "motionsense_dataset_path = Path(\"../data/views/MotionSense/balanced_view\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
