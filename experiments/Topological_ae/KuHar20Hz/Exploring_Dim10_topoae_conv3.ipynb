{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1630ed-63c8-4ee7-86c9-62dfb528b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedad853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1e18129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "<torch.cuda.device object at 0x7f98c06c10a0>\n",
      "NVIDIA TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available()) # True\n",
    "print(torch.cuda.device_count()) # 1\n",
    "print(torch.cuda.current_device()) # 0\n",
    "print(torch.cuda.device(0)) # <torch.cuda.device at 0x7efce0b03be0>\n",
    "print(torch.cuda.get_device_name(0)) # 'GeForce GTX 950M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da911e3-e82f-47c9-8090-f71f453956ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f067405-ab8c-4f22-b056-ab9e1febe71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5007e3-ef19-4658-ac7d-2a6f617bb9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 13:27:23.114247: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-24 13:27:24.670726: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-24 13:27:24.670858: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-24 13:27:24.670874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from librep.datasets.har.loaders import (\n",
    "    KuHar_BalancedView20HzMotionSenseEquivalent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9f66b2-7b1c-4d4e-a0e3-cde9552a595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librep.transforms.topo_ae import (\n",
    "    TopologicalDimensionalityReduction,\n",
    "    CustomTopoDimRedTransform\n",
    ")\n",
    "from librep.transforms import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "from experiments.Topological_ae.Experiment_utils import *\n",
    "from librep.datasets.multimodal import TransformMultiModalDataset\n",
    "from librep.transforms.fft import FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85bea424-07ff-47ec-945a-aef385afe7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MotionSense Loader\n",
    "loader = KuHar_BalancedView20HzMotionSenseEquivalent(\n",
    "    root_dir=\"../../../data/views/KuHar/balanced_20Hz_motionsense_equivalent-v1\", \n",
    "    download=False\n",
    ")\n",
    "\n",
    "# Print the readme (optional)\n",
    "# loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d817b0c-4e6b-4227-9b48-cd23c3df190d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=3114, features=360, no. window=6, label_columns='standard activity code',\n",
       " PandasMultiModalDataset: samples=246, features=360, no. window=6, label_columns='standard activity code')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# If concat_train_validation is true, return a tuple (train+validation, test)\n",
    "train_val, test = loader.load(concat_train_validation=True, label=loader.standard_label)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9491b1e0-0b60-439c-9bed-ea0ee7918b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_HD = np.array(train_val[:][0])\n",
    "train_Y = np.array(train_val[:][1])\n",
    "test_HD = np.array(test[:][0])\n",
    "test_Y = np.array(test[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc4810ac-c560-4b23-9b72-481a2d0f1a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3114, 360) (3114,) (246, 360) (246,)\n"
     ]
    }
   ],
   "source": [
    "print(train_HD.shape, train_Y.shape, test_HD.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93d061-6ea0-418b-b690-2d977f6297c9",
   "metadata": {},
   "source": [
    "# Aplicar FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5255de5-0259-404a-a2d8-507ad4f2ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_transform = FFT(centered = True)\n",
    "transformer = TransformMultiModalDataset(\n",
    "    transforms=[fft_transform],\n",
    "    new_window_name_prefix=\"fft.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29195f38-f8a9-4167-937f-d3f98ddc7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_fft = transformer(train_val)\n",
    "test_dataset_fft = transformer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c8cfc25-0323-498a-931d-1249078f2796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3114, 180)\n",
      "(246, 180)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_fft.X.shape)\n",
    "print(test_dataset_fft.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d413cc20-e5b5-4594-b1f7-f11153c00c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199.4899017510002\n"
     ]
    }
   ],
   "source": [
    "print(np.max(train_dataset_fft.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea2e105-a571-4a2f-ac76-096b08a6ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_HD = train_dataset_fft.X\n",
    "train_LD = None\n",
    "train_Y = train_dataset_fft.y\n",
    "test_HD = test_dataset_fft.X\n",
    "test_LD = None\n",
    "test_Y = test_dataset_fft.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee13aef1-77c7-42a9-930b-b1a56f980d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3114, 180) (3114,) (246, 180) (246,)\n"
     ]
    }
   ],
   "source": [
    "print(train_HD.shape, train_Y.shape, test_HD.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adbc2e-4af6-40a4-85dc-99b2336d656c",
   "metadata": {},
   "source": [
    "# Visualization helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d56d7ec7-6bf5-4c32-91aa-d48a8746a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: sit (569 train, 101 validation, 170 test)\n",
    "# 1: stand (569 train, 101 validation, 170 test)\n",
    "# 2: walk (569 train, 101 validation, 170 test)\n",
    "# 3: stair up (569 train, 101 validation, 170 test)\n",
    "# 4: stair down (569 train, 101 validation, 170 test)\n",
    "# 5: run (569 train, 101 validation, 170 test)\n",
    "def visualize(X, Y):\n",
    "    labels = ['sit', 'stand', 'walk', 'stair up', 'stair down', 'run']\n",
    "    df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=Y))\n",
    "    groups = df.groupby('label')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.margins(0.05)\n",
    "    for name, group in groups:\n",
    "        ax.plot(group.x, group.y, marker='.', linestyle='', ms=8, label=labels[name])\n",
    "    # Shrink current axis by 20%\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "    # Put a legend to the right of the current axis\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80f4771a-da3f-407b-9fa9-b0619a129e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to reuse\n",
    "model_dim = 10\n",
    "model_epc = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8264b9-7756-4a8d-a8c0-18a1645b8e28",
   "metadata": {},
   "source": [
    "# Reducing with Convolutional Topological Autoencoders (L=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8eefd1ec-28b6-4937-872a-ce26dc99d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lam = 1\n",
    "# model_dim = 10\n",
    "# model_epc = 1500\n",
    "# topo_reducer = TopologicalDimensionalityReduction(\n",
    "#     ae_model='ConvolutionalAutoencoder_custom_dim2',\n",
    "#     lam = model_lam,\n",
    "#     ae_kwargs = {'input_dims':(1, 180), 'custom_dim':model_dim},\n",
    "#     input_shape = (-1, 1, 1, 180),\n",
    "#     patience = None,\n",
    "#     num_epochs = model_epc\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d80ae63d-92f7-49c6-9527-212af7500573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topologically Regularized ConvolutionalAutoencoder_custom_dim3\n",
      "Using python to compute signatures\n",
      "ConvAECustomDim, Input: (1, 180) Inner dim: 10\n",
      "ENCODER STRUCT torch.Size([4, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "model_lam = 0\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d46f8ff8-e7a8-42e1-9fd2-d1e9a17b8cad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darlinne.soto/librep-hiaac/experiments/Topological_ae/KuHar20Hz/../../../librep/transforms/topo_ae.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  in_tensor = torch.tensor(reshaped_data, device=cuda0).float()\n",
      "/home/darlinne.soto/librep-hiaac/experiments/Topological_ae/KuHar20Hz/../../../librep/transforms/topo_ae.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  in_tensor = torch.tensor(reshaped_data, device=cuda0).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, P:None, Loss:492.0904, Loss-ae:492.0904, Loss-topo:279.3173\n",
      "Epoch:2, P:None, Loss:411.2936, Loss-ae:411.2936, Loss-topo:274.0433\n",
      "Epoch:3, P:None, Loss:384.3358, Loss-ae:384.3358, Loss-topo:331.1529\n",
      "Epoch:4, P:None, Loss:370.8838, Loss-ae:370.8838, Loss-topo:620.0950\n",
      "Epoch:5, P:None, Loss:301.7717, Loss-ae:301.7717, Loss-topo:1637.4410\n",
      "Epoch:6, P:None, Loss:253.7304, Loss-ae:253.7304, Loss-topo:3723.6897\n",
      "Epoch:7, P:None, Loss:229.8337, Loss-ae:229.8337, Loss-topo:5554.5778\n",
      "Epoch:8, P:None, Loss:212.3578, Loss-ae:212.3578, Loss-topo:6905.7755\n",
      "Epoch:9, P:None, Loss:200.1848, Loss-ae:200.1848, Loss-topo:7849.5191\n",
      "Epoch:10, P:None, Loss:185.0492, Loss-ae:185.0492, Loss-topo:11288.9084\n",
      "Epoch:11, P:None, Loss:170.5841, Loss-ae:170.5841, Loss-topo:13169.5381\n",
      "Epoch:12, P:None, Loss:161.2224, Loss-ae:161.2224, Loss-topo:16640.8980\n",
      "Epoch:13, P:None, Loss:154.7648, Loss-ae:154.7648, Loss-topo:18834.2260\n",
      "Epoch:14, P:None, Loss:149.3900, Loss-ae:149.3900, Loss-topo:19699.9762\n",
      "Epoch:15, P:None, Loss:145.1856, Loss-ae:145.1856, Loss-topo:20177.0883\n",
      "Epoch:16, P:None, Loss:140.5640, Loss-ae:140.5640, Loss-topo:22031.3563\n",
      "Epoch:17, P:None, Loss:137.8207, Loss-ae:137.8207, Loss-topo:24701.0934\n",
      "Epoch:18, P:None, Loss:134.4309, Loss-ae:134.4309, Loss-topo:26646.9098\n",
      "Epoch:19, P:None, Loss:130.6756, Loss-ae:130.6756, Loss-topo:28715.7090\n",
      "Epoch:20, P:None, Loss:129.0105, Loss-ae:129.0105, Loss-topo:30417.9832\n",
      "Epoch:21, P:None, Loss:126.8209, Loss-ae:126.8209, Loss-topo:30454.1996\n",
      "Epoch:22, P:None, Loss:123.9180, Loss-ae:123.9180, Loss-topo:33358.5461\n",
      "Epoch:23, P:None, Loss:121.2446, Loss-ae:121.2446, Loss-topo:38175.5387\n",
      "Epoch:24, P:None, Loss:121.8722, Loss-ae:121.8722, Loss-topo:32728.0996\n",
      "Epoch:25, P:None, Loss:118.1067, Loss-ae:118.1067, Loss-topo:40523.1711\n",
      "Epoch:26, P:None, Loss:115.9420, Loss-ae:115.9420, Loss-topo:40410.5809\n",
      "Epoch:27, P:None, Loss:117.5536, Loss-ae:117.5536, Loss-topo:39117.3914\n",
      "Epoch:28, P:None, Loss:114.3534, Loss-ae:114.3534, Loss-topo:43209.9969\n",
      "Epoch:29, P:None, Loss:112.4975, Loss-ae:112.4975, Loss-topo:41983.0547\n",
      "Epoch:30, P:None, Loss:111.0482, Loss-ae:111.0482, Loss-topo:43344.5531\n",
      "Epoch:31, P:None, Loss:111.1066, Loss-ae:111.1066, Loss-topo:43798.7684\n",
      "Epoch:32, P:None, Loss:109.0532, Loss-ae:109.0532, Loss-topo:48265.0687\n",
      "Epoch:33, P:None, Loss:109.5651, Loss-ae:109.5651, Loss-topo:45389.0121\n",
      "Epoch:34, P:None, Loss:108.2683, Loss-ae:108.2683, Loss-topo:46294.1594\n",
      "Epoch:35, P:None, Loss:109.7010, Loss-ae:109.7010, Loss-topo:48615.2844\n",
      "Epoch:36, P:None, Loss:106.6049, Loss-ae:106.6049, Loss-topo:50594.2852\n",
      "Epoch:37, P:None, Loss:106.7050, Loss-ae:106.7050, Loss-topo:52296.7734\n",
      "Epoch:38, P:None, Loss:105.7949, Loss-ae:105.7949, Loss-topo:52921.8930\n",
      "Epoch:39, P:None, Loss:105.0817, Loss-ae:105.0817, Loss-topo:52880.2539\n",
      "Epoch:40, P:None, Loss:103.9750, Loss-ae:103.9750, Loss-topo:57028.7102\n",
      "Epoch:41, P:None, Loss:104.0217, Loss-ae:104.0217, Loss-topo:60910.4617\n",
      "Epoch:42, P:None, Loss:91.5417, Loss-ae:91.5417, Loss-topo:60222.4055\n",
      "Epoch:43, P:None, Loss:85.7875, Loss-ae:85.7875, Loss-topo:62529.4266\n",
      "Epoch:44, P:None, Loss:83.2729, Loss-ae:83.2729, Loss-topo:64803.2164\n",
      "Epoch:45, P:None, Loss:81.5514, Loss-ae:81.5514, Loss-topo:62228.4313\n",
      "Epoch:46, P:None, Loss:80.6225, Loss-ae:80.6225, Loss-topo:60263.1969\n",
      "Epoch:47, P:None, Loss:80.6354, Loss-ae:80.6354, Loss-topo:66050.8836\n",
      "Epoch:48, P:None, Loss:80.1254, Loss-ae:80.1254, Loss-topo:65482.0055\n",
      "Epoch:49, P:None, Loss:79.9230, Loss-ae:79.9230, Loss-topo:60070.4648\n",
      "Epoch:50, P:None, Loss:79.4920, Loss-ae:79.4920, Loss-topo:70565.5828\n",
      "Epoch:51, P:None, Loss:78.1022, Loss-ae:78.1022, Loss-topo:70217.4844\n",
      "Epoch:52, P:None, Loss:78.8243, Loss-ae:78.8243, Loss-topo:65577.5391\n",
      "Epoch:53, P:None, Loss:78.5016, Loss-ae:78.5016, Loss-topo:70321.8609\n",
      "Epoch:54, P:None, Loss:77.5800, Loss-ae:77.5800, Loss-topo:66554.0039\n",
      "Epoch:55, P:None, Loss:76.8265, Loss-ae:76.8265, Loss-topo:70274.6508\n",
      "Epoch:56, P:None, Loss:76.6009, Loss-ae:76.6009, Loss-topo:68163.8703\n",
      "Epoch:57, P:None, Loss:77.2367, Loss-ae:77.2367, Loss-topo:67552.0211\n",
      "Epoch:58, P:None, Loss:76.4114, Loss-ae:76.4114, Loss-topo:71029.6625\n",
      "Epoch:59, P:None, Loss:77.0283, Loss-ae:77.0283, Loss-topo:72703.3570\n",
      "Epoch:60, P:None, Loss:76.8468, Loss-ae:76.8468, Loss-topo:76756.4578\n",
      "Epoch:61, P:None, Loss:74.8971, Loss-ae:74.8971, Loss-topo:69875.3977\n",
      "Epoch:62, P:None, Loss:77.7423, Loss-ae:77.7423, Loss-topo:77562.1062\n",
      "Epoch:63, P:None, Loss:75.4035, Loss-ae:75.4035, Loss-topo:73998.2430\n",
      "Epoch:64, P:None, Loss:73.9845, Loss-ae:73.9845, Loss-topo:67956.5844\n",
      "Epoch:65, P:None, Loss:74.2135, Loss-ae:74.2135, Loss-topo:74368.4984\n",
      "Epoch:66, P:None, Loss:75.6253, Loss-ae:75.6253, Loss-topo:76164.7172\n",
      "Epoch:67, P:None, Loss:74.9145, Loss-ae:74.9145, Loss-topo:77672.5570\n",
      "Epoch:68, P:None, Loss:74.6496, Loss-ae:74.6496, Loss-topo:81483.1266\n",
      "Epoch:69, P:None, Loss:74.5492, Loss-ae:74.5492, Loss-topo:77687.6898\n",
      "Epoch:70, P:None, Loss:73.2660, Loss-ae:73.2660, Loss-topo:77242.7898\n",
      "Epoch:71, P:None, Loss:74.5541, Loss-ae:74.5541, Loss-topo:77517.1547\n",
      "Epoch:72, P:None, Loss:73.5446, Loss-ae:73.5446, Loss-topo:77472.0406\n",
      "Epoch:73, P:None, Loss:73.6320, Loss-ae:73.6320, Loss-topo:79639.6891\n",
      "Epoch:74, P:None, Loss:72.7261, Loss-ae:72.7261, Loss-topo:79855.1219\n",
      "Epoch:75, P:None, Loss:73.2611, Loss-ae:73.2611, Loss-topo:79795.1977\n",
      "Epoch:76, P:None, Loss:74.0583, Loss-ae:74.0583, Loss-topo:84562.3266\n",
      "Epoch:77, P:None, Loss:73.0532, Loss-ae:73.0532, Loss-topo:77709.8453\n",
      "Epoch:78, P:None, Loss:73.4719, Loss-ae:73.4719, Loss-topo:85364.8766\n",
      "Epoch:79, P:None, Loss:71.9409, Loss-ae:71.9409, Loss-topo:87440.3953\n",
      "Epoch:80, P:None, Loss:72.8065, Loss-ae:72.8065, Loss-topo:84941.6391\n",
      "Epoch:81, P:None, Loss:74.1056, Loss-ae:74.1056, Loss-topo:82001.6016\n",
      "Epoch:82, P:None, Loss:71.8410, Loss-ae:71.8410, Loss-topo:84881.3484\n",
      "Epoch:83, P:None, Loss:71.6476, Loss-ae:71.6476, Loss-topo:87984.1406\n",
      "Epoch:84, P:None, Loss:72.8794, Loss-ae:72.8794, Loss-topo:88549.5813\n",
      "Epoch:85, P:None, Loss:72.0464, Loss-ae:72.0464, Loss-topo:85078.4156\n",
      "Epoch:86, P:None, Loss:71.9186, Loss-ae:71.9186, Loss-topo:83576.9648\n",
      "Epoch:87, P:None, Loss:71.4923, Loss-ae:71.4923, Loss-topo:83826.8844\n",
      "Epoch:88, P:None, Loss:71.4059, Loss-ae:71.4059, Loss-topo:85445.9922\n",
      "Epoch:89, P:None, Loss:71.8837, Loss-ae:71.8837, Loss-topo:92644.8875\n",
      "Epoch:90, P:None, Loss:72.5695, Loss-ae:72.5695, Loss-topo:86262.2266\n",
      "Epoch:91, P:None, Loss:71.4488, Loss-ae:71.4488, Loss-topo:88928.3734\n",
      "Epoch:92, P:None, Loss:70.8026, Loss-ae:70.8026, Loss-topo:84306.3281\n",
      "Epoch:93, P:None, Loss:72.0850, Loss-ae:72.0850, Loss-topo:88328.7391\n",
      "Epoch:94, P:None, Loss:70.3049, Loss-ae:70.3049, Loss-topo:90827.3156\n",
      "Epoch:95, P:None, Loss:70.8215, Loss-ae:70.8215, Loss-topo:87653.4187\n",
      "Epoch:96, P:None, Loss:70.7481, Loss-ae:70.7481, Loss-topo:88818.5609\n",
      "Epoch:97, P:None, Loss:72.8434, Loss-ae:72.8434, Loss-topo:89921.0594\n",
      "Epoch:98, P:None, Loss:72.0809, Loss-ae:72.0809, Loss-topo:84096.4516\n",
      "Epoch:99, P:None, Loss:70.6537, Loss-ae:70.6537, Loss-topo:84971.3039\n",
      "Epoch:100, P:None, Loss:70.8559, Loss-ae:70.8559, Loss-topo:92283.4594\n",
      "Epoch:101, P:None, Loss:70.3658, Loss-ae:70.3658, Loss-topo:85586.5711\n",
      "Epoch:102, P:None, Loss:69.4680, Loss-ae:69.4680, Loss-topo:92185.0828\n",
      "Epoch:103, P:None, Loss:70.3794, Loss-ae:70.3794, Loss-topo:86215.1484\n",
      "Epoch:104, P:None, Loss:70.6733, Loss-ae:70.6733, Loss-topo:88814.5797\n",
      "Epoch:105, P:None, Loss:70.4008, Loss-ae:70.4008, Loss-topo:87420.6656\n",
      "Epoch:106, P:None, Loss:70.2642, Loss-ae:70.2642, Loss-topo:101232.2906\n",
      "Epoch:107, P:None, Loss:70.0857, Loss-ae:70.0857, Loss-topo:91437.9469\n",
      "Epoch:108, P:None, Loss:70.1273, Loss-ae:70.1273, Loss-topo:93406.3844\n",
      "Epoch:109, P:None, Loss:70.6106, Loss-ae:70.6106, Loss-topo:89673.6109\n",
      "Epoch:110, P:None, Loss:70.7724, Loss-ae:70.7724, Loss-topo:94351.0109\n",
      "Epoch:111, P:None, Loss:69.3627, Loss-ae:69.3627, Loss-topo:88964.7883\n",
      "Epoch:112, P:None, Loss:68.9972, Loss-ae:68.9972, Loss-topo:87022.7977\n",
      "Epoch:113, P:None, Loss:69.2801, Loss-ae:69.2801, Loss-topo:94008.3047\n",
      "Epoch:114, P:None, Loss:68.5407, Loss-ae:68.5407, Loss-topo:91358.1641\n",
      "Epoch:115, P:None, Loss:69.6130, Loss-ae:69.6130, Loss-topo:91263.9312\n",
      "Epoch:116, P:None, Loss:68.3246, Loss-ae:68.3246, Loss-topo:94176.9594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:117, P:None, Loss:69.6500, Loss-ae:69.6500, Loss-topo:97975.7156\n",
      "Epoch:118, P:None, Loss:68.8206, Loss-ae:68.8206, Loss-topo:91599.0203\n",
      "Epoch:119, P:None, Loss:68.9461, Loss-ae:68.9461, Loss-topo:94375.9375\n",
      "Epoch:120, P:None, Loss:69.3616, Loss-ae:69.3616, Loss-topo:94346.7375\n",
      "Epoch:121, P:None, Loss:69.1478, Loss-ae:69.1478, Loss-topo:93658.8047\n",
      "Epoch:122, P:None, Loss:72.2385, Loss-ae:72.2385, Loss-topo:104975.4047\n",
      "Epoch:123, P:None, Loss:69.7558, Loss-ae:69.7558, Loss-topo:88644.3844\n",
      "Epoch:124, P:None, Loss:69.7637, Loss-ae:69.7637, Loss-topo:99373.1391\n",
      "Epoch:125, P:None, Loss:67.8613, Loss-ae:67.8613, Loss-topo:95007.9500\n",
      "Epoch:126, P:None, Loss:68.0685, Loss-ae:68.0685, Loss-topo:97462.7266\n",
      "Epoch:127, P:None, Loss:69.5109, Loss-ae:69.5109, Loss-topo:94443.9250\n",
      "Epoch:128, P:None, Loss:68.7243, Loss-ae:68.7243, Loss-topo:100531.7250\n",
      "Epoch:129, P:None, Loss:69.0679, Loss-ae:69.0679, Loss-topo:96053.2766\n",
      "Epoch:130, P:None, Loss:68.5301, Loss-ae:68.5301, Loss-topo:105322.4406\n",
      "Epoch:131, P:None, Loss:68.9980, Loss-ae:68.9980, Loss-topo:99071.0719\n",
      "Epoch:132, P:None, Loss:67.7325, Loss-ae:67.7325, Loss-topo:92634.9969\n",
      "Epoch:133, P:None, Loss:68.6430, Loss-ae:68.6430, Loss-topo:100396.8266\n",
      "Epoch:134, P:None, Loss:68.9091, Loss-ae:68.9091, Loss-topo:101526.0016\n",
      "Epoch:135, P:None, Loss:67.7226, Loss-ae:67.7226, Loss-topo:95026.4500\n",
      "Epoch:136, P:None, Loss:67.6589, Loss-ae:67.6589, Loss-topo:96251.9109\n",
      "Epoch:137, P:None, Loss:68.4361, Loss-ae:68.4361, Loss-topo:103277.8969\n",
      "Epoch:138, P:None, Loss:68.2551, Loss-ae:68.2551, Loss-topo:97527.5750\n",
      "Epoch:139, P:None, Loss:68.2011, Loss-ae:68.2011, Loss-topo:105819.1219\n",
      "Epoch:140, P:None, Loss:67.9981, Loss-ae:67.9981, Loss-topo:105297.9875\n",
      "Epoch:141, P:None, Loss:68.1586, Loss-ae:68.1586, Loss-topo:103677.8125\n",
      "Epoch:142, P:None, Loss:68.4815, Loss-ae:68.4815, Loss-topo:97296.5719\n",
      "Epoch:143, P:None, Loss:67.4980, Loss-ae:67.4980, Loss-topo:101412.5203\n",
      "Epoch:144, P:None, Loss:67.2238, Loss-ae:67.2238, Loss-topo:97859.1016\n",
      "Epoch:145, P:None, Loss:67.5454, Loss-ae:67.5454, Loss-topo:104018.3313\n",
      "Epoch:146, P:None, Loss:68.1744, Loss-ae:68.1744, Loss-topo:102157.4531\n",
      "Epoch:147, P:None, Loss:67.7604, Loss-ae:67.7604, Loss-topo:108250.2906\n",
      "Epoch:148, P:None, Loss:68.1054, Loss-ae:68.1054, Loss-topo:100932.7984\n",
      "Epoch:149, P:None, Loss:68.7084, Loss-ae:68.7084, Loss-topo:101405.9719\n",
      "Epoch:150, P:None, Loss:68.4395, Loss-ae:68.4395, Loss-topo:99933.8109\n",
      "Epoch:151, P:None, Loss:66.8963, Loss-ae:66.8963, Loss-topo:103216.3391\n",
      "Epoch:152, P:None, Loss:67.8286, Loss-ae:67.8286, Loss-topo:99311.6406\n",
      "Epoch:153, P:None, Loss:68.4923, Loss-ae:68.4923, Loss-topo:98996.2406\n",
      "Epoch:154, P:None, Loss:67.8425, Loss-ae:67.8425, Loss-topo:105571.1406\n",
      "Epoch:155, P:None, Loss:67.6770, Loss-ae:67.6770, Loss-topo:102690.5672\n",
      "Epoch:156, P:None, Loss:67.8054, Loss-ae:67.8054, Loss-topo:105621.3906\n",
      "Epoch:157, P:None, Loss:68.0481, Loss-ae:68.0481, Loss-topo:102295.4344\n",
      "Epoch:158, P:None, Loss:66.9887, Loss-ae:66.9887, Loss-topo:100144.4469\n",
      "Epoch:159, P:None, Loss:66.9891, Loss-ae:66.9891, Loss-topo:104178.8547\n",
      "Epoch:160, P:None, Loss:67.5425, Loss-ae:67.5425, Loss-topo:101774.5453\n",
      "Epoch:161, P:None, Loss:67.5779, Loss-ae:67.5779, Loss-topo:103248.6141\n",
      "Epoch:162, P:None, Loss:67.2083, Loss-ae:67.2083, Loss-topo:102208.6094\n",
      "Epoch:163, P:None, Loss:67.3230, Loss-ae:67.3230, Loss-topo:100523.0531\n",
      "Epoch:164, P:None, Loss:67.3100, Loss-ae:67.3100, Loss-topo:100911.6578\n",
      "Epoch:165, P:None, Loss:67.5076, Loss-ae:67.5076, Loss-topo:106702.1313\n",
      "Epoch:166, P:None, Loss:66.9530, Loss-ae:66.9530, Loss-topo:104949.4281\n",
      "Epoch:167, P:None, Loss:67.8744, Loss-ae:67.8744, Loss-topo:106198.0203\n",
      "Epoch:168, P:None, Loss:66.4613, Loss-ae:66.4613, Loss-topo:97714.5156\n",
      "Epoch:169, P:None, Loss:66.6743, Loss-ae:66.6743, Loss-topo:103744.7484\n",
      "Epoch:170, P:None, Loss:66.9756, Loss-ae:66.9756, Loss-topo:109398.3453\n",
      "Epoch:171, P:None, Loss:66.2547, Loss-ae:66.2547, Loss-topo:106935.6078\n",
      "Epoch:172, P:None, Loss:66.7379, Loss-ae:66.7379, Loss-topo:97613.2188\n",
      "Epoch:173, P:None, Loss:66.8313, Loss-ae:66.8313, Loss-topo:104028.7141\n",
      "Epoch:174, P:None, Loss:65.8280, Loss-ae:65.8280, Loss-topo:95067.2578\n",
      "Epoch:175, P:None, Loss:68.0726, Loss-ae:68.0726, Loss-topo:100348.7172\n",
      "Epoch:176, P:None, Loss:66.6871, Loss-ae:66.6871, Loss-topo:97947.1047\n",
      "Epoch:177, P:None, Loss:66.2032, Loss-ae:66.2032, Loss-topo:107936.3422\n",
      "Epoch:178, P:None, Loss:66.8413, Loss-ae:66.8413, Loss-topo:103927.1422\n",
      "Epoch:179, P:None, Loss:67.0299, Loss-ae:67.0299, Loss-topo:104106.1187\n",
      "Epoch:180, P:None, Loss:67.1438, Loss-ae:67.1438, Loss-topo:104983.1094\n",
      "Epoch:181, P:None, Loss:66.8068, Loss-ae:66.8068, Loss-topo:101625.3328\n",
      "Epoch:182, P:None, Loss:66.4266, Loss-ae:66.4266, Loss-topo:109091.8203\n",
      "Epoch:183, P:None, Loss:67.2432, Loss-ae:67.2432, Loss-topo:110975.6703\n",
      "Epoch:184, P:None, Loss:66.9014, Loss-ae:66.9014, Loss-topo:103022.6344\n",
      "Epoch:185, P:None, Loss:66.0831, Loss-ae:66.0831, Loss-topo:103712.7063\n",
      "Epoch:186, P:None, Loss:66.2600, Loss-ae:66.2600, Loss-topo:106611.4281\n",
      "Epoch:187, P:None, Loss:65.9681, Loss-ae:65.9681, Loss-topo:106559.0437\n",
      "Epoch:188, P:None, Loss:66.1935, Loss-ae:66.1935, Loss-topo:104800.8766\n",
      "Epoch:189, P:None, Loss:66.9748, Loss-ae:66.9748, Loss-topo:111411.5156\n",
      "Epoch:190, P:None, Loss:66.6788, Loss-ae:66.6788, Loss-topo:110569.3359\n",
      "Epoch:191, P:None, Loss:66.1864, Loss-ae:66.1864, Loss-topo:111468.9062\n",
      "Epoch:192, P:None, Loss:66.2195, Loss-ae:66.2195, Loss-topo:107118.6781\n",
      "Epoch:193, P:None, Loss:66.2221, Loss-ae:66.2221, Loss-topo:100515.7453\n",
      "Epoch:194, P:None, Loss:66.2890, Loss-ae:66.2890, Loss-topo:109793.9016\n",
      "Epoch:195, P:None, Loss:66.6305, Loss-ae:66.6305, Loss-topo:107267.7266\n",
      "Epoch:196, P:None, Loss:66.3814, Loss-ae:66.3814, Loss-topo:110577.3062\n",
      "Epoch:197, P:None, Loss:66.4843, Loss-ae:66.4843, Loss-topo:101910.5750\n",
      "Epoch:198, P:None, Loss:66.2503, Loss-ae:66.2503, Loss-topo:109594.7500\n",
      "Epoch:199, P:None, Loss:65.6411, Loss-ae:65.6411, Loss-topo:104933.6422\n",
      "Epoch:200, P:None, Loss:65.7864, Loss-ae:65.7864, Loss-topo:115638.2953\n",
      "Epoch:201, P:None, Loss:66.1773, Loss-ae:66.1773, Loss-topo:105949.4062\n",
      "Epoch:202, P:None, Loss:66.2611, Loss-ae:66.2611, Loss-topo:107963.3453\n",
      "Epoch:203, P:None, Loss:65.7433, Loss-ae:65.7433, Loss-topo:106576.1141\n",
      "Epoch:204, P:None, Loss:66.6267, Loss-ae:66.6267, Loss-topo:117050.2406\n",
      "Epoch:205, P:None, Loss:67.8288, Loss-ae:67.8288, Loss-topo:110218.4328\n",
      "Epoch:206, P:None, Loss:65.8825, Loss-ae:65.8825, Loss-topo:104055.0922\n",
      "Epoch:207, P:None, Loss:65.8831, Loss-ae:65.8831, Loss-topo:111554.0016\n",
      "Epoch:208, P:None, Loss:65.7221, Loss-ae:65.7221, Loss-topo:112397.5125\n",
      "Epoch:209, P:None, Loss:65.3989, Loss-ae:65.3989, Loss-topo:107185.3844\n",
      "Epoch:210, P:None, Loss:65.7001, Loss-ae:65.7001, Loss-topo:104713.1609\n",
      "Epoch:211, P:None, Loss:65.8713, Loss-ae:65.8713, Loss-topo:121748.7125\n",
      "Epoch:212, P:None, Loss:65.8468, Loss-ae:65.8468, Loss-topo:105153.4187\n",
      "Epoch:213, P:None, Loss:65.7912, Loss-ae:65.7912, Loss-topo:112250.4812\n",
      "Epoch:214, P:None, Loss:67.1527, Loss-ae:67.1527, Loss-topo:108316.5375\n",
      "Epoch:215, P:None, Loss:65.4814, Loss-ae:65.4814, Loss-topo:107150.4469\n",
      "Epoch:216, P:None, Loss:65.4967, Loss-ae:65.4967, Loss-topo:113813.7047\n",
      "Epoch:217, P:None, Loss:65.8347, Loss-ae:65.8347, Loss-topo:111008.7094\n",
      "Epoch:218, P:None, Loss:66.0986, Loss-ae:66.0986, Loss-topo:110141.2359\n",
      "Epoch:219, P:None, Loss:66.1223, Loss-ae:66.1223, Loss-topo:112659.4766\n",
      "Epoch:220, P:None, Loss:65.6966, Loss-ae:65.6966, Loss-topo:105093.2109\n",
      "Epoch:221, P:None, Loss:66.1679, Loss-ae:66.1679, Loss-topo:109959.5516\n",
      "Epoch:222, P:None, Loss:66.2158, Loss-ae:66.2158, Loss-topo:111754.4438\n",
      "Epoch:223, P:None, Loss:65.4839, Loss-ae:65.4839, Loss-topo:111047.9359\n",
      "Epoch:224, P:None, Loss:65.9251, Loss-ae:65.9251, Loss-topo:119738.9500\n",
      "Epoch:225, P:None, Loss:65.9675, Loss-ae:65.9675, Loss-topo:115732.2984\n",
      "Epoch:226, P:None, Loss:65.6084, Loss-ae:65.6084, Loss-topo:107237.1906\n",
      "Epoch:227, P:None, Loss:65.3040, Loss-ae:65.3040, Loss-topo:105023.1062\n",
      "Epoch:228, P:None, Loss:66.1443, Loss-ae:66.1443, Loss-topo:113089.7188\n",
      "Epoch:229, P:None, Loss:65.6347, Loss-ae:65.6347, Loss-topo:110982.2469\n",
      "Epoch:230, P:None, Loss:66.1307, Loss-ae:66.1307, Loss-topo:106194.4250\n",
      "Epoch:231, P:None, Loss:65.1292, Loss-ae:65.1292, Loss-topo:116356.9656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:232, P:None, Loss:65.3798, Loss-ae:65.3798, Loss-topo:111554.2266\n",
      "Epoch:233, P:None, Loss:65.3365, Loss-ae:65.3365, Loss-topo:113500.3859\n",
      "Epoch:234, P:None, Loss:65.8546, Loss-ae:65.8546, Loss-topo:112457.6766\n",
      "Epoch:235, P:None, Loss:65.9074, Loss-ae:65.9074, Loss-topo:111311.3953\n",
      "Epoch:236, P:None, Loss:65.8993, Loss-ae:65.8993, Loss-topo:114386.9703\n",
      "Epoch:237, P:None, Loss:65.4541, Loss-ae:65.4541, Loss-topo:117211.0469\n",
      "Epoch:238, P:None, Loss:65.7872, Loss-ae:65.7872, Loss-topo:108617.3344\n",
      "Epoch:239, P:None, Loss:65.4958, Loss-ae:65.4958, Loss-topo:115821.1859\n",
      "Epoch:240, P:None, Loss:65.3089, Loss-ae:65.3089, Loss-topo:110318.3188\n",
      "Epoch:241, P:None, Loss:65.6418, Loss-ae:65.6418, Loss-topo:112111.5297\n",
      "Epoch:242, P:None, Loss:65.0867, Loss-ae:65.0867, Loss-topo:115296.7031\n",
      "Epoch:243, P:None, Loss:65.6320, Loss-ae:65.6320, Loss-topo:120093.1875\n",
      "Epoch:244, P:None, Loss:65.0977, Loss-ae:65.0977, Loss-topo:119604.4734\n",
      "Epoch:245, P:None, Loss:66.0088, Loss-ae:66.0088, Loss-topo:111980.2875\n",
      "Epoch:246, P:None, Loss:65.4723, Loss-ae:65.4723, Loss-topo:120066.4656\n",
      "Epoch:247, P:None, Loss:65.4897, Loss-ae:65.4897, Loss-topo:107479.3297\n",
      "Epoch:248, P:None, Loss:65.9390, Loss-ae:65.9390, Loss-topo:113013.4594\n",
      "Epoch:249, P:None, Loss:64.8475, Loss-ae:64.8475, Loss-topo:103209.0047\n",
      "Epoch:250, P:None, Loss:65.0336, Loss-ae:65.0336, Loss-topo:112369.1156\n",
      "Epoch:251, P:None, Loss:64.9801, Loss-ae:64.9801, Loss-topo:110729.2016\n",
      "Epoch:252, P:None, Loss:65.7706, Loss-ae:65.7706, Loss-topo:111418.5656\n",
      "Epoch:253, P:None, Loss:65.9049, Loss-ae:65.9049, Loss-topo:114224.5063\n",
      "Epoch:254, P:None, Loss:65.0532, Loss-ae:65.0532, Loss-topo:107952.1313\n",
      "Epoch:255, P:None, Loss:65.8518, Loss-ae:65.8518, Loss-topo:115911.5781\n",
      "Epoch:256, P:None, Loss:64.6572, Loss-ae:64.6572, Loss-topo:113640.8500\n",
      "Epoch:257, P:None, Loss:65.2721, Loss-ae:65.2721, Loss-topo:117797.9844\n",
      "Epoch:258, P:None, Loss:65.3803, Loss-ae:65.3803, Loss-topo:114697.4109\n",
      "Epoch:259, P:None, Loss:65.1800, Loss-ae:65.1800, Loss-topo:116463.6391\n",
      "Epoch:260, P:None, Loss:65.1010, Loss-ae:65.1010, Loss-topo:103660.2758\n",
      "Epoch:261, P:None, Loss:65.0476, Loss-ae:65.0476, Loss-topo:111367.1359\n",
      "Epoch:262, P:None, Loss:65.7801, Loss-ae:65.7801, Loss-topo:111846.3141\n",
      "Epoch:263, P:None, Loss:65.3899, Loss-ae:65.3899, Loss-topo:117277.9234\n",
      "Epoch:264, P:None, Loss:64.9694, Loss-ae:64.9694, Loss-topo:113292.7063\n",
      "Epoch:265, P:None, Loss:64.8637, Loss-ae:64.8637, Loss-topo:118594.9391\n",
      "Epoch:266, P:None, Loss:65.3307, Loss-ae:65.3307, Loss-topo:117116.0375\n",
      "Epoch:267, P:None, Loss:64.6041, Loss-ae:64.6041, Loss-topo:118560.6141\n",
      "Epoch:268, P:None, Loss:65.2207, Loss-ae:65.2207, Loss-topo:110320.6031\n",
      "Epoch:269, P:None, Loss:65.4777, Loss-ae:65.4777, Loss-topo:115685.6156\n",
      "Epoch:270, P:None, Loss:65.0007, Loss-ae:65.0007, Loss-topo:113566.7203\n",
      "Epoch:271, P:None, Loss:64.4004, Loss-ae:64.4004, Loss-topo:112214.3445\n",
      "Epoch:272, P:None, Loss:65.9840, Loss-ae:65.9840, Loss-topo:114651.2203\n",
      "Epoch:273, P:None, Loss:65.9667, Loss-ae:65.9667, Loss-topo:113645.5406\n",
      "Epoch:274, P:None, Loss:64.0956, Loss-ae:64.0956, Loss-topo:103929.1938\n",
      "Epoch:275, P:None, Loss:65.3391, Loss-ae:65.3391, Loss-topo:117533.2094\n",
      "Epoch:276, P:None, Loss:64.4961, Loss-ae:64.4961, Loss-topo:112676.9250\n",
      "Epoch:277, P:None, Loss:65.2428, Loss-ae:65.2428, Loss-topo:110283.6609\n",
      "Epoch:278, P:None, Loss:64.6971, Loss-ae:64.6971, Loss-topo:118494.4234\n",
      "Epoch:279, P:None, Loss:64.9815, Loss-ae:64.9815, Loss-topo:123367.3375\n",
      "Epoch:280, P:None, Loss:65.9882, Loss-ae:65.9882, Loss-topo:120903.3531\n",
      "Epoch:281, P:None, Loss:65.2258, Loss-ae:65.2258, Loss-topo:113682.1391\n",
      "Epoch:282, P:None, Loss:65.4956, Loss-ae:65.4956, Loss-topo:113995.4297\n",
      "Epoch:283, P:None, Loss:65.5079, Loss-ae:65.5079, Loss-topo:124558.1047\n",
      "Epoch:284, P:None, Loss:66.1826, Loss-ae:66.1826, Loss-topo:113476.1359\n",
      "Epoch:285, P:None, Loss:65.2965, Loss-ae:65.2965, Loss-topo:120021.1141\n",
      "Epoch:286, P:None, Loss:65.1876, Loss-ae:65.1876, Loss-topo:121998.5063\n",
      "Epoch:287, P:None, Loss:64.3976, Loss-ae:64.3976, Loss-topo:115814.1984\n",
      "Epoch:288, P:None, Loss:65.4716, Loss-ae:65.4716, Loss-topo:125028.8859\n",
      "Epoch:289, P:None, Loss:65.3667, Loss-ae:65.3667, Loss-topo:108560.4875\n",
      "Epoch:290, P:None, Loss:65.0212, Loss-ae:65.0212, Loss-topo:116988.5984\n",
      "Epoch:291, P:None, Loss:64.1844, Loss-ae:64.1844, Loss-topo:119675.3266\n",
      "Epoch:292, P:None, Loss:64.9356, Loss-ae:64.9356, Loss-topo:118610.5969\n",
      "Epoch:293, P:None, Loss:64.5715, Loss-ae:64.5715, Loss-topo:111906.4703\n",
      "Epoch:294, P:None, Loss:65.3416, Loss-ae:65.3416, Loss-topo:126226.7312\n",
      "Epoch:295, P:None, Loss:65.2233, Loss-ae:65.2233, Loss-topo:125103.3031\n",
      "Epoch:296, P:None, Loss:66.3765, Loss-ae:66.3765, Loss-topo:120733.1547\n",
      "Epoch:297, P:None, Loss:65.6388, Loss-ae:65.6388, Loss-topo:114436.0469\n",
      "Epoch:298, P:None, Loss:65.6993, Loss-ae:65.6993, Loss-topo:112754.8625\n",
      "Epoch:299, P:None, Loss:66.4358, Loss-ae:66.4358, Loss-topo:117457.0266\n",
      "Epoch:300, P:None, Loss:65.2265, Loss-ae:65.2265, Loss-topo:116846.9500\n",
      "Epoch:301, P:None, Loss:64.4836, Loss-ae:64.4836, Loss-topo:116644.4531\n",
      "Epoch:302, P:None, Loss:64.7565, Loss-ae:64.7565, Loss-topo:121745.7031\n",
      "Epoch:303, P:None, Loss:64.5486, Loss-ae:64.5486, Loss-topo:116274.3969\n",
      "Epoch:304, P:None, Loss:64.1508, Loss-ae:64.1508, Loss-topo:114517.0406\n",
      "Epoch:305, P:None, Loss:64.8465, Loss-ae:64.8465, Loss-topo:114990.8422\n",
      "Epoch:306, P:None, Loss:65.1567, Loss-ae:65.1567, Loss-topo:115501.1516\n",
      "Epoch:307, P:None, Loss:64.6492, Loss-ae:64.6492, Loss-topo:118525.9812\n",
      "Epoch:308, P:None, Loss:65.5081, Loss-ae:65.5081, Loss-topo:119324.2781\n",
      "Epoch:309, P:None, Loss:64.8627, Loss-ae:64.8627, Loss-topo:118897.4953\n",
      "Epoch:310, P:None, Loss:65.2773, Loss-ae:65.2773, Loss-topo:129761.3125\n",
      "Epoch:311, P:None, Loss:64.0313, Loss-ae:64.0313, Loss-topo:104336.9898\n",
      "Epoch:312, P:None, Loss:65.2876, Loss-ae:65.2876, Loss-topo:120132.4016\n",
      "Epoch:313, P:None, Loss:64.5175, Loss-ae:64.5175, Loss-topo:119647.5156\n",
      "Epoch:314, P:None, Loss:65.2773, Loss-ae:65.2773, Loss-topo:122202.6734\n",
      "Epoch:315, P:None, Loss:64.7760, Loss-ae:64.7760, Loss-topo:121614.7563\n",
      "Epoch:316, P:None, Loss:64.6602, Loss-ae:64.6602, Loss-topo:124794.7734\n",
      "Epoch:317, P:None, Loss:64.3571, Loss-ae:64.3571, Loss-topo:119673.1000\n",
      "Epoch:318, P:None, Loss:64.9991, Loss-ae:64.9991, Loss-topo:119872.9047\n",
      "Epoch:319, P:None, Loss:65.2234, Loss-ae:65.2234, Loss-topo:110939.6828\n",
      "Epoch:320, P:None, Loss:64.8635, Loss-ae:64.8635, Loss-topo:112212.9906\n",
      "Epoch:321, P:None, Loss:65.4483, Loss-ae:65.4483, Loss-topo:125212.1578\n",
      "Epoch:322, P:None, Loss:65.4523, Loss-ae:65.4523, Loss-topo:118410.1734\n",
      "Epoch:323, P:None, Loss:64.8077, Loss-ae:64.8077, Loss-topo:122579.9797\n",
      "Epoch:324, P:None, Loss:65.0671, Loss-ae:65.0671, Loss-topo:111521.2922\n",
      "Epoch:325, P:None, Loss:65.2501, Loss-ae:65.2501, Loss-topo:121439.7922\n",
      "Epoch:326, P:None, Loss:65.7120, Loss-ae:65.7120, Loss-topo:112377.4078\n",
      "Epoch:327, P:None, Loss:64.8443, Loss-ae:64.8443, Loss-topo:118849.9781\n",
      "Epoch:328, P:None, Loss:66.0946, Loss-ae:66.0946, Loss-topo:107099.7547\n",
      "Epoch:329, P:None, Loss:65.4188, Loss-ae:65.4188, Loss-topo:121107.5375\n",
      "Epoch:330, P:None, Loss:65.6299, Loss-ae:65.6299, Loss-topo:113799.2406\n",
      "Epoch:331, P:None, Loss:65.6219, Loss-ae:65.6219, Loss-topo:121273.8578\n",
      "Epoch:332, P:None, Loss:64.4837, Loss-ae:64.4837, Loss-topo:115265.6547\n",
      "Epoch:333, P:None, Loss:65.3785, Loss-ae:65.3785, Loss-topo:119758.9781\n",
      "Epoch:334, P:None, Loss:64.8497, Loss-ae:64.8497, Loss-topo:119288.3859\n",
      "Epoch:335, P:None, Loss:64.7709, Loss-ae:64.7709, Loss-topo:114689.4250\n",
      "Epoch:336, P:None, Loss:64.8350, Loss-ae:64.8350, Loss-topo:123385.1766\n",
      "Epoch:337, P:None, Loss:64.7952, Loss-ae:64.7952, Loss-topo:119660.5953\n",
      "Epoch:338, P:None, Loss:65.1068, Loss-ae:65.1068, Loss-topo:117618.3953\n",
      "Epoch:339, P:None, Loss:65.0661, Loss-ae:65.0661, Loss-topo:123885.5578\n",
      "Epoch:340, P:None, Loss:64.6181, Loss-ae:64.6181, Loss-topo:124500.4500\n",
      "Epoch:341, P:None, Loss:65.2823, Loss-ae:65.2823, Loss-topo:120745.6141\n",
      "Epoch:342, P:None, Loss:65.0899, Loss-ae:65.0899, Loss-topo:123018.8188\n",
      "Epoch:343, P:None, Loss:64.5326, Loss-ae:64.5326, Loss-topo:115728.9766\n",
      "Epoch:344, P:None, Loss:64.9686, Loss-ae:64.9686, Loss-topo:111383.1547\n",
      "Epoch:345, P:None, Loss:64.9818, Loss-ae:64.9818, Loss-topo:118211.2188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:346, P:None, Loss:65.0910, Loss-ae:65.0910, Loss-topo:124389.7672\n",
      "Epoch:347, P:None, Loss:64.9846, Loss-ae:64.9846, Loss-topo:114163.0469\n",
      "Epoch:348, P:None, Loss:64.8417, Loss-ae:64.8417, Loss-topo:118313.7547\n",
      "Epoch:349, P:None, Loss:64.5378, Loss-ae:64.5378, Loss-topo:118052.9500\n",
      "Epoch:350, P:None, Loss:65.0486, Loss-ae:65.0486, Loss-topo:123959.2937\n",
      "Epoch:351, P:None, Loss:65.3947, Loss-ae:65.3947, Loss-topo:125259.6469\n",
      "Epoch:352, P:None, Loss:64.9802, Loss-ae:64.9802, Loss-topo:117536.3469\n",
      "Epoch:353, P:None, Loss:65.3115, Loss-ae:65.3115, Loss-topo:120357.2250\n",
      "Epoch:354, P:None, Loss:65.4324, Loss-ae:65.4324, Loss-topo:119429.6406\n",
      "Epoch:355, P:None, Loss:64.7345, Loss-ae:64.7345, Loss-topo:109836.0016\n",
      "Epoch:356, P:None, Loss:65.4051, Loss-ae:65.4051, Loss-topo:118210.7141\n",
      "Epoch:357, P:None, Loss:64.7314, Loss-ae:64.7314, Loss-topo:113109.5750\n",
      "Epoch:358, P:None, Loss:64.8983, Loss-ae:64.8983, Loss-topo:125749.5469\n",
      "Epoch:359, P:None, Loss:65.1122, Loss-ae:65.1122, Loss-topo:118156.4406\n",
      "Epoch:360, P:None, Loss:64.8068, Loss-ae:64.8068, Loss-topo:118620.4906\n",
      "Epoch:361, P:None, Loss:64.4050, Loss-ae:64.4050, Loss-topo:113990.9453\n",
      "Epoch:362, P:None, Loss:64.7828, Loss-ae:64.7828, Loss-topo:116609.2891\n",
      "Epoch:363, P:None, Loss:64.9902, Loss-ae:64.9902, Loss-topo:127638.4625\n",
      "Epoch:364, P:None, Loss:64.1243, Loss-ae:64.1243, Loss-topo:112105.2375\n",
      "Epoch:365, P:None, Loss:65.8140, Loss-ae:65.8140, Loss-topo:117793.4828\n",
      "Epoch:366, P:None, Loss:66.2745, Loss-ae:66.2745, Loss-topo:115339.0516\n",
      "Epoch:367, P:None, Loss:65.2522, Loss-ae:65.2522, Loss-topo:116602.0250\n",
      "Epoch:368, P:None, Loss:64.3695, Loss-ae:64.3695, Loss-topo:121673.4031\n",
      "Epoch:369, P:None, Loss:65.2330, Loss-ae:65.2330, Loss-topo:124270.2906\n",
      "Epoch:370, P:None, Loss:65.2805, Loss-ae:65.2805, Loss-topo:117755.2203\n",
      "Epoch:371, P:None, Loss:64.9878, Loss-ae:64.9878, Loss-topo:119918.7641\n",
      "Epoch:372, P:None, Loss:65.3612, Loss-ae:65.3612, Loss-topo:118936.3234\n",
      "Epoch:373, P:None, Loss:64.7784, Loss-ae:64.7784, Loss-topo:115572.6750\n",
      "Epoch:374, P:None, Loss:64.5095, Loss-ae:64.5095, Loss-topo:119642.3906\n",
      "Epoch:375, P:None, Loss:64.4088, Loss-ae:64.4088, Loss-topo:124417.3578\n",
      "Epoch:376, P:None, Loss:64.7485, Loss-ae:64.7485, Loss-topo:129948.3297\n",
      "Epoch:377, P:None, Loss:64.8546, Loss-ae:64.8546, Loss-topo:126283.4719\n",
      "Epoch:378, P:None, Loss:64.6533, Loss-ae:64.6533, Loss-topo:117097.7922\n",
      "Epoch:379, P:None, Loss:65.0311, Loss-ae:65.0311, Loss-topo:117593.6016\n",
      "Epoch:380, P:None, Loss:65.7723, Loss-ae:65.7723, Loss-topo:116686.8750\n",
      "Epoch:381, P:None, Loss:64.6866, Loss-ae:64.6866, Loss-topo:114556.0750\n",
      "Epoch:382, P:None, Loss:65.0706, Loss-ae:65.0706, Loss-topo:121870.9672\n",
      "Epoch:383, P:None, Loss:65.4978, Loss-ae:65.4978, Loss-topo:117470.1156\n",
      "Epoch:384, P:None, Loss:65.1203, Loss-ae:65.1203, Loss-topo:118193.8250\n",
      "Epoch:385, P:None, Loss:65.1470, Loss-ae:65.1470, Loss-topo:116236.5797\n",
      "Epoch:386, P:None, Loss:64.9904, Loss-ae:64.9904, Loss-topo:121290.1938\n",
      "Epoch:387, P:None, Loss:64.4694, Loss-ae:64.4694, Loss-topo:114923.6359\n",
      "Epoch:388, P:None, Loss:65.3461, Loss-ae:65.3461, Loss-topo:121336.6609\n",
      "Epoch:389, P:None, Loss:65.2550, Loss-ae:65.2550, Loss-topo:109008.7688\n",
      "Epoch:390, P:None, Loss:65.0786, Loss-ae:65.0786, Loss-topo:119350.8234\n",
      "Epoch:391, P:None, Loss:64.8896, Loss-ae:64.8896, Loss-topo:121768.7219\n",
      "Epoch:392, P:None, Loss:64.7389, Loss-ae:64.7389, Loss-topo:122148.5828\n",
      "Epoch:393, P:None, Loss:65.4072, Loss-ae:65.4072, Loss-topo:124918.0688\n",
      "Epoch:394, P:None, Loss:65.6691, Loss-ae:65.6691, Loss-topo:130498.8234\n",
      "Epoch:395, P:None, Loss:65.3797, Loss-ae:65.3797, Loss-topo:115345.1453\n",
      "Epoch:396, P:None, Loss:64.8926, Loss-ae:64.8926, Loss-topo:115142.3891\n",
      "Epoch:397, P:None, Loss:64.6001, Loss-ae:64.6001, Loss-topo:117818.8313\n",
      "Epoch:398, P:None, Loss:64.0254, Loss-ae:64.0254, Loss-topo:122942.2875\n",
      "Epoch:399, P:None, Loss:64.8687, Loss-ae:64.8687, Loss-topo:124890.3062\n",
      "Epoch:400, P:None, Loss:65.1154, Loss-ae:65.1154, Loss-topo:118412.6344\n",
      "Epoch:401, P:None, Loss:65.2309, Loss-ae:65.2309, Loss-topo:126631.5094\n",
      "Epoch:402, P:None, Loss:65.3865, Loss-ae:65.3865, Loss-topo:112923.7188\n",
      "Epoch:403, P:None, Loss:65.8833, Loss-ae:65.8833, Loss-topo:118418.0859\n",
      "Epoch:404, P:None, Loss:65.4865, Loss-ae:65.4865, Loss-topo:115883.4031\n",
      "Epoch:405, P:None, Loss:64.6158, Loss-ae:64.6158, Loss-topo:124634.1047\n",
      "Epoch:406, P:None, Loss:66.0296, Loss-ae:66.0296, Loss-topo:121645.7234\n",
      "Epoch:407, P:None, Loss:64.8210, Loss-ae:64.8210, Loss-topo:113832.8719\n",
      "Epoch:408, P:None, Loss:66.0241, Loss-ae:66.0241, Loss-topo:126238.7063\n",
      "Epoch:409, P:None, Loss:65.5197, Loss-ae:65.5197, Loss-topo:109077.0922\n",
      "Epoch:410, P:None, Loss:64.3023, Loss-ae:64.3023, Loss-topo:110278.0484\n",
      "Epoch:411, P:None, Loss:65.1914, Loss-ae:65.1914, Loss-topo:120114.0312\n",
      "Epoch:412, P:None, Loss:64.7895, Loss-ae:64.7895, Loss-topo:114618.8078\n",
      "Epoch:413, P:None, Loss:64.8308, Loss-ae:64.8308, Loss-topo:126852.5609\n",
      "Epoch:414, P:None, Loss:65.0151, Loss-ae:65.0151, Loss-topo:116378.2766\n",
      "Epoch:415, P:None, Loss:65.6469, Loss-ae:65.6469, Loss-topo:117418.7703\n",
      "Epoch:416, P:None, Loss:65.2362, Loss-ae:65.2362, Loss-topo:118810.9000\n",
      "Epoch:417, P:None, Loss:65.5201, Loss-ae:65.5201, Loss-topo:124564.5000\n",
      "Epoch:418, P:None, Loss:65.7588, Loss-ae:65.7588, Loss-topo:113353.2516\n",
      "Epoch:419, P:None, Loss:65.4279, Loss-ae:65.4279, Loss-topo:114995.8422\n",
      "Epoch:420, P:None, Loss:65.6412, Loss-ae:65.6412, Loss-topo:120542.4766\n",
      "Epoch:421, P:None, Loss:64.7324, Loss-ae:64.7324, Loss-topo:117059.4625\n",
      "Epoch:422, P:None, Loss:64.7149, Loss-ae:64.7149, Loss-topo:121299.5922\n",
      "Epoch:423, P:None, Loss:64.2874, Loss-ae:64.2874, Loss-topo:119178.7078\n",
      "Epoch:424, P:None, Loss:65.1272, Loss-ae:65.1272, Loss-topo:115099.6969\n",
      "Epoch:425, P:None, Loss:66.3395, Loss-ae:66.3395, Loss-topo:126386.2844\n",
      "Epoch:426, P:None, Loss:64.8164, Loss-ae:64.8164, Loss-topo:116889.7844\n",
      "Epoch:427, P:None, Loss:64.9359, Loss-ae:64.9359, Loss-topo:116859.3016\n",
      "Epoch:428, P:None, Loss:64.8430, Loss-ae:64.8430, Loss-topo:117698.5969\n",
      "Epoch:429, P:None, Loss:64.9051, Loss-ae:64.9051, Loss-topo:117975.1578\n",
      "Epoch:430, P:None, Loss:65.2009, Loss-ae:65.2009, Loss-topo:123586.9984\n",
      "Epoch:431, P:None, Loss:65.0951, Loss-ae:65.0951, Loss-topo:122143.2516\n",
      "Epoch:432, P:None, Loss:65.4971, Loss-ae:65.4971, Loss-topo:115433.0875\n",
      "Epoch:433, P:None, Loss:65.1471, Loss-ae:65.1471, Loss-topo:117335.6047\n",
      "Epoch:434, P:None, Loss:65.0910, Loss-ae:65.0910, Loss-topo:119892.4172\n",
      "Epoch:435, P:None, Loss:65.4105, Loss-ae:65.4105, Loss-topo:119745.5891\n",
      "Epoch:436, P:None, Loss:64.7800, Loss-ae:64.7800, Loss-topo:116372.1641\n",
      "Epoch:437, P:None, Loss:66.0648, Loss-ae:66.0648, Loss-topo:117761.1703\n",
      "Epoch:438, P:None, Loss:65.4487, Loss-ae:65.4487, Loss-topo:124100.8328\n",
      "Epoch:439, P:None, Loss:65.5560, Loss-ae:65.5560, Loss-topo:113692.2594\n",
      "Epoch:440, P:None, Loss:65.9980, Loss-ae:65.9980, Loss-topo:124667.1625\n",
      "Epoch:441, P:None, Loss:65.8476, Loss-ae:65.8476, Loss-topo:121705.6922\n",
      "Epoch:442, P:None, Loss:65.6337, Loss-ae:65.6337, Loss-topo:118326.3938\n",
      "Epoch:443, P:None, Loss:64.9819, Loss-ae:64.9819, Loss-topo:122595.3375\n",
      "Epoch:444, P:None, Loss:65.6622, Loss-ae:65.6622, Loss-topo:121477.3531\n",
      "Epoch:445, P:None, Loss:65.5514, Loss-ae:65.5514, Loss-topo:126240.7250\n",
      "Epoch:446, P:None, Loss:65.8245, Loss-ae:65.8245, Loss-topo:113596.3625\n",
      "Epoch:447, P:None, Loss:65.4458, Loss-ae:65.4458, Loss-topo:119634.4109\n",
      "Epoch:448, P:None, Loss:65.9627, Loss-ae:65.9627, Loss-topo:119639.9516\n",
      "Epoch:449, P:None, Loss:65.4572, Loss-ae:65.4572, Loss-topo:112921.9078\n",
      "Epoch:450, P:None, Loss:65.0080, Loss-ae:65.0080, Loss-topo:122126.6266\n",
      "Epoch:451, P:None, Loss:64.8074, Loss-ae:64.8074, Loss-topo:123998.7063\n",
      "Epoch:452, P:None, Loss:64.5733, Loss-ae:64.5733, Loss-topo:116821.3703\n",
      "Epoch:453, P:None, Loss:65.2643, Loss-ae:65.2643, Loss-topo:123527.2266\n",
      "Epoch:454, P:None, Loss:65.2735, Loss-ae:65.2735, Loss-topo:115177.7984\n",
      "Epoch:455, P:None, Loss:65.0136, Loss-ae:65.0136, Loss-topo:117316.6969\n",
      "Epoch:456, P:None, Loss:65.1396, Loss-ae:65.1396, Loss-topo:118277.1844\n",
      "Epoch:457, P:None, Loss:65.2439, Loss-ae:65.2439, Loss-topo:113999.1266\n",
      "Epoch:458, P:None, Loss:65.1501, Loss-ae:65.1501, Loss-topo:125519.6938\n",
      "Epoch:459, P:None, Loss:65.2854, Loss-ae:65.2854, Loss-topo:118148.1438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:460, P:None, Loss:65.6165, Loss-ae:65.6165, Loss-topo:123263.7625\n",
      "Epoch:461, P:None, Loss:65.2269, Loss-ae:65.2269, Loss-topo:119057.2391\n",
      "Epoch:462, P:None, Loss:65.2667, Loss-ae:65.2667, Loss-topo:115843.3000\n",
      "Epoch:463, P:None, Loss:65.1758, Loss-ae:65.1758, Loss-topo:119813.3266\n",
      "Epoch:464, P:None, Loss:65.3696, Loss-ae:65.3696, Loss-topo:119161.9906\n",
      "Epoch:465, P:None, Loss:65.7606, Loss-ae:65.7606, Loss-topo:124863.3297\n",
      "Epoch:466, P:None, Loss:65.3999, Loss-ae:65.3999, Loss-topo:122093.4875\n",
      "Epoch:467, P:None, Loss:65.6780, Loss-ae:65.6780, Loss-topo:124010.1844\n",
      "Epoch:468, P:None, Loss:66.2647, Loss-ae:66.2647, Loss-topo:120881.6109\n",
      "Epoch:469, P:None, Loss:66.7291, Loss-ae:66.7291, Loss-topo:121292.1359\n",
      "Epoch:470, P:None, Loss:65.3044, Loss-ae:65.3044, Loss-topo:127343.0344\n",
      "Epoch:471, P:None, Loss:64.7390, Loss-ae:64.7390, Loss-topo:124164.9344\n",
      "Epoch:472, P:None, Loss:65.1681, Loss-ae:65.1681, Loss-topo:118533.8906\n",
      "Epoch:473, P:None, Loss:65.1169, Loss-ae:65.1169, Loss-topo:121206.9078\n",
      "Epoch:474, P:None, Loss:65.5416, Loss-ae:65.5416, Loss-topo:118175.4812\n",
      "Epoch:475, P:None, Loss:66.0723, Loss-ae:66.0723, Loss-topo:127539.7359\n",
      "Epoch:476, P:None, Loss:65.4400, Loss-ae:65.4400, Loss-topo:121288.1266\n",
      "Epoch:477, P:None, Loss:65.9256, Loss-ae:65.9256, Loss-topo:116301.9266\n",
      "Epoch:478, P:None, Loss:65.4245, Loss-ae:65.4245, Loss-topo:114191.9937\n",
      "Epoch:479, P:None, Loss:66.2158, Loss-ae:66.2158, Loss-topo:119680.6828\n",
      "Epoch:480, P:None, Loss:65.1564, Loss-ae:65.1564, Loss-topo:114147.8000\n",
      "Epoch:481, P:None, Loss:65.1899, Loss-ae:65.1899, Loss-topo:125546.9344\n",
      "Epoch:482, P:None, Loss:65.7745, Loss-ae:65.7745, Loss-topo:120591.0281\n",
      "Epoch:483, P:None, Loss:65.9721, Loss-ae:65.9721, Loss-topo:118543.5266\n",
      "Epoch:484, P:None, Loss:65.2059, Loss-ae:65.2059, Loss-topo:121539.4625\n",
      "Epoch:485, P:None, Loss:65.3753, Loss-ae:65.3753, Loss-topo:110677.6859\n",
      "Epoch:486, P:None, Loss:65.1104, Loss-ae:65.1104, Loss-topo:115241.5625\n",
      "Epoch:487, P:None, Loss:66.3048, Loss-ae:66.3048, Loss-topo:119662.8109\n",
      "Epoch:488, P:None, Loss:66.3536, Loss-ae:66.3536, Loss-topo:123300.2672\n",
      "Epoch:489, P:None, Loss:65.2545, Loss-ae:65.2545, Loss-topo:112264.3062\n",
      "Epoch:490, P:None, Loss:65.6670, Loss-ae:65.6670, Loss-topo:116831.9656\n",
      "Epoch:491, P:None, Loss:65.3868, Loss-ae:65.3868, Loss-topo:112214.0125\n",
      "Epoch:492, P:None, Loss:66.6682, Loss-ae:66.6682, Loss-topo:123734.8125\n",
      "Epoch:493, P:None, Loss:66.5782, Loss-ae:66.5782, Loss-topo:121194.3156\n",
      "Epoch:494, P:None, Loss:66.1637, Loss-ae:66.1637, Loss-topo:122289.2828\n",
      "Epoch:495, P:None, Loss:66.1373, Loss-ae:66.1373, Loss-topo:118653.9594\n",
      "Epoch:496, P:None, Loss:65.5225, Loss-ae:65.5225, Loss-topo:125361.1719\n",
      "Epoch:497, P:None, Loss:65.6112, Loss-ae:65.6112, Loss-topo:118239.4109\n",
      "Epoch:498, P:None, Loss:64.9304, Loss-ae:64.9304, Loss-topo:121776.1109\n",
      "Epoch:499, P:None, Loss:65.9268, Loss-ae:65.9268, Loss-topo:121521.8313\n",
      "Epoch:500, P:None, Loss:64.8510, Loss-ae:64.8510, Loss-topo:113896.7563\n",
      "Epoch:501, P:None, Loss:65.9490, Loss-ae:65.9490, Loss-topo:119114.6375\n",
      "Epoch:502, P:None, Loss:65.5939, Loss-ae:65.5939, Loss-topo:121059.0234\n",
      "Epoch:503, P:None, Loss:64.9127, Loss-ae:64.9127, Loss-topo:120128.8250\n",
      "Epoch:504, P:None, Loss:65.4971, Loss-ae:65.4971, Loss-topo:119274.1625\n",
      "Epoch:505, P:None, Loss:65.3440, Loss-ae:65.3440, Loss-topo:115487.0875\n",
      "Epoch:506, P:None, Loss:65.2405, Loss-ae:65.2405, Loss-topo:121331.0297\n",
      "Epoch:507, P:None, Loss:66.7507, Loss-ae:66.7507, Loss-topo:118848.7531\n",
      "Epoch:508, P:None, Loss:65.0501, Loss-ae:65.0501, Loss-topo:119747.3984\n",
      "Epoch:509, P:None, Loss:65.1871, Loss-ae:65.1871, Loss-topo:115427.5437\n",
      "Epoch:510, P:None, Loss:64.8946, Loss-ae:64.8946, Loss-topo:117453.3516\n",
      "Epoch:511, P:None, Loss:65.4792, Loss-ae:65.4792, Loss-topo:122137.2859\n",
      "Epoch:512, P:None, Loss:65.4568, Loss-ae:65.4568, Loss-topo:121333.0281\n",
      "Epoch:513, P:None, Loss:66.2385, Loss-ae:66.2385, Loss-topo:123161.3047\n",
      "Epoch:514, P:None, Loss:66.2980, Loss-ae:66.2980, Loss-topo:119594.6172\n",
      "Epoch:515, P:None, Loss:65.6599, Loss-ae:65.6599, Loss-topo:123149.0734\n",
      "Epoch:516, P:None, Loss:65.9844, Loss-ae:65.9844, Loss-topo:119537.6594\n",
      "Epoch:517, P:None, Loss:64.8848, Loss-ae:64.8848, Loss-topo:120296.5484\n",
      "Epoch:518, P:None, Loss:65.9725, Loss-ae:65.9725, Loss-topo:116945.5891\n",
      "Epoch:519, P:None, Loss:65.8370, Loss-ae:65.8370, Loss-topo:118134.2219\n",
      "Epoch:520, P:None, Loss:65.7188, Loss-ae:65.7188, Loss-topo:120473.7063\n",
      "Epoch:521, P:None, Loss:65.4322, Loss-ae:65.4322, Loss-topo:124261.4125\n",
      "Epoch:522, P:None, Loss:65.8061, Loss-ae:65.8061, Loss-topo:128287.7000\n",
      "Epoch:523, P:None, Loss:65.4738, Loss-ae:65.4738, Loss-topo:115807.2703\n",
      "Epoch:524, P:None, Loss:65.3227, Loss-ae:65.3227, Loss-topo:120602.3016\n",
      "Epoch:525, P:None, Loss:66.5570, Loss-ae:66.5570, Loss-topo:114096.9062\n",
      "Epoch:526, P:None, Loss:66.7195, Loss-ae:66.7195, Loss-topo:119907.9891\n",
      "Epoch:527, P:None, Loss:66.1419, Loss-ae:66.1419, Loss-topo:113317.7688\n",
      "Epoch:528, P:None, Loss:66.0080, Loss-ae:66.0080, Loss-topo:112220.9359\n",
      "Epoch:529, P:None, Loss:66.6304, Loss-ae:66.6304, Loss-topo:122978.6187\n",
      "Epoch:530, P:None, Loss:65.8686, Loss-ae:65.8686, Loss-topo:118823.4516\n",
      "Epoch:531, P:None, Loss:65.4579, Loss-ae:65.4579, Loss-topo:116621.2953\n",
      "Epoch:532, P:None, Loss:65.7097, Loss-ae:65.7097, Loss-topo:118712.1484\n",
      "Epoch:533, P:None, Loss:65.6234, Loss-ae:65.6234, Loss-topo:120203.0891\n",
      "Epoch:534, P:None, Loss:65.9479, Loss-ae:65.9479, Loss-topo:116111.6016\n",
      "Epoch:535, P:None, Loss:65.6323, Loss-ae:65.6323, Loss-topo:112793.2469\n",
      "Epoch:536, P:None, Loss:65.6615, Loss-ae:65.6615, Loss-topo:114845.4406\n",
      "Epoch:537, P:None, Loss:65.1694, Loss-ae:65.1694, Loss-topo:116031.9516\n",
      "Epoch:538, P:None, Loss:64.9480, Loss-ae:64.9480, Loss-topo:118212.9641\n",
      "Epoch:539, P:None, Loss:65.8139, Loss-ae:65.8139, Loss-topo:105043.4555\n",
      "Epoch:540, P:None, Loss:65.8040, Loss-ae:65.8040, Loss-topo:117815.4094\n",
      "Epoch:541, P:None, Loss:66.1201, Loss-ae:66.1201, Loss-topo:124501.6344\n",
      "Epoch:542, P:None, Loss:65.2127, Loss-ae:65.2127, Loss-topo:109102.6562\n",
      "Epoch:543, P:None, Loss:65.8751, Loss-ae:65.8751, Loss-topo:120902.0625\n",
      "Epoch:544, P:None, Loss:65.8350, Loss-ae:65.8350, Loss-topo:120829.3219\n",
      "Epoch:545, P:None, Loss:65.7148, Loss-ae:65.7148, Loss-topo:121899.9125\n",
      "Epoch:546, P:None, Loss:66.7186, Loss-ae:66.7186, Loss-topo:117654.7781\n",
      "Epoch:547, P:None, Loss:65.7319, Loss-ae:65.7319, Loss-topo:118188.3609\n",
      "Epoch:548, P:None, Loss:66.1151, Loss-ae:66.1151, Loss-topo:111073.6531\n",
      "Epoch:549, P:None, Loss:65.7707, Loss-ae:65.7707, Loss-topo:115781.9141\n",
      "Epoch:550, P:None, Loss:65.7621, Loss-ae:65.7621, Loss-topo:116378.3953\n",
      "Epoch:551, P:None, Loss:65.5454, Loss-ae:65.5454, Loss-topo:115561.4703\n",
      "Epoch:552, P:None, Loss:65.7103, Loss-ae:65.7103, Loss-topo:119890.9891\n",
      "Epoch:553, P:None, Loss:66.2939, Loss-ae:66.2939, Loss-topo:126195.2391\n",
      "Epoch:554, P:None, Loss:66.5321, Loss-ae:66.5321, Loss-topo:113838.1797\n",
      "Epoch:555, P:None, Loss:65.8628, Loss-ae:65.8628, Loss-topo:120696.4359\n",
      "Epoch:556, P:None, Loss:65.7767, Loss-ae:65.7767, Loss-topo:110960.6094\n",
      "Epoch:557, P:None, Loss:65.4029, Loss-ae:65.4029, Loss-topo:117634.3047\n",
      "Epoch:558, P:None, Loss:65.3503, Loss-ae:65.3503, Loss-topo:126418.2469\n",
      "Epoch:559, P:None, Loss:65.8826, Loss-ae:65.8826, Loss-topo:121469.9344\n",
      "Epoch:560, P:None, Loss:65.4465, Loss-ae:65.4465, Loss-topo:121581.1000\n",
      "Epoch:561, P:None, Loss:66.3745, Loss-ae:66.3745, Loss-topo:122170.6578\n",
      "Epoch:562, P:None, Loss:66.2262, Loss-ae:66.2262, Loss-topo:120097.8438\n",
      "Epoch:563, P:None, Loss:65.4483, Loss-ae:65.4483, Loss-topo:118206.7594\n",
      "Epoch:564, P:None, Loss:66.4687, Loss-ae:66.4687, Loss-topo:123471.4891\n",
      "Epoch:565, P:None, Loss:66.5574, Loss-ae:66.5574, Loss-topo:117916.1781\n",
      "Epoch:566, P:None, Loss:66.6560, Loss-ae:66.6560, Loss-topo:115523.2547\n",
      "Epoch:567, P:None, Loss:65.9528, Loss-ae:65.9528, Loss-topo:122167.7500\n",
      "Epoch:568, P:None, Loss:65.9802, Loss-ae:65.9802, Loss-topo:122866.3031\n",
      "Epoch:569, P:None, Loss:65.7163, Loss-ae:65.7163, Loss-topo:116637.1578\n",
      "Epoch:570, P:None, Loss:66.6034, Loss-ae:66.6034, Loss-topo:117415.9250\n",
      "Epoch:571, P:None, Loss:65.5366, Loss-ae:65.5366, Loss-topo:115808.4859\n",
      "Epoch:572, P:None, Loss:66.3098, Loss-ae:66.3098, Loss-topo:120591.8406\n",
      "Epoch:573, P:None, Loss:65.4385, Loss-ae:65.4385, Loss-topo:117641.9781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:574, P:None, Loss:66.4063, Loss-ae:66.4063, Loss-topo:117686.9656\n",
      "Epoch:575, P:None, Loss:67.0499, Loss-ae:67.0499, Loss-topo:119993.6687\n",
      "Epoch:576, P:None, Loss:65.6819, Loss-ae:65.6819, Loss-topo:117365.7250\n",
      "Epoch:577, P:None, Loss:66.5720, Loss-ae:66.5720, Loss-topo:114849.5578\n",
      "Epoch:578, P:None, Loss:65.6985, Loss-ae:65.6985, Loss-topo:120676.9266\n",
      "Epoch:579, P:None, Loss:66.3005, Loss-ae:66.3005, Loss-topo:115166.5000\n",
      "Epoch:580, P:None, Loss:66.4565, Loss-ae:66.4565, Loss-topo:115405.3000\n",
      "Epoch:581, P:None, Loss:65.8278, Loss-ae:65.8278, Loss-topo:123541.4359\n",
      "Epoch:582, P:None, Loss:66.5203, Loss-ae:66.5203, Loss-topo:114510.3078\n",
      "Epoch:583, P:None, Loss:66.5898, Loss-ae:66.5898, Loss-topo:114926.8844\n",
      "Epoch:584, P:None, Loss:66.0603, Loss-ae:66.0603, Loss-topo:110472.4906\n",
      "Epoch:585, P:None, Loss:65.6074, Loss-ae:65.6074, Loss-topo:118702.7859\n",
      "Epoch:586, P:None, Loss:66.0838, Loss-ae:66.0838, Loss-topo:120829.6250\n",
      "Epoch:587, P:None, Loss:66.1270, Loss-ae:66.1270, Loss-topo:121247.4531\n",
      "Epoch:588, P:None, Loss:66.2422, Loss-ae:66.2422, Loss-topo:115793.5266\n",
      "Epoch:589, P:None, Loss:66.2539, Loss-ae:66.2539, Loss-topo:117917.4469\n",
      "Epoch:590, P:None, Loss:65.7546, Loss-ae:65.7546, Loss-topo:113926.9219\n",
      "Epoch:591, P:None, Loss:65.8132, Loss-ae:65.8132, Loss-topo:115848.0109\n",
      "Epoch:592, P:None, Loss:65.7840, Loss-ae:65.7840, Loss-topo:110337.1953\n",
      "Epoch:593, P:None, Loss:65.5682, Loss-ae:65.5682, Loss-topo:117449.7781\n",
      "Epoch:594, P:None, Loss:65.2403, Loss-ae:65.2403, Loss-topo:116350.3813\n",
      "Epoch:595, P:None, Loss:65.6983, Loss-ae:65.6983, Loss-topo:115123.1500\n",
      "Epoch:596, P:None, Loss:67.1168, Loss-ae:67.1168, Loss-topo:114449.2734\n",
      "Epoch:597, P:None, Loss:64.9067, Loss-ae:64.9067, Loss-topo:115231.0063\n",
      "Epoch:598, P:None, Loss:65.5723, Loss-ae:65.5723, Loss-topo:114432.4203\n",
      "Epoch:599, P:None, Loss:65.8206, Loss-ae:65.8206, Loss-topo:118353.9594\n",
      "Epoch:600, P:None, Loss:66.5920, Loss-ae:66.5920, Loss-topo:121326.4219\n",
      "Epoch:601, P:None, Loss:66.6894, Loss-ae:66.6894, Loss-topo:113246.0109\n",
      "Epoch:602, P:None, Loss:65.3420, Loss-ae:65.3420, Loss-topo:117451.4172\n",
      "Epoch:603, P:None, Loss:65.9639, Loss-ae:65.9639, Loss-topo:120165.9000\n",
      "Epoch:604, P:None, Loss:66.3428, Loss-ae:66.3428, Loss-topo:113124.6141\n",
      "Epoch:605, P:None, Loss:66.4492, Loss-ae:66.4492, Loss-topo:113073.5859\n",
      "Epoch:606, P:None, Loss:66.0973, Loss-ae:66.0973, Loss-topo:118873.3125\n",
      "Epoch:607, P:None, Loss:66.1090, Loss-ae:66.1090, Loss-topo:108902.7609\n",
      "Epoch:608, P:None, Loss:67.0203, Loss-ae:67.0203, Loss-topo:121710.4000\n",
      "Epoch:609, P:None, Loss:66.3206, Loss-ae:66.3206, Loss-topo:114804.2750\n",
      "Epoch:610, P:None, Loss:65.6619, Loss-ae:65.6619, Loss-topo:114394.6594\n",
      "Epoch:611, P:None, Loss:66.2713, Loss-ae:66.2713, Loss-topo:121219.5703\n",
      "Epoch:612, P:None, Loss:67.7380, Loss-ae:67.7380, Loss-topo:116772.9422\n",
      "Epoch:613, P:None, Loss:66.0723, Loss-ae:66.0723, Loss-topo:111909.6070\n",
      "Epoch:614, P:None, Loss:66.4969, Loss-ae:66.4969, Loss-topo:117337.5188\n",
      "Epoch:615, P:None, Loss:66.1375, Loss-ae:66.1375, Loss-topo:115017.6750\n",
      "Epoch:616, P:None, Loss:66.3072, Loss-ae:66.3072, Loss-topo:116142.1133\n",
      "Epoch:617, P:None, Loss:66.0496, Loss-ae:66.0496, Loss-topo:115091.9359\n",
      "Epoch:618, P:None, Loss:67.2323, Loss-ae:67.2323, Loss-topo:116772.7437\n",
      "Epoch:619, P:None, Loss:66.8051, Loss-ae:66.8051, Loss-topo:116088.8984\n",
      "Epoch:620, P:None, Loss:66.5454, Loss-ae:66.5454, Loss-topo:108017.0953\n",
      "Epoch:621, P:None, Loss:65.6487, Loss-ae:65.6487, Loss-topo:114524.1812\n",
      "Epoch:622, P:None, Loss:65.9495, Loss-ae:65.9495, Loss-topo:117867.0625\n",
      "Epoch:623, P:None, Loss:65.8599, Loss-ae:65.8599, Loss-topo:112461.7922\n",
      "Epoch:624, P:None, Loss:65.7052, Loss-ae:65.7052, Loss-topo:106383.9859\n",
      "Epoch:625, P:None, Loss:65.9127, Loss-ae:65.9127, Loss-topo:106799.9906\n",
      "Epoch:626, P:None, Loss:66.5813, Loss-ae:66.5813, Loss-topo:120939.7297\n",
      "Epoch:627, P:None, Loss:65.4714, Loss-ae:65.4714, Loss-topo:114388.2594\n",
      "Epoch:628, P:None, Loss:67.3565, Loss-ae:67.3565, Loss-topo:123398.7641\n",
      "Epoch:629, P:None, Loss:66.1516, Loss-ae:66.1516, Loss-topo:118405.2984\n",
      "Epoch:630, P:None, Loss:65.9782, Loss-ae:65.9782, Loss-topo:126960.4094\n",
      "Epoch:631, P:None, Loss:65.9599, Loss-ae:65.9599, Loss-topo:113453.7031\n",
      "Epoch:632, P:None, Loss:65.8687, Loss-ae:65.8687, Loss-topo:116625.5063\n",
      "Epoch:633, P:None, Loss:67.3157, Loss-ae:67.3157, Loss-topo:117076.6109\n",
      "Epoch:634, P:None, Loss:65.5147, Loss-ae:65.5147, Loss-topo:113866.6203\n",
      "Epoch:635, P:None, Loss:65.9684, Loss-ae:65.9684, Loss-topo:112536.4469\n",
      "Epoch:636, P:None, Loss:66.1181, Loss-ae:66.1181, Loss-topo:116537.1328\n",
      "Epoch:637, P:None, Loss:66.0561, Loss-ae:66.0561, Loss-topo:118301.3953\n",
      "Epoch:638, P:None, Loss:68.3661, Loss-ae:68.3661, Loss-topo:111269.2875\n",
      "Epoch:639, P:None, Loss:66.8995, Loss-ae:66.8995, Loss-topo:109049.1453\n",
      "Epoch:640, P:None, Loss:66.0183, Loss-ae:66.0183, Loss-topo:112761.0984\n",
      "Epoch:641, P:None, Loss:67.7838, Loss-ae:67.7838, Loss-topo:114061.2141\n",
      "Epoch:642, P:None, Loss:65.9920, Loss-ae:65.9920, Loss-topo:107393.2484\n",
      "Epoch:643, P:None, Loss:66.3112, Loss-ae:66.3112, Loss-topo:115332.4203\n",
      "Epoch:644, P:None, Loss:66.0095, Loss-ae:66.0095, Loss-topo:114029.4750\n",
      "Epoch:645, P:None, Loss:65.9139, Loss-ae:65.9139, Loss-topo:116685.0328\n",
      "Epoch:646, P:None, Loss:66.2879, Loss-ae:66.2879, Loss-topo:115472.3844\n",
      "Epoch:647, P:None, Loss:65.1341, Loss-ae:65.1341, Loss-topo:118948.3766\n",
      "Epoch:648, P:None, Loss:65.1401, Loss-ae:65.1401, Loss-topo:114726.7164\n",
      "Epoch:649, P:None, Loss:65.7762, Loss-ae:65.7762, Loss-topo:113694.7875\n",
      "Epoch:650, P:None, Loss:65.5919, Loss-ae:65.5919, Loss-topo:116119.5000\n",
      "Epoch:651, P:None, Loss:66.1799, Loss-ae:66.1799, Loss-topo:122211.2688\n",
      "Epoch:652, P:None, Loss:65.9605, Loss-ae:65.9605, Loss-topo:114779.4344\n",
      "Epoch:653, P:None, Loss:65.8350, Loss-ae:65.8350, Loss-topo:116253.8781\n",
      "Epoch:654, P:None, Loss:65.6338, Loss-ae:65.6338, Loss-topo:124494.2797\n",
      "Epoch:655, P:None, Loss:66.7367, Loss-ae:66.7367, Loss-topo:115614.2391\n",
      "Epoch:656, P:None, Loss:65.6048, Loss-ae:65.6048, Loss-topo:104386.4734\n",
      "Epoch:657, P:None, Loss:65.7430, Loss-ae:65.7430, Loss-topo:110825.8969\n",
      "Epoch:658, P:None, Loss:66.9748, Loss-ae:66.9748, Loss-topo:122721.2500\n",
      "Epoch:659, P:None, Loss:65.9524, Loss-ae:65.9524, Loss-topo:114903.6250\n",
      "Epoch:660, P:None, Loss:66.0329, Loss-ae:66.0329, Loss-topo:115024.6844\n",
      "Epoch:661, P:None, Loss:66.5204, Loss-ae:66.5204, Loss-topo:112758.9703\n",
      "Epoch:662, P:None, Loss:65.8329, Loss-ae:65.8329, Loss-topo:115129.0594\n",
      "Epoch:663, P:None, Loss:66.3261, Loss-ae:66.3261, Loss-topo:109347.5250\n",
      "Epoch:664, P:None, Loss:65.3650, Loss-ae:65.3650, Loss-topo:113497.7719\n",
      "Epoch:665, P:None, Loss:65.7730, Loss-ae:65.7730, Loss-topo:116483.2937\n",
      "Epoch:666, P:None, Loss:65.9224, Loss-ae:65.9224, Loss-topo:115117.7125\n",
      "Epoch:667, P:None, Loss:65.3654, Loss-ae:65.3654, Loss-topo:114448.1031\n",
      "Epoch:668, P:None, Loss:65.3906, Loss-ae:65.3906, Loss-topo:113566.8234\n",
      "Epoch:669, P:None, Loss:65.8139, Loss-ae:65.8139, Loss-topo:118970.8859\n",
      "Epoch:670, P:None, Loss:66.2393, Loss-ae:66.2393, Loss-topo:117509.0328\n",
      "Epoch:671, P:None, Loss:66.3156, Loss-ae:66.3156, Loss-topo:118032.8984\n",
      "Epoch:672, P:None, Loss:65.8642, Loss-ae:65.8642, Loss-topo:114879.8672\n",
      "Epoch:673, P:None, Loss:65.5707, Loss-ae:65.5707, Loss-topo:119501.1422\n",
      "Epoch:674, P:None, Loss:66.0770, Loss-ae:66.0770, Loss-topo:116962.9578\n",
      "Epoch:675, P:None, Loss:66.1817, Loss-ae:66.1817, Loss-topo:112451.0156\n",
      "Epoch:676, P:None, Loss:66.0922, Loss-ae:66.0922, Loss-topo:117961.4531\n",
      "Epoch:677, P:None, Loss:65.3511, Loss-ae:65.3511, Loss-topo:118471.2359\n",
      "Epoch:678, P:None, Loss:65.8496, Loss-ae:65.8496, Loss-topo:110034.2047\n",
      "Epoch:679, P:None, Loss:65.9717, Loss-ae:65.9717, Loss-topo:109346.8555\n",
      "Epoch:680, P:None, Loss:64.8973, Loss-ae:64.8973, Loss-topo:117098.6359\n",
      "Epoch:681, P:None, Loss:65.9827, Loss-ae:65.9827, Loss-topo:114685.3781\n",
      "Epoch:682, P:None, Loss:66.2275, Loss-ae:66.2275, Loss-topo:115524.0516\n",
      "Epoch:683, P:None, Loss:66.9140, Loss-ae:66.9140, Loss-topo:108477.6859\n",
      "Epoch:684, P:None, Loss:66.1485, Loss-ae:66.1485, Loss-topo:116475.8375\n",
      "Epoch:685, P:None, Loss:65.9483, Loss-ae:65.9483, Loss-topo:117695.8891\n",
      "Epoch:686, P:None, Loss:65.6867, Loss-ae:65.6867, Loss-topo:114748.3078\n",
      "Epoch:687, P:None, Loss:65.9285, Loss-ae:65.9285, Loss-topo:111264.4406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:688, P:None, Loss:67.3685, Loss-ae:67.3685, Loss-topo:119449.4703\n",
      "Epoch:689, P:None, Loss:66.7628, Loss-ae:66.7628, Loss-topo:117732.1328\n",
      "Epoch:690, P:None, Loss:66.3993, Loss-ae:66.3993, Loss-topo:110552.6578\n",
      "Epoch:691, P:None, Loss:67.3433, Loss-ae:67.3433, Loss-topo:120418.1891\n",
      "Epoch:692, P:None, Loss:66.7781, Loss-ae:66.7781, Loss-topo:113245.1141\n",
      "Epoch:693, P:None, Loss:66.0097, Loss-ae:66.0097, Loss-topo:117350.1531\n",
      "Epoch:694, P:None, Loss:66.6020, Loss-ae:66.6020, Loss-topo:111147.3281\n",
      "Epoch:695, P:None, Loss:66.8111, Loss-ae:66.8111, Loss-topo:112810.4453\n",
      "Epoch:696, P:None, Loss:66.5403, Loss-ae:66.5403, Loss-topo:111814.2328\n",
      "Epoch:697, P:None, Loss:65.9157, Loss-ae:65.9157, Loss-topo:114955.4766\n",
      "Epoch:698, P:None, Loss:66.1653, Loss-ae:66.1653, Loss-topo:114082.2297\n",
      "Epoch:699, P:None, Loss:66.1351, Loss-ae:66.1351, Loss-topo:118659.6656\n",
      "Epoch:700, P:None, Loss:66.9987, Loss-ae:66.9987, Loss-topo:109388.7406\n",
      "Epoch:701, P:None, Loss:66.5448, Loss-ae:66.5448, Loss-topo:112900.4250\n",
      "Epoch:702, P:None, Loss:66.4124, Loss-ae:66.4124, Loss-topo:114362.6297\n",
      "Epoch:703, P:None, Loss:66.9148, Loss-ae:66.9148, Loss-topo:110929.0344\n",
      "Epoch:704, P:None, Loss:66.8705, Loss-ae:66.8705, Loss-topo:111373.4312\n",
      "Epoch:705, P:None, Loss:66.6877, Loss-ae:66.6877, Loss-topo:114244.1734\n",
      "Epoch:706, P:None, Loss:66.8327, Loss-ae:66.8327, Loss-topo:107610.6297\n",
      "Epoch:707, P:None, Loss:66.5647, Loss-ae:66.5647, Loss-topo:103816.9766\n",
      "Epoch:708, P:None, Loss:66.4221, Loss-ae:66.4221, Loss-topo:113483.5641\n",
      "Epoch:709, P:None, Loss:66.1747, Loss-ae:66.1747, Loss-topo:107665.4406\n",
      "Epoch:710, P:None, Loss:66.0240, Loss-ae:66.0240, Loss-topo:111436.8344\n",
      "Epoch:711, P:None, Loss:66.0001, Loss-ae:66.0001, Loss-topo:111887.7625\n",
      "Epoch:712, P:None, Loss:66.3113, Loss-ae:66.3113, Loss-topo:110606.7641\n",
      "Epoch:713, P:None, Loss:66.1056, Loss-ae:66.1056, Loss-topo:107428.8125\n",
      "Epoch:714, P:None, Loss:66.2208, Loss-ae:66.2208, Loss-topo:114010.5969\n",
      "Epoch:715, P:None, Loss:66.1656, Loss-ae:66.1656, Loss-topo:110112.6203\n",
      "Epoch:716, P:None, Loss:66.3678, Loss-ae:66.3678, Loss-topo:107932.6187\n",
      "Epoch:717, P:None, Loss:66.7526, Loss-ae:66.7526, Loss-topo:113822.9688\n",
      "Epoch:718, P:None, Loss:66.7444, Loss-ae:66.7444, Loss-topo:113741.0375\n",
      "Epoch:719, P:None, Loss:66.5303, Loss-ae:66.5303, Loss-topo:116056.0047\n",
      "Epoch:720, P:None, Loss:66.4055, Loss-ae:66.4055, Loss-topo:109075.3047\n",
      "Epoch:721, P:None, Loss:66.8140, Loss-ae:66.8140, Loss-topo:105305.7766\n",
      "Epoch:722, P:None, Loss:66.1889, Loss-ae:66.1889, Loss-topo:108826.1922\n",
      "Epoch:723, P:None, Loss:67.2040, Loss-ae:67.2040, Loss-topo:116852.4469\n",
      "Epoch:724, P:None, Loss:66.2305, Loss-ae:66.2305, Loss-topo:103170.0437\n",
      "Epoch:725, P:None, Loss:66.3620, Loss-ae:66.3620, Loss-topo:117535.9172\n",
      "Epoch:726, P:None, Loss:66.1608, Loss-ae:66.1608, Loss-topo:108876.7437\n",
      "Epoch:727, P:None, Loss:67.6037, Loss-ae:67.6037, Loss-topo:114606.0719\n",
      "Epoch:728, P:None, Loss:66.4796, Loss-ae:66.4796, Loss-topo:109230.8922\n",
      "Epoch:729, P:None, Loss:66.7751, Loss-ae:66.7751, Loss-topo:114530.6891\n",
      "Epoch:730, P:None, Loss:66.5331, Loss-ae:66.5331, Loss-topo:101667.2297\n",
      "Epoch:731, P:None, Loss:66.2408, Loss-ae:66.2408, Loss-topo:113063.6156\n",
      "Epoch:732, P:None, Loss:67.0148, Loss-ae:67.0148, Loss-topo:105213.2867\n",
      "Epoch:733, P:None, Loss:66.8545, Loss-ae:66.8545, Loss-topo:109831.5969\n",
      "Epoch:734, P:None, Loss:69.0881, Loss-ae:69.0881, Loss-topo:115283.7625\n",
      "Epoch:735, P:None, Loss:66.5456, Loss-ae:66.5456, Loss-topo:110502.5328\n",
      "Epoch:736, P:None, Loss:66.9814, Loss-ae:66.9814, Loss-topo:111535.3375\n",
      "Epoch:737, P:None, Loss:65.7158, Loss-ae:65.7158, Loss-topo:108994.7133\n",
      "Epoch:738, P:None, Loss:66.6418, Loss-ae:66.6418, Loss-topo:113870.9109\n",
      "Epoch:739, P:None, Loss:66.3531, Loss-ae:66.3531, Loss-topo:111334.4078\n",
      "Epoch:740, P:None, Loss:66.5812, Loss-ae:66.5812, Loss-topo:115365.2969\n",
      "Epoch:741, P:None, Loss:65.6290, Loss-ae:65.6290, Loss-topo:108462.4734\n",
      "Epoch:742, P:None, Loss:66.3715, Loss-ae:66.3715, Loss-topo:119450.7344\n",
      "Epoch:743, P:None, Loss:65.9814, Loss-ae:65.9814, Loss-topo:120678.0125\n",
      "Epoch:744, P:None, Loss:66.2313, Loss-ae:66.2313, Loss-topo:119916.1703\n",
      "Epoch:745, P:None, Loss:66.5080, Loss-ae:66.5080, Loss-topo:112680.0141\n",
      "Epoch:746, P:None, Loss:66.7203, Loss-ae:66.7203, Loss-topo:107792.1492\n",
      "Epoch:747, P:None, Loss:66.3339, Loss-ae:66.3339, Loss-topo:111570.4234\n",
      "Epoch:748, P:None, Loss:67.5333, Loss-ae:67.5333, Loss-topo:114516.5172\n",
      "Epoch:749, P:None, Loss:66.3025, Loss-ae:66.3025, Loss-topo:108818.8359\n",
      "Epoch:750, P:None, Loss:66.7760, Loss-ae:66.7760, Loss-topo:109637.6687\n",
      "Epoch:751, P:None, Loss:66.2525, Loss-ae:66.2525, Loss-topo:113581.7453\n",
      "Epoch:752, P:None, Loss:66.6072, Loss-ae:66.6072, Loss-topo:114160.5984\n",
      "Epoch:753, P:None, Loss:66.8720, Loss-ae:66.8720, Loss-topo:119237.3641\n",
      "Epoch:754, P:None, Loss:66.9249, Loss-ae:66.9249, Loss-topo:115177.1031\n",
      "Epoch:755, P:None, Loss:66.9466, Loss-ae:66.9466, Loss-topo:109864.3164\n",
      "Epoch:756, P:None, Loss:66.4979, Loss-ae:66.4979, Loss-topo:108676.4672\n",
      "Epoch:757, P:None, Loss:66.9902, Loss-ae:66.9902, Loss-topo:112455.4016\n",
      "Epoch:758, P:None, Loss:66.9166, Loss-ae:66.9166, Loss-topo:112404.0172\n",
      "Epoch:759, P:None, Loss:66.3637, Loss-ae:66.3637, Loss-topo:109705.5562\n",
      "Epoch:760, P:None, Loss:66.0560, Loss-ae:66.0560, Loss-topo:110744.3219\n",
      "Epoch:761, P:None, Loss:67.0148, Loss-ae:67.0148, Loss-topo:108908.4141\n",
      "Epoch:762, P:None, Loss:66.0676, Loss-ae:66.0676, Loss-topo:107451.2219\n",
      "Epoch:763, P:None, Loss:66.9909, Loss-ae:66.9909, Loss-topo:108541.1500\n",
      "Epoch:764, P:None, Loss:66.0073, Loss-ae:66.0073, Loss-topo:113048.4547\n",
      "Epoch:765, P:None, Loss:66.7098, Loss-ae:66.7098, Loss-topo:111027.9984\n",
      "Epoch:766, P:None, Loss:67.1203, Loss-ae:67.1203, Loss-topo:114522.1594\n",
      "Epoch:767, P:None, Loss:67.4138, Loss-ae:67.4138, Loss-topo:105164.3297\n",
      "Epoch:768, P:None, Loss:67.2435, Loss-ae:67.2435, Loss-topo:111909.5516\n",
      "Epoch:769, P:None, Loss:66.2289, Loss-ae:66.2289, Loss-topo:108566.3953\n",
      "Epoch:770, P:None, Loss:66.1749, Loss-ae:66.1749, Loss-topo:108579.2531\n",
      "Epoch:771, P:None, Loss:66.9427, Loss-ae:66.9427, Loss-topo:113616.1094\n",
      "Epoch:772, P:None, Loss:66.9692, Loss-ae:66.9692, Loss-topo:112161.4844\n",
      "Epoch:773, P:None, Loss:66.8149, Loss-ae:66.8149, Loss-topo:114548.7125\n",
      "Epoch:774, P:None, Loss:66.2848, Loss-ae:66.2848, Loss-topo:109868.4844\n",
      "Epoch:775, P:None, Loss:66.6036, Loss-ae:66.6036, Loss-topo:106366.0500\n",
      "Epoch:776, P:None, Loss:67.4170, Loss-ae:67.4170, Loss-topo:111194.5859\n",
      "Epoch:777, P:None, Loss:66.4344, Loss-ae:66.4344, Loss-topo:116760.2391\n",
      "Epoch:778, P:None, Loss:67.1532, Loss-ae:67.1532, Loss-topo:113842.4594\n",
      "Epoch:779, P:None, Loss:66.8783, Loss-ae:66.8783, Loss-topo:112375.8469\n",
      "Epoch:780, P:None, Loss:67.5947, Loss-ae:67.5947, Loss-topo:117747.7719\n",
      "Epoch:781, P:None, Loss:66.8500, Loss-ae:66.8500, Loss-topo:110309.8328\n",
      "Epoch:782, P:None, Loss:66.7933, Loss-ae:66.7933, Loss-topo:99593.3078\n",
      "Epoch:783, P:None, Loss:66.6619, Loss-ae:66.6619, Loss-topo:114149.5078\n",
      "Epoch:784, P:None, Loss:66.9359, Loss-ae:66.9359, Loss-topo:112902.2172\n",
      "Epoch:785, P:None, Loss:67.2922, Loss-ae:67.2922, Loss-topo:114027.2891\n",
      "Epoch:786, P:None, Loss:67.4526, Loss-ae:67.4526, Loss-topo:100337.0437\n",
      "Epoch:787, P:None, Loss:66.5263, Loss-ae:66.5263, Loss-topo:118244.0109\n",
      "Epoch:788, P:None, Loss:66.4760, Loss-ae:66.4760, Loss-topo:109250.6656\n",
      "Epoch:789, P:None, Loss:66.9113, Loss-ae:66.9113, Loss-topo:110136.8453\n",
      "Epoch:790, P:None, Loss:66.8582, Loss-ae:66.8582, Loss-topo:106593.7297\n",
      "Epoch:791, P:None, Loss:66.8902, Loss-ae:66.8902, Loss-topo:114658.9078\n",
      "Epoch:792, P:None, Loss:67.4651, Loss-ae:67.4651, Loss-topo:115072.8625\n",
      "Epoch:793, P:None, Loss:65.9794, Loss-ae:65.9794, Loss-topo:116659.8344\n",
      "Epoch:794, P:None, Loss:66.5286, Loss-ae:66.5286, Loss-topo:108269.9859\n",
      "Epoch:795, P:None, Loss:67.5684, Loss-ae:67.5684, Loss-topo:108645.0609\n",
      "Epoch:796, P:None, Loss:66.4347, Loss-ae:66.4347, Loss-topo:106954.0359\n",
      "Epoch:797, P:None, Loss:67.3720, Loss-ae:67.3720, Loss-topo:112034.8344\n",
      "Epoch:798, P:None, Loss:66.4476, Loss-ae:66.4476, Loss-topo:112931.6219\n",
      "Epoch:799, P:None, Loss:67.5102, Loss-ae:67.5102, Loss-topo:110529.6492\n",
      "Epoch:800, P:None, Loss:67.2663, Loss-ae:67.2663, Loss-topo:107967.5688\n",
      "Epoch:801, P:None, Loss:66.8866, Loss-ae:66.8866, Loss-topo:111408.2484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:802, P:None, Loss:67.9738, Loss-ae:67.9738, Loss-topo:117825.1641\n",
      "Epoch:803, P:None, Loss:66.8939, Loss-ae:66.8939, Loss-topo:110343.1969\n",
      "Epoch:804, P:None, Loss:66.6901, Loss-ae:66.6901, Loss-topo:108183.0047\n",
      "Epoch:805, P:None, Loss:67.6158, Loss-ae:67.6158, Loss-topo:109712.4531\n",
      "Epoch:806, P:None, Loss:66.9675, Loss-ae:66.9675, Loss-topo:114170.5969\n",
      "Epoch:807, P:None, Loss:67.4309, Loss-ae:67.4309, Loss-topo:112284.6125\n",
      "Epoch:808, P:None, Loss:67.3943, Loss-ae:67.3943, Loss-topo:107453.8766\n",
      "Epoch:809, P:None, Loss:68.7231, Loss-ae:68.7231, Loss-topo:105705.8766\n",
      "Epoch:810, P:None, Loss:67.0367, Loss-ae:67.0367, Loss-topo:110033.5016\n",
      "Epoch:811, P:None, Loss:67.3668, Loss-ae:67.3668, Loss-topo:110191.2828\n",
      "Epoch:812, P:None, Loss:67.2632, Loss-ae:67.2632, Loss-topo:109613.5750\n",
      "Epoch:813, P:None, Loss:66.9371, Loss-ae:66.9371, Loss-topo:110674.2266\n",
      "Epoch:814, P:None, Loss:66.6737, Loss-ae:66.6737, Loss-topo:102765.8438\n",
      "Epoch:815, P:None, Loss:68.6943, Loss-ae:68.6943, Loss-topo:109838.7000\n",
      "Epoch:816, P:None, Loss:68.7413, Loss-ae:68.7413, Loss-topo:117454.7391\n",
      "Epoch:817, P:None, Loss:67.1302, Loss-ae:67.1302, Loss-topo:113252.2031\n",
      "Epoch:818, P:None, Loss:68.1235, Loss-ae:68.1235, Loss-topo:108989.2984\n",
      "Epoch:819, P:None, Loss:66.2177, Loss-ae:66.2177, Loss-topo:103904.4219\n",
      "Epoch:820, P:None, Loss:67.1964, Loss-ae:67.1964, Loss-topo:112437.3031\n",
      "Epoch:821, P:None, Loss:66.6035, Loss-ae:66.6035, Loss-topo:112499.3219\n",
      "Epoch:822, P:None, Loss:67.3142, Loss-ae:67.3142, Loss-topo:112306.4781\n",
      "Epoch:823, P:None, Loss:66.5726, Loss-ae:66.5726, Loss-topo:112785.8938\n",
      "Epoch:824, P:None, Loss:67.1840, Loss-ae:67.1840, Loss-topo:113354.0906\n",
      "Epoch:825, P:None, Loss:67.4091, Loss-ae:67.4091, Loss-topo:106473.8438\n",
      "Epoch:826, P:None, Loss:66.5259, Loss-ae:66.5259, Loss-topo:109016.0766\n",
      "Epoch:827, P:None, Loss:66.0483, Loss-ae:66.0483, Loss-topo:110222.3375\n",
      "Epoch:828, P:None, Loss:66.6927, Loss-ae:66.6927, Loss-topo:110775.0094\n",
      "Epoch:829, P:None, Loss:67.4877, Loss-ae:67.4877, Loss-topo:109612.2859\n",
      "Epoch:830, P:None, Loss:67.6007, Loss-ae:67.6007, Loss-topo:115800.5828\n",
      "Epoch:831, P:None, Loss:67.4834, Loss-ae:67.4834, Loss-topo:116970.8953\n",
      "Epoch:832, P:None, Loss:67.0548, Loss-ae:67.0548, Loss-topo:108120.2344\n",
      "Epoch:833, P:None, Loss:68.4642, Loss-ae:68.4642, Loss-topo:112671.3281\n",
      "Epoch:834, P:None, Loss:67.3887, Loss-ae:67.3887, Loss-topo:109902.8562\n",
      "Epoch:835, P:None, Loss:67.6002, Loss-ae:67.6002, Loss-topo:110248.6656\n",
      "Epoch:836, P:None, Loss:67.6359, Loss-ae:67.6359, Loss-topo:105926.9984\n",
      "Epoch:837, P:None, Loss:67.4260, Loss-ae:67.4260, Loss-topo:105867.3891\n",
      "Epoch:838, P:None, Loss:68.2230, Loss-ae:68.2230, Loss-topo:104300.3156\n",
      "Epoch:839, P:None, Loss:68.1383, Loss-ae:68.1383, Loss-topo:113578.1375\n",
      "Epoch:840, P:None, Loss:68.0415, Loss-ae:68.0415, Loss-topo:116068.3016\n",
      "Epoch:841, P:None, Loss:66.7029, Loss-ae:66.7029, Loss-topo:107636.2922\n",
      "Epoch:842, P:None, Loss:67.3805, Loss-ae:67.3805, Loss-topo:108377.6578\n",
      "Epoch:843, P:None, Loss:67.3695, Loss-ae:67.3695, Loss-topo:108555.1891\n",
      "Epoch:844, P:None, Loss:67.0393, Loss-ae:67.0393, Loss-topo:108955.8938\n",
      "Epoch:845, P:None, Loss:67.2883, Loss-ae:67.2883, Loss-topo:105353.0859\n",
      "Epoch:846, P:None, Loss:67.7659, Loss-ae:67.7659, Loss-topo:104584.9812\n",
      "Epoch:847, P:None, Loss:68.4482, Loss-ae:68.4482, Loss-topo:112615.5250\n",
      "Epoch:848, P:None, Loss:68.0108, Loss-ae:68.0108, Loss-topo:114893.8547\n",
      "Epoch:849, P:None, Loss:67.5051, Loss-ae:67.5051, Loss-topo:111396.5609\n",
      "Epoch:850, P:None, Loss:66.5881, Loss-ae:66.5881, Loss-topo:104836.3211\n",
      "Epoch:851, P:None, Loss:67.5910, Loss-ae:67.5910, Loss-topo:109742.1375\n",
      "Epoch:852, P:None, Loss:67.4206, Loss-ae:67.4206, Loss-topo:109428.1984\n",
      "Epoch:853, P:None, Loss:67.0544, Loss-ae:67.0544, Loss-topo:112241.3641\n",
      "Epoch:854, P:None, Loss:67.7755, Loss-ae:67.7755, Loss-topo:111170.6812\n",
      "Epoch:855, P:None, Loss:67.6281, Loss-ae:67.6281, Loss-topo:105304.7984\n",
      "Epoch:856, P:None, Loss:67.6987, Loss-ae:67.6987, Loss-topo:104250.4453\n",
      "Epoch:857, P:None, Loss:67.8929, Loss-ae:67.8929, Loss-topo:107694.8234\n",
      "Epoch:858, P:None, Loss:67.7915, Loss-ae:67.7915, Loss-topo:104319.1391\n",
      "Epoch:859, P:None, Loss:67.4504, Loss-ae:67.4504, Loss-topo:109274.8766\n",
      "Epoch:860, P:None, Loss:67.0704, Loss-ae:67.0704, Loss-topo:106096.9453\n",
      "Epoch:861, P:None, Loss:67.1006, Loss-ae:67.1006, Loss-topo:103066.0734\n",
      "Epoch:862, P:None, Loss:67.5870, Loss-ae:67.5870, Loss-topo:96832.8133\n",
      "Epoch:863, P:None, Loss:67.1810, Loss-ae:67.1810, Loss-topo:114957.5844\n",
      "Epoch:864, P:None, Loss:67.1732, Loss-ae:67.1732, Loss-topo:112893.3094\n",
      "Epoch:865, P:None, Loss:67.6749, Loss-ae:67.6749, Loss-topo:106973.9266\n",
      "Epoch:866, P:None, Loss:67.0631, Loss-ae:67.0631, Loss-topo:115353.4328\n",
      "Epoch:867, P:None, Loss:67.6877, Loss-ae:67.6877, Loss-topo:107667.3969\n",
      "Epoch:868, P:None, Loss:67.1952, Loss-ae:67.1952, Loss-topo:102372.8359\n",
      "Epoch:869, P:None, Loss:67.1595, Loss-ae:67.1595, Loss-topo:108033.4828\n",
      "Epoch:870, P:None, Loss:67.4002, Loss-ae:67.4002, Loss-topo:112954.0813\n",
      "Epoch:871, P:None, Loss:67.8236, Loss-ae:67.8236, Loss-topo:107095.5437\n",
      "Epoch:872, P:None, Loss:67.4443, Loss-ae:67.4443, Loss-topo:107874.8141\n",
      "Epoch:873, P:None, Loss:67.4170, Loss-ae:67.4170, Loss-topo:107203.4641\n",
      "Epoch:874, P:None, Loss:67.5302, Loss-ae:67.5302, Loss-topo:107127.7188\n",
      "Epoch:875, P:None, Loss:67.7627, Loss-ae:67.7627, Loss-topo:108034.5359\n",
      "Epoch:876, P:None, Loss:67.1202, Loss-ae:67.1202, Loss-topo:105905.9219\n",
      "Epoch:877, P:None, Loss:68.6080, Loss-ae:68.6080, Loss-topo:112343.9797\n",
      "Epoch:878, P:None, Loss:67.5390, Loss-ae:67.5390, Loss-topo:110896.1250\n",
      "Epoch:879, P:None, Loss:66.7068, Loss-ae:66.7068, Loss-topo:106892.2703\n",
      "Epoch:880, P:None, Loss:68.1848, Loss-ae:68.1848, Loss-topo:108236.9969\n",
      "Epoch:881, P:None, Loss:67.0811, Loss-ae:67.0811, Loss-topo:104244.2906\n",
      "Epoch:882, P:None, Loss:68.2301, Loss-ae:68.2301, Loss-topo:111140.5344\n",
      "Epoch:883, P:None, Loss:67.1311, Loss-ae:67.1311, Loss-topo:101685.5781\n",
      "Epoch:884, P:None, Loss:67.7879, Loss-ae:67.7879, Loss-topo:110583.2047\n",
      "Epoch:885, P:None, Loss:67.3815, Loss-ae:67.3815, Loss-topo:108836.2078\n",
      "Epoch:886, P:None, Loss:67.6939, Loss-ae:67.6939, Loss-topo:104601.6719\n",
      "Epoch:887, P:None, Loss:67.8786, Loss-ae:67.8786, Loss-topo:105771.4937\n",
      "Epoch:888, P:None, Loss:68.0571, Loss-ae:68.0571, Loss-topo:109884.3016\n",
      "Epoch:889, P:None, Loss:66.8294, Loss-ae:66.8294, Loss-topo:106517.6766\n",
      "Epoch:890, P:None, Loss:67.8946, Loss-ae:67.8946, Loss-topo:110879.5813\n",
      "Epoch:891, P:None, Loss:68.2423, Loss-ae:68.2423, Loss-topo:108294.8391\n",
      "Epoch:892, P:None, Loss:67.4096, Loss-ae:67.4096, Loss-topo:104493.6359\n",
      "Epoch:893, P:None, Loss:67.9114, Loss-ae:67.9114, Loss-topo:102257.9766\n",
      "Epoch:894, P:None, Loss:67.7470, Loss-ae:67.7470, Loss-topo:107100.1031\n",
      "Epoch:895, P:None, Loss:67.8388, Loss-ae:67.8388, Loss-topo:111914.3703\n",
      "Epoch:896, P:None, Loss:68.2285, Loss-ae:68.2285, Loss-topo:106344.0906\n",
      "Epoch:897, P:None, Loss:66.8410, Loss-ae:66.8410, Loss-topo:103474.8047\n",
      "Epoch:898, P:None, Loss:68.3846, Loss-ae:68.3846, Loss-topo:106728.8344\n",
      "Epoch:899, P:None, Loss:69.0584, Loss-ae:69.0584, Loss-topo:112214.4234\n",
      "Epoch:900, P:None, Loss:68.1753, Loss-ae:68.1753, Loss-topo:103049.6250\n",
      "Epoch:901, P:None, Loss:67.4769, Loss-ae:67.4769, Loss-topo:105028.2688\n",
      "Epoch:902, P:None, Loss:68.2564, Loss-ae:68.2564, Loss-topo:106951.5688\n",
      "Epoch:903, P:None, Loss:68.7826, Loss-ae:68.7826, Loss-topo:112455.0000\n",
      "Epoch:904, P:None, Loss:67.7013, Loss-ae:67.7013, Loss-topo:108610.1500\n",
      "Epoch:905, P:None, Loss:67.4993, Loss-ae:67.4993, Loss-topo:108377.0547\n",
      "Epoch:906, P:None, Loss:67.9045, Loss-ae:67.9045, Loss-topo:106105.9227\n",
      "Epoch:907, P:None, Loss:67.1828, Loss-ae:67.1828, Loss-topo:106853.4812\n",
      "Epoch:908, P:None, Loss:68.2953, Loss-ae:68.2953, Loss-topo:107400.6234\n",
      "Epoch:909, P:None, Loss:68.9533, Loss-ae:68.9533, Loss-topo:109002.8078\n",
      "Epoch:910, P:None, Loss:68.7381, Loss-ae:68.7381, Loss-topo:112907.0141\n",
      "Epoch:911, P:None, Loss:67.8695, Loss-ae:67.8695, Loss-topo:112042.7406\n",
      "Epoch:912, P:None, Loss:68.3061, Loss-ae:68.3061, Loss-topo:109696.1734\n",
      "Epoch:913, P:None, Loss:67.7443, Loss-ae:67.7443, Loss-topo:106227.6781\n",
      "Epoch:914, P:None, Loss:67.1224, Loss-ae:67.1224, Loss-topo:103348.3477\n",
      "Epoch:915, P:None, Loss:68.6802, Loss-ae:68.6802, Loss-topo:110319.4078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:916, P:None, Loss:68.2334, Loss-ae:68.2334, Loss-topo:106408.8828\n",
      "Epoch:917, P:None, Loss:68.0384, Loss-ae:68.0384, Loss-topo:104523.9187\n",
      "Epoch:918, P:None, Loss:68.3299, Loss-ae:68.3299, Loss-topo:103228.3125\n",
      "Epoch:919, P:None, Loss:68.3542, Loss-ae:68.3542, Loss-topo:100953.7266\n",
      "Epoch:920, P:None, Loss:67.8376, Loss-ae:67.8376, Loss-topo:105641.5125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m title_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMotionSense 20Hz\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTopoAE lambda \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_lam)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtopo_reducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_HD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle_plot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle_plot\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/darlinne.soto/librep-hiaac/experiments/Topological_ae/KuHar20Hz/../../../librep/transforms/topo_ae.py:77\u001b[0m, in \u001b[0;36mTopologicalDimensionalityReduction.fit\u001b[0;34m(self, X, y, title_plot)\u001b[0m\n\u001b[1;32m     75\u001b[0m in_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(reshaped_data, device\u001b[38;5;241m=\u001b[39mcuda0)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(cuda0)\n\u001b[0;32m---> 77\u001b[0m loss, loss_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     79\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1427\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1426\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/darlinne.soto/librep-hiaac/experiments/Topological_ae/KuHar20Hz/../../../librep/estimators/ae/torch/models/topological_ae/topological_ae.py:68\u001b[0m, in \u001b[0;36mTopologicallyRegularizedAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m         ae_loss, ae_loss_comp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder(x)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#         print('TEST'*20)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#         print(self.topo_sig(x_distances, latent_distances))\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m         topo_error, topo_error_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopo_sig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx_distances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_distances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;66;03m# normalize topo_error according to batch_size\u001b[39;00m\n\u001b[1;32m     72\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m dimensions[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1427\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1426\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/darlinne.soto/librep-hiaac/experiments/Topological_ae/KuHar20Hz/../../../librep/estimators/ae/torch/models/topological_ae/topological_signature_distance.py:86\u001b[0m, in \u001b[0;36mTopologicalSignatureDistance.forward\u001b[0;34m(self, distances1, distances2)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, distances1, distances2):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124;03m\"\"\"Return topological distance of two pairwise distance matrices.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        distances1: Distance matrix in space 1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m        distance, dict(additional outputs)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     pairs1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_pairings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistances1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     pairs2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pairings(distances2)\n\u001b[1;32m     89\u001b[0m     distance_components \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics.matched_pairs_0D\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_matching_pairs(\n\u001b[1;32m     91\u001b[0m             pairs1[\u001b[38;5;241m0\u001b[39m], pairs2[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     92\u001b[0m     }\n",
      "File \u001b[0;32m/home/darlinne.soto/librep-hiaac/experiments/Topological_ae/KuHar20Hz/../../../librep/estimators/ae/torch/models/topological_ae/topological_signature_distance.py:41\u001b[0m, in \u001b[0;36mTopologicalSignatureDistance._get_pairings\u001b[0;34m(self, distances)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_pairings\u001b[39m(\u001b[38;5;28mself\u001b[39m, distances):\n\u001b[0;32m---> 41\u001b[0m     pairs_0, pairs_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature_calculator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistances\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pairs_0, pairs_1\n",
      "File \u001b[0;32m/home/darlinne.soto/librep-hiaac/experiments/Topological_ae/KuHar20Hz/../../../librep/estimators/ae/torch/models/topological_ae/topology.py:79\u001b[0m, in \u001b[0;36mPersistentHomologyCalculation.__call__\u001b[0;34m(self, matrix)\u001b[0m\n\u001b[1;32m     76\u001b[0m older_component \u001b[38;5;241m=\u001b[39m uf\u001b[38;5;241m.\u001b[39mfind(v)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Not an edge of the MST, so skip it\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43myounger_component\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43molder_component\u001b[49m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m younger_component \u001b[38;5;241m>\u001b[39m older_component:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98d66c-b630-493e-a3d0-6aee364a1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456c1f2-f856-44b5-b4c3-1c1b59781951",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52697960-6cb0-4f76-a033-21828d9effbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97126520-db06-4926-9f62-b8382a68d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d394d-adf1-4b47-90dc-47d402fc389b",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a246d2-1000-4a0b-a833-e13b05bfaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 1\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd10d4c-3f50-46a1-89e7-5d03513b1908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3455110-7d0f-4ae3-981b-dfc4d60b390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a5042-2a17-405a-9050-d4db3696d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc9431-db7f-4d90-842a-5973855211e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db0e416-dedc-42bf-ae22-3de332d7f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc62e6a4-6962-47e2-84f9-29fff5c662f3",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb29ec-76b3-4f98-bdd5-404e64da5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 5\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8782b60-204a-4e91-a8fd-a56ab3d50a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a924330-75ac-40f1-9756-b7f56c2df3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a64ca-2da3-41bb-b20c-3566ae56876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8dac9c-8a14-41a9-8974-27708e65a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f9fad-ac7a-4e0c-986b-9b4488e9f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9003d3-7f20-4458-bcf2-29e3e96eeb2d",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58053da5-5f27-4cc1-b960-a1091462f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 10\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90b324-05ae-46ea-871b-cd1bf602cf57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f32e46-bfa8-4fd9-97e4-becfe3c0c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2948fd-3ff5-4055-bd5d-1ea460ece54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f48f0c-8167-4f92-b5e3-41026aeca3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a7cfa-75b3-453e-b1d9-ded339fb22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b63bc06-eb93-4755-8ee3-6add8e40f7c1",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454ea1e-b392-4867-8f6d-8d04ffcee910",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 100\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122769a9-ea11-49de-8476-cd57135a5f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bb0dd-d189-4e9b-83de-2d9371e450cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a42d610-e5ac-4c50-97cd-830abea35003",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae177f50-7f6e-4af4-85c4-94684b3557aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4c244-97bd-4d17-b263-88ec61dcfef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52efb733-3f7f-47a1-9520-2b1f7f249ebb",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288716cc-11e3-430b-84d8-b0171c9381e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 0.1\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d4038-557d-47b5-af9e-e699b5375d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eac8b7-edf5-43a0-b165-c2a3bac3fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3af3aa-cf3e-487a-a4cb-7372e2fa878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76956d30-c84a-4709-8fa7-5d40b5ea3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a10091-8295-484a-b3f6-46553c9cc6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e376a06-d032-4217-83a6-b471479895d2",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802da098-9f7d-43bc-b38a-f593ce163bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 0.01\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf236a2-d382-41f2-9b2d-ff2a7f2ece1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423f23b-d40a-4e92-bcea-38a5c7614781",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7680b3bf-8516-491f-8507-664ef97be97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3498b7-6cb3-4323-a28f-7a32e3c71e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc4944-7ea5-4b73-9f88-8f4cdcf899b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
