{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40df6f11-a295-4bb3-8ad7-c167a0adb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4fb8f2-0dc0-4673-813c-885f3c91dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ae4be2-202a-476a-85fa-478520f4a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0610c100-8395-4d7d-9c42-e666a4100e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 19:08:04.574415: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-08 19:08:04.574435: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from librep.datasets.har.loaders import MotionSense_BalancedView20HZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760926cb-890f-44a1-8cb4-df665c8ae570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librep.transforms.topo_ae import (\n",
    "    TopologicalDimensionalityReduction,\n",
    "    CustomTopoDimRedTransform\n",
    ")\n",
    "from librep.metrics.dimred_evaluator import DimensionalityReductionQualityReport\n",
    "from Experiments_topoae_KuHar20Hz_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a423355-2d96-48a5-918f-e29e4777de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from librep.utils.workflow import SimpleTrainEvalWorkflow, MultiRunWorkflow\n",
    "# from librep.metrics.report import ClassificationReport\n",
    "# from librep.estimators import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# from librep.transforms.topo_ae import TopologicalDimensionalityReduction\n",
    "# from librep.estimators.ae.torch.models.topological_ae.topological_ae import TopologicallyRegularizedAutoencoder\n",
    "\n",
    "\n",
    "# from librep.transforms import UMAP\n",
    "# from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c608afd1-657c-403d-b497-1a27a6a8d480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Balanced MotionSense View Resampled to 20Hz with Gravity - Multiplied acc by 9.81m/sÂ²\n",
       "\n",
       "This is a view from [MotionSense] that was spllited into 3s windows and was resampled to 20Hz using the [FFT method](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.resample.html#scipy.signal.resample). \n",
       "\n",
       "The data was first splitted in three sets: train, validation and test. Each one with the following proportions:\n",
       "- Train: 70% of samples\n",
       "- Validation: 10% of samples\n",
       "- Test: 20% of samples\n",
       "\n",
       "After splits, the datasets were balanced in relation to the activity code column, that is, each subset have the same number of activitiy samples.\n",
       "\n",
       "**NOTE**: Each subset contain samples from distinct users, that is, samples of one user belongs exclusivelly to one of three subsets.\n",
       "\n",
       "## Activity codes\n",
       "- 0: downstairs (569 train, 101 validation, 170 test) \n",
       "- 1: upstairs (569 train, 101 validation, 170 test) \n",
       "- 2: sitting (569 train, 101 validation, 170 test) \n",
       "- 3: standing (569 train, 101 validation, 170 test) \n",
       "- 4: walking (569 train, 101 validation, 170 test) \n",
       "- 5: jogging (569 train, 101 validation, 170 test) \n",
       " \n",
       "\n",
       "## Standartized activity codes\n",
       "- 0: sit (569 train, 101 validation, 170 test) \n",
       "- 1: stand (569 train, 101 validation, 170 test) \n",
       "- 2: walk (569 train, 101 validation, 170 test) \n",
       "- 3: stair up (569 train, 101 validation, 170 test) \n",
       "- 4: stair down (569 train, 101 validation, 170 test) \n",
       "- 5: run (569 train, 101 validation, 170 test) \n",
       "      \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MotionSense Loader\n",
    "loader = MotionSense_BalancedView20HZ(\n",
    "    root_dir=\"../../../data/views/MotionSense/balanced_view_20Hz_with_gravity_9.81_acc_standard\", \n",
    "    download=False\n",
    ")\n",
    "\n",
    "# Print the readme (optional)\n",
    "loader.print_readme()\n",
    "# kuhar_data = obtainKuHar20Hz()\n",
    "# train_HD = kuhar_data['train_HD']\n",
    "# train_LD = kuhar_data['train_LD']\n",
    "# train_Y = kuhar_data['train_Y']\n",
    "# test_HD = kuhar_data['test_HD']\n",
    "# test_LD = kuhar_data['test_LD']\n",
    "# test_Y = kuhar_data['test_Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f7a12c-ed5b-4424-bdf2-0635e4393c6a",
   "metadata": {},
   "source": [
    "# Preparing Table\n",
    "\n",
    "Columns:\n",
    "* RF (Accuracy, F1)\n",
    "* SVC (Accuracy, F1)\n",
    "* KNN (Accuracy, F1)\n",
    "* Trustworthiness\n",
    "* Continuity\n",
    "* Co-k-nearest-neighbor-size\n",
    "\n",
    "Rows:\n",
    "* UMAP (2 dim)\n",
    "* Generic Autoencoders\n",
    "* Topological autoencoders (L=1)\n",
    "* Topological autoencoders (L=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a4c4d9e-dcbe-4ab1-90d2-d9d5df30229c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=4020, features=360, no. window=6, label_columns='standard activity code',\n",
       " PandasMultiModalDataset: samples=1020, features=360, no. window=6, label_columns='standard activity code')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# If concat_train_validation is true, return a tuple (train+validation, test)\n",
    "train_val, test = loader.load(concat_train_validation=True, label=loader.standard_label)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "054979cd-4aa7-43c6-9247-5ab0bdfb56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_HD = np.array(train_val[:][0])\n",
    "train_Y = np.array(train_val[:][1])\n",
    "test_HD = np.array(test[:][0])\n",
    "test_Y = np.array(test[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cfa5f5a-27d0-4451-84ea-5374cd062905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-49.37072418962073 61.15645404569216\n",
      "-46.928394719326405 60.69211608524809\n"
     ]
    }
   ],
   "source": [
    "print(np.min(train_val[:][0]), np.max(train_val[:][0]))\n",
    "print(np.min(test[:][0]), np.max(test[:][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5f2d2-e075-4f6d-9152-2953252757b2",
   "metadata": {},
   "source": [
    "# Aplicar FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e5db146-cf08-4327-a760-eadde3f7e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_transform = FFT(centered = True)\n",
    "transformer = TransformMultiModalDataset(\n",
    "    transforms=[fft_transform],\n",
    "    new_window_name_prefix=\"fft.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1daf6cbd-f477-4c77-b430-872680d2bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_fft = transformer(train_val)\n",
    "# validation_dataset_fft = transformer(validation_dataset)\n",
    "test_dataset_fft = transformer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8078650a-5885-4273-b8ea-9bdd158311cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4020, 180)\n",
      "(1020, 180)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_fft.X.shape)\n",
    "# print(validation_dataset_fft.X.shape)\n",
    "print(test_dataset_fft.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e488804-1b51-4489-bde3-5410a6f06b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797.3368276806895\n"
     ]
    }
   ],
   "source": [
    "print(np.max(train_dataset_fft.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83f04cd4-4b02-43a5-9516-bf5d65c1180d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[117.47446671,   7.69616708,   9.12345317, ...,   1.58565333,\n",
       "          1.92709353,   2.56103481],\n",
       "       [ 56.99305528,   7.03810632,  21.13174295, ...,   3.59397955,\n",
       "          1.24491107,   2.19807336],\n",
       "       [ 43.67148129,   9.31303897,  48.54955013, ...,   0.20668447,\n",
       "          2.1782541 ,   1.17453151],\n",
       "       ...,\n",
       "       [ 48.49117037,   5.91621941,  13.46303591, ...,   0.95960646,\n",
       "          3.65355504,   4.60541355],\n",
       "       [ 88.69039888,  27.8862055 ,  19.11225658, ...,   1.48263297,\n",
       "          3.9119881 ,   2.55326373],\n",
       "       [ 55.520648  ,   2.58290685,   8.95693627, ...,   5.4340967 ,\n",
       "          4.2408613 ,  10.63953696]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_fft.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4092f242-283d-4465-9b88-0829ac9317f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_HD = train_dataset_fft.X\n",
    "train_LD = None\n",
    "train_Y = train_dataset_fft.y\n",
    "test_HD = test_dataset_fft.X\n",
    "test_LD = None\n",
    "test_Y = test_dataset_fft.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bdc1702-1f63-426c-b03c-847684491912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4020, 180) (4020,) (1020, 180) (1020,)\n"
     ]
    }
   ],
   "source": [
    "print(train_HD.shape, train_Y.shape, test_HD.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fab09893-141b-4812-81ba-576f9d72cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.983111398280336e-05 797.3368276806895\n",
      "1.2681145826193632e-05 787.0749219958633\n"
     ]
    }
   ],
   "source": [
    "print(np.min(train_HD), np.max(train_HD))\n",
    "print(np.min(test_HD), np.max(test_HD))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a3c1d5-1e3c-4bf2-b4cb-86dabeb01b2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualization helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41934c11-dbd6-4ac2-956d-d6a103e1a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: sit (569 train, 101 validation, 170 test)\n",
    "# 1: stand (569 train, 101 validation, 170 test)\n",
    "# 2: walk (569 train, 101 validation, 170 test)\n",
    "# 3: stair up (569 train, 101 validation, 170 test)\n",
    "# 4: stair down (569 train, 101 validation, 170 test)\n",
    "# 5: run (569 train, 101 validation, 170 test)\n",
    "def visualize(X, Y):\n",
    "    labels = ['sit', 'stand', 'walk', 'stair up', 'stair down', 'run']\n",
    "    df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=Y))\n",
    "    groups = df.groupby('label')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.margins(0.05)\n",
    "    for name, group in groups:\n",
    "        ax.plot(group.x, group.y, marker='.', linestyle='', ms=8, label=labels[name])\n",
    "    # ax.legend()\n",
    "    # Shrink current axis by 20%\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "    # Put a legend to the right of the current axis\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71409f92-8cd5-434b-919f-d068996dbc66",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Applying Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9b17e-f585-4143-942f-b9138a69c24e",
   "metadata": {},
   "source": [
    "MinMaxScaler, MaxAbsScaler and StandardScaler apply the scaling PER FEATURE, which means the distance between points would actually be modified, and so, the ranking as well.\n",
    "Because of this, a new Scaler is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46248f1a-e532-4c3a-8f3d-5ce8f97a95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "\n",
    "# scaler = preprocessing.MinMaxScaler()\n",
    "# scaler.fit(train_dataset_fft.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7743e300-d869-4128-a490-f6eac79155fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_HD = scaler.transform(train_dataset_fft.X)\n",
    "# train_LD = None\n",
    "# # train_Y = train_dataset_fft.y\n",
    "# test_HD = scaler.transform(test_dataset_fft.X)\n",
    "# test_LD = None\n",
    "# # test_Y = test_dataset_fft.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2d91f41-34e0-4416-b334-a199b18979d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.min(train_HD), np.max(train_HD))\n",
    "# print(np.min(test_HD), np.max(test_HD))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a74dc-f5b0-4bcb-9027-8f301cd4fc9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set Reporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "517c120c-b28a-4940-a6e6-278ab9bd98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporter = ClassificationReport(\n",
    "    use_accuracy=True, \n",
    "    use_f1_score=True,\n",
    "    use_classification_report=False,\n",
    "    use_confusion_matrix=False,\n",
    "    plot_confusion_matrix=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0f3ee-9908-4f24-90a7-f1f26bcedbd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **SECTION:** Exploring Topological AE (lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16134b93-4594-498e-9acf-e9feb00bc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_to_explore = [0, 1, 5, 10, 100, 500]\n",
    "lambdas_to_explore = [1000]\n",
    "executions_per_model = 10\n",
    "executions_per_model = 1\n",
    "\n",
    "def explore_lambda(train_HD, train_Y, test_HD, test_Y, topoae_lambda, times_to_execute=10):\n",
    "    result_object = {\n",
    "        'RF-ACC': [],\n",
    "        'RF-F1': [],\n",
    "        'SVC-ACC': [],\n",
    "        'SVC-F1': [],\n",
    "        'KNN-ACC': [],\n",
    "        'KNN-F1': [],\n",
    "        'Trustworthiness': [],\n",
    "        'Continuity': [],\n",
    "        'Co-k-NNs': []   \n",
    "    }\n",
    "    for _ in range(times_to_execute):\n",
    "        model_epc = 600\n",
    "        topo_reducer = CustomTopoDimRedTransform(\n",
    "            model_name='DeepAE_custom_dim2',\n",
    "            model_lambda=topoae_lambda,\n",
    "            patience=None,\n",
    "            num_epochs=model_epc,\n",
    "            from_dim=180,\n",
    "            to_dim=10\n",
    "        )\n",
    "        title_plot = \"MotionSense 20Hz\\nTopoAE lambda \" + str(topoae_lambda)\n",
    "        topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)\n",
    "        train_LD = np.reshape(topo_reducer.transform(train_HD), (-1,10))\n",
    "        test_LD = np.reshape(topo_reducer.transform(test_HD), (-1,10))\n",
    "        experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "        metrics_reporter = DimensionalityReductionQualityReport()\n",
    "        metrics_report = metrics_reporter.evaluate([test_HD, test_LD])\n",
    "        \n",
    "        result_object['RF-ACC'].append(experiments_result['RF-ACC'])\n",
    "        result_object['RF-F1'].append(experiments_result['RF-F1'])\n",
    "        result_object['SVC-ACC'].append(experiments_result['SVC-ACC'])\n",
    "        result_object['SVC-F1'].append(experiments_result['SVC-F1'])\n",
    "        result_object['KNN-ACC'].append(experiments_result['KNN-ACC'])\n",
    "        result_object['KNN-F1'].append(experiments_result['KNN-F1'])\n",
    "        \n",
    "        result_object['Trustworthiness'].append(metrics_report['trustworthiness'])\n",
    "        result_object['Continuity'].append(metrics_report['continuity'])\n",
    "        result_object['Co-k-NNs'].append(metrics_report['co k nearest neighbor size'])\n",
    "    return result_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "048b3fb3-798a-4999-baa4-2bb29ab10f33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topologically Regularized DeepAE_custom_dim2\n",
      "Using python to compute signatures\n",
      "DeepAE_custom_dim, Input: (1, 180) Inner dim: 10\n",
      "Epoch:1, P:None, Loss:968.5673, Loss-ae:922.5097, Loss-topo:0.0461\n",
      "Epoch:2, P:None, Loss:763.1454, Loss-ae:736.6598, Loss-topo:0.0265\n",
      "Epoch:3, P:None, Loss:948.9246, Loss-ae:945.3133, Loss-topo:0.0036\n",
      "Epoch:4, P:None, Loss:760.7613, Loss-ae:735.3443, Loss-topo:0.0254\n",
      "Epoch:5, P:None, Loss:742.8690, Loss-ae:729.1104, Loss-topo:0.0138\n",
      "Epoch:6, P:None, Loss:732.9295, Loss-ae:723.8460, Loss-topo:0.0091\n",
      "Epoch:7, P:None, Loss:753.2179, Loss-ae:747.5376, Loss-topo:0.0057\n",
      "Epoch:8, P:None, Loss:775.3448, Loss-ae:773.2660, Loss-topo:0.0021\n",
      "Epoch:9, P:None, Loss:719.9686, Loss-ae:715.9608, Loss-topo:0.0040\n",
      "Epoch:10, P:None, Loss:704.7707, Loss-ae:699.2100, Loss-topo:0.0056\n",
      "Epoch:11, P:None, Loss:608.1164, Loss-ae:599.6828, Loss-topo:0.0084\n",
      "Epoch:12, P:None, Loss:465.5195, Loss-ae:457.4541, Loss-topo:0.0081\n",
      "Epoch:13, P:None, Loss:421.9740, Loss-ae:417.3572, Loss-topo:0.0046\n",
      "Epoch:14, P:None, Loss:387.7284, Loss-ae:383.7208, Loss-topo:0.0040\n",
      "Epoch:15, P:None, Loss:392.9766, Loss-ae:388.0642, Loss-topo:0.0049\n",
      "Epoch:16, P:None, Loss:374.4960, Loss-ae:372.0646, Loss-topo:0.0024\n",
      "Epoch:17, P:None, Loss:375.8595, Loss-ae:371.3956, Loss-topo:0.0045\n",
      "Epoch:18, P:None, Loss:378.7980, Loss-ae:375.5918, Loss-topo:0.0032\n",
      "Epoch:19, P:None, Loss:374.7777, Loss-ae:371.2670, Loss-topo:0.0035\n",
      "Epoch:20, P:None, Loss:363.3915, Loss-ae:358.6263, Loss-topo:0.0048\n",
      "Epoch:21, P:None, Loss:353.8596, Loss-ae:351.4853, Loss-topo:0.0024\n",
      "Epoch:22, P:None, Loss:360.4992, Loss-ae:357.9048, Loss-topo:0.0026\n",
      "Epoch:23, P:None, Loss:354.9065, Loss-ae:352.9304, Loss-topo:0.0020\n",
      "Epoch:24, P:None, Loss:365.1622, Loss-ae:362.6286, Loss-topo:0.0025\n",
      "Epoch:25, P:None, Loss:347.9961, Loss-ae:345.0725, Loss-topo:0.0029\n",
      "Epoch:26, P:None, Loss:340.9265, Loss-ae:339.8130, Loss-topo:0.0011\n",
      "Epoch:27, P:None, Loss:340.8124, Loss-ae:338.1305, Loss-topo:0.0027\n",
      "Epoch:28, P:None, Loss:332.9359, Loss-ae:329.7383, Loss-topo:0.0032\n",
      "Epoch:29, P:None, Loss:380.8519, Loss-ae:376.9977, Loss-topo:0.0039\n",
      "Epoch:30, P:None, Loss:340.6426, Loss-ae:338.9011, Loss-topo:0.0017\n",
      "Epoch:31, P:None, Loss:320.4532, Loss-ae:318.3016, Loss-topo:0.0022\n",
      "Epoch:32, P:None, Loss:323.6061, Loss-ae:320.5683, Loss-topo:0.0030\n",
      "Epoch:33, P:None, Loss:321.0834, Loss-ae:318.4280, Loss-topo:0.0027\n",
      "Epoch:34, P:None, Loss:326.2068, Loss-ae:323.8271, Loss-topo:0.0024\n",
      "Epoch:35, P:None, Loss:315.8985, Loss-ae:313.7343, Loss-topo:0.0022\n",
      "Epoch:36, P:None, Loss:309.1544, Loss-ae:307.1866, Loss-topo:0.0020\n",
      "Epoch:37, P:None, Loss:323.1324, Loss-ae:318.6222, Loss-topo:0.0045\n",
      "Epoch:38, P:None, Loss:309.5652, Loss-ae:306.3460, Loss-topo:0.0032\n",
      "Epoch:39, P:None, Loss:323.1230, Loss-ae:317.7991, Loss-topo:0.0053\n",
      "Epoch:40, P:None, Loss:299.0736, Loss-ae:297.2344, Loss-topo:0.0018\n",
      "Epoch:41, P:None, Loss:310.6271, Loss-ae:308.4635, Loss-topo:0.0022\n",
      "Epoch:42, P:None, Loss:370.3513, Loss-ae:367.3209, Loss-topo:0.0030\n",
      "Epoch:43, P:None, Loss:312.1104, Loss-ae:310.2334, Loss-topo:0.0019\n",
      "Epoch:44, P:None, Loss:309.2072, Loss-ae:306.6582, Loss-topo:0.0025\n",
      "Epoch:45, P:None, Loss:281.1453, Loss-ae:277.6198, Loss-topo:0.0035\n",
      "Epoch:46, P:None, Loss:277.4025, Loss-ae:271.7675, Loss-topo:0.0056\n",
      "Epoch:47, P:None, Loss:285.1604, Loss-ae:279.7203, Loss-topo:0.0054\n",
      "Epoch:48, P:None, Loss:268.3794, Loss-ae:265.3687, Loss-topo:0.0030\n",
      "Epoch:49, P:None, Loss:263.9731, Loss-ae:259.1722, Loss-topo:0.0048\n",
      "Epoch:50, P:None, Loss:255.7125, Loss-ae:250.8590, Loss-topo:0.0049\n",
      "Epoch:51, P:None, Loss:268.8583, Loss-ae:265.3015, Loss-topo:0.0036\n",
      "Epoch:52, P:None, Loss:250.4834, Loss-ae:247.4732, Loss-topo:0.0030\n",
      "Epoch:53, P:None, Loss:239.8688, Loss-ae:237.4417, Loss-topo:0.0024\n",
      "Epoch:54, P:None, Loss:245.7735, Loss-ae:242.8644, Loss-topo:0.0029\n",
      "Epoch:55, P:None, Loss:236.1128, Loss-ae:234.4920, Loss-topo:0.0016\n",
      "Epoch:56, P:None, Loss:252.0149, Loss-ae:248.7232, Loss-topo:0.0033\n",
      "Epoch:57, P:None, Loss:238.2097, Loss-ae:236.1337, Loss-topo:0.0021\n",
      "Epoch:58, P:None, Loss:234.8724, Loss-ae:232.1360, Loss-topo:0.0027\n",
      "Epoch:59, P:None, Loss:246.5145, Loss-ae:244.0072, Loss-topo:0.0025\n",
      "Epoch:60, P:None, Loss:228.6899, Loss-ae:227.2817, Loss-topo:0.0014\n",
      "Epoch:61, P:None, Loss:236.4369, Loss-ae:235.4071, Loss-topo:0.0010\n",
      "Epoch:62, P:None, Loss:233.5054, Loss-ae:231.4177, Loss-topo:0.0021\n",
      "Epoch:63, P:None, Loss:234.8204, Loss-ae:233.5162, Loss-topo:0.0013\n",
      "Epoch:64, P:None, Loss:236.6680, Loss-ae:234.6706, Loss-topo:0.0020\n",
      "Epoch:65, P:None, Loss:240.6648, Loss-ae:238.9619, Loss-topo:0.0017\n",
      "Epoch:66, P:None, Loss:233.1466, Loss-ae:231.7439, Loss-topo:0.0014\n",
      "Epoch:67, P:None, Loss:232.7602, Loss-ae:230.6455, Loss-topo:0.0021\n",
      "Epoch:68, P:None, Loss:229.9332, Loss-ae:228.8810, Loss-topo:0.0011\n",
      "Epoch:69, P:None, Loss:228.9422, Loss-ae:227.3614, Loss-topo:0.0016\n",
      "Epoch:70, P:None, Loss:228.9226, Loss-ae:227.5394, Loss-topo:0.0014\n",
      "Epoch:71, P:None, Loss:232.9694, Loss-ae:231.1928, Loss-topo:0.0018\n",
      "Epoch:72, P:None, Loss:230.9708, Loss-ae:229.7322, Loss-topo:0.0012\n",
      "Epoch:73, P:None, Loss:236.5956, Loss-ae:235.3612, Loss-topo:0.0012\n",
      "Epoch:74, P:None, Loss:230.1150, Loss-ae:228.7307, Loss-topo:0.0014\n",
      "Epoch:75, P:None, Loss:228.9032, Loss-ae:227.3180, Loss-topo:0.0016\n",
      "Epoch:76, P:None, Loss:227.0883, Loss-ae:225.1463, Loss-topo:0.0019\n",
      "Epoch:77, P:None, Loss:219.6605, Loss-ae:218.1686, Loss-topo:0.0015\n",
      "Epoch:78, P:None, Loss:228.7754, Loss-ae:227.5752, Loss-topo:0.0012\n",
      "Epoch:79, P:None, Loss:223.5734, Loss-ae:222.2404, Loss-topo:0.0013\n",
      "Epoch:80, P:None, Loss:231.7641, Loss-ae:230.2775, Loss-topo:0.0015\n",
      "Epoch:81, P:None, Loss:229.1732, Loss-ae:227.4883, Loss-topo:0.0017\n",
      "Epoch:82, P:None, Loss:213.3944, Loss-ae:211.4557, Loss-topo:0.0019\n",
      "Epoch:83, P:None, Loss:220.5778, Loss-ae:219.7871, Loss-topo:0.0008\n",
      "Epoch:84, P:None, Loss:228.7790, Loss-ae:226.9203, Loss-topo:0.0019\n",
      "Epoch:85, P:None, Loss:220.2966, Loss-ae:219.1529, Loss-topo:0.0011\n",
      "Epoch:86, P:None, Loss:231.6465, Loss-ae:229.9011, Loss-topo:0.0017\n",
      "Epoch:87, P:None, Loss:226.2124, Loss-ae:225.3566, Loss-topo:0.0009\n",
      "Epoch:88, P:None, Loss:220.9967, Loss-ae:219.9672, Loss-topo:0.0010\n",
      "Epoch:89, P:None, Loss:223.3064, Loss-ae:221.8029, Loss-topo:0.0015\n",
      "Epoch:90, P:None, Loss:226.5172, Loss-ae:225.6876, Loss-topo:0.0008\n",
      "Epoch:91, P:None, Loss:215.8416, Loss-ae:214.6855, Loss-topo:0.0012\n",
      "Epoch:92, P:None, Loss:213.5856, Loss-ae:212.7636, Loss-topo:0.0008\n",
      "Epoch:93, P:None, Loss:219.6420, Loss-ae:218.2823, Loss-topo:0.0014\n",
      "Epoch:94, P:None, Loss:212.6768, Loss-ae:211.7276, Loss-topo:0.0009\n",
      "Epoch:95, P:None, Loss:216.3484, Loss-ae:215.3916, Loss-topo:0.0010\n",
      "Epoch:96, P:None, Loss:211.9773, Loss-ae:211.0836, Loss-topo:0.0009\n",
      "Epoch:97, P:None, Loss:225.7641, Loss-ae:225.2408, Loss-topo:0.0005\n",
      "Epoch:98, P:None, Loss:225.6094, Loss-ae:225.1055, Loss-topo:0.0005\n",
      "Epoch:99, P:None, Loss:212.7063, Loss-ae:211.7532, Loss-topo:0.0010\n",
      "Epoch:100, P:None, Loss:216.3829, Loss-ae:215.7293, Loss-topo:0.0007\n",
      "Epoch:101, P:None, Loss:213.5729, Loss-ae:212.9558, Loss-topo:0.0006\n",
      "Epoch:102, P:None, Loss:207.2761, Loss-ae:206.1237, Loss-topo:0.0012\n",
      "Epoch:103, P:None, Loss:202.8708, Loss-ae:202.1524, Loss-topo:0.0007\n",
      "Epoch:104, P:None, Loss:212.0575, Loss-ae:211.4348, Loss-topo:0.0006\n",
      "Epoch:105, P:None, Loss:211.1670, Loss-ae:210.3635, Loss-topo:0.0008\n",
      "Epoch:106, P:None, Loss:208.1766, Loss-ae:207.6145, Loss-topo:0.0006\n",
      "Epoch:107, P:None, Loss:207.0764, Loss-ae:206.1542, Loss-topo:0.0009\n",
      "Epoch:108, P:None, Loss:204.9269, Loss-ae:204.4874, Loss-topo:0.0004\n",
      "Epoch:109, P:None, Loss:221.9850, Loss-ae:220.6885, Loss-topo:0.0013\n",
      "Epoch:110, P:None, Loss:210.2083, Loss-ae:208.8647, Loss-topo:0.0013\n",
      "Epoch:111, P:None, Loss:209.2282, Loss-ae:207.9328, Loss-topo:0.0013\n",
      "Epoch:112, P:None, Loss:207.9119, Loss-ae:207.1147, Loss-topo:0.0008\n",
      "Epoch:113, P:None, Loss:204.9994, Loss-ae:204.0496, Loss-topo:0.0009\n",
      "Epoch:114, P:None, Loss:208.3790, Loss-ae:207.6633, Loss-topo:0.0007\n",
      "Epoch:115, P:None, Loss:201.8370, Loss-ae:201.2711, Loss-topo:0.0006\n",
      "Epoch:116, P:None, Loss:207.7991, Loss-ae:207.2689, Loss-topo:0.0005\n",
      "Epoch:117, P:None, Loss:207.6521, Loss-ae:207.1934, Loss-topo:0.0005\n",
      "Epoch:118, P:None, Loss:203.5680, Loss-ae:202.2286, Loss-topo:0.0013\n",
      "Epoch:119, P:None, Loss:208.1712, Loss-ae:207.3555, Loss-topo:0.0008\n",
      "Epoch:120, P:None, Loss:210.6758, Loss-ae:210.1410, Loss-topo:0.0005\n",
      "Epoch:121, P:None, Loss:203.6737, Loss-ae:202.6240, Loss-topo:0.0010\n",
      "Epoch:122, P:None, Loss:203.6393, Loss-ae:202.6724, Loss-topo:0.0010\n",
      "Epoch:123, P:None, Loss:197.7635, Loss-ae:196.8251, Loss-topo:0.0009\n",
      "Epoch:124, P:None, Loss:197.1016, Loss-ae:196.6805, Loss-topo:0.0004\n",
      "Epoch:125, P:None, Loss:202.2442, Loss-ae:201.7399, Loss-topo:0.0005\n",
      "Epoch:126, P:None, Loss:198.2801, Loss-ae:197.2739, Loss-topo:0.0010\n",
      "Epoch:127, P:None, Loss:198.2505, Loss-ae:197.6917, Loss-topo:0.0006\n",
      "Epoch:128, P:None, Loss:201.3358, Loss-ae:200.8769, Loss-topo:0.0005\n",
      "Epoch:129, P:None, Loss:199.9930, Loss-ae:199.2602, Loss-topo:0.0007\n",
      "Epoch:130, P:None, Loss:198.7119, Loss-ae:197.8939, Loss-topo:0.0008\n",
      "Epoch:131, P:None, Loss:207.1312, Loss-ae:205.9585, Loss-topo:0.0012\n",
      "Epoch:132, P:None, Loss:201.4097, Loss-ae:200.5124, Loss-topo:0.0009\n",
      "Epoch:133, P:None, Loss:199.6228, Loss-ae:198.9046, Loss-topo:0.0007\n",
      "Epoch:134, P:None, Loss:199.7926, Loss-ae:199.1871, Loss-topo:0.0006\n",
      "Epoch:135, P:None, Loss:200.8484, Loss-ae:199.6871, Loss-topo:0.0012\n",
      "Epoch:136, P:None, Loss:200.2724, Loss-ae:199.0535, Loss-topo:0.0012\n",
      "Epoch:137, P:None, Loss:195.5094, Loss-ae:194.7631, Loss-topo:0.0007\n",
      "Epoch:138, P:None, Loss:196.7599, Loss-ae:196.0952, Loss-topo:0.0007\n",
      "Epoch:139, P:None, Loss:200.4871, Loss-ae:199.6504, Loss-topo:0.0008\n",
      "Epoch:140, P:None, Loss:202.2723, Loss-ae:201.7974, Loss-topo:0.0005\n",
      "Epoch:141, P:None, Loss:202.7632, Loss-ae:202.0862, Loss-topo:0.0007\n",
      "Epoch:142, P:None, Loss:197.4978, Loss-ae:196.7091, Loss-topo:0.0008\n",
      "Epoch:143, P:None, Loss:196.4963, Loss-ae:195.9158, Loss-topo:0.0006\n",
      "Epoch:144, P:None, Loss:204.4182, Loss-ae:203.3593, Loss-topo:0.0011\n",
      "Epoch:145, P:None, Loss:193.9841, Loss-ae:193.1438, Loss-topo:0.0008\n",
      "Epoch:146, P:None, Loss:189.1158, Loss-ae:188.2913, Loss-topo:0.0008\n",
      "Epoch:147, P:None, Loss:193.4266, Loss-ae:192.7384, Loss-topo:0.0007\n",
      "Epoch:148, P:None, Loss:194.3942, Loss-ae:193.3111, Loss-topo:0.0011\n",
      "Epoch:149, P:None, Loss:195.3774, Loss-ae:194.7648, Loss-topo:0.0006\n",
      "Epoch:150, P:None, Loss:199.8615, Loss-ae:199.0498, Loss-topo:0.0008\n",
      "Epoch:151, P:None, Loss:194.1133, Loss-ae:193.5411, Loss-topo:0.0006\n",
      "Epoch:152, P:None, Loss:196.3935, Loss-ae:195.4348, Loss-topo:0.0010\n",
      "Epoch:153, P:None, Loss:191.5838, Loss-ae:191.0995, Loss-topo:0.0005\n",
      "Epoch:154, P:None, Loss:196.0501, Loss-ae:195.2122, Loss-topo:0.0008\n",
      "Epoch:155, P:None, Loss:192.9433, Loss-ae:192.5271, Loss-topo:0.0004\n",
      "Epoch:156, P:None, Loss:201.8042, Loss-ae:201.3473, Loss-topo:0.0005\n",
      "Epoch:157, P:None, Loss:193.6958, Loss-ae:192.9275, Loss-topo:0.0008\n",
      "Epoch:158, P:None, Loss:190.1933, Loss-ae:189.5260, Loss-topo:0.0007\n",
      "Epoch:159, P:None, Loss:198.2876, Loss-ae:197.6747, Loss-topo:0.0006\n",
      "Epoch:160, P:None, Loss:194.9508, Loss-ae:194.4083, Loss-topo:0.0005\n",
      "Epoch:161, P:None, Loss:182.9690, Loss-ae:182.6894, Loss-topo:0.0003\n",
      "Epoch:162, P:None, Loss:199.1330, Loss-ae:198.6856, Loss-topo:0.0004\n",
      "Epoch:163, P:None, Loss:194.1008, Loss-ae:193.4850, Loss-topo:0.0006\n",
      "Epoch:164, P:None, Loss:194.3441, Loss-ae:193.9597, Loss-topo:0.0004\n",
      "Epoch:165, P:None, Loss:190.9018, Loss-ae:190.4829, Loss-topo:0.0004\n",
      "Epoch:166, P:None, Loss:188.8554, Loss-ae:188.3313, Loss-topo:0.0005\n",
      "Epoch:167, P:None, Loss:191.4262, Loss-ae:190.9114, Loss-topo:0.0005\n",
      "Epoch:168, P:None, Loss:194.5536, Loss-ae:193.8476, Loss-topo:0.0007\n",
      "Epoch:169, P:None, Loss:196.5216, Loss-ae:195.9162, Loss-topo:0.0006\n",
      "Epoch:170, P:None, Loss:193.6633, Loss-ae:193.2007, Loss-topo:0.0005\n",
      "Epoch:171, P:None, Loss:192.0501, Loss-ae:191.1285, Loss-topo:0.0009\n",
      "Epoch:172, P:None, Loss:181.5331, Loss-ae:181.0512, Loss-topo:0.0005\n",
      "Epoch:173, P:None, Loss:184.1276, Loss-ae:183.5711, Loss-topo:0.0006\n",
      "Epoch:174, P:None, Loss:199.5431, Loss-ae:198.9360, Loss-topo:0.0006\n",
      "Epoch:175, P:None, Loss:186.2270, Loss-ae:185.5219, Loss-topo:0.0007\n",
      "Epoch:176, P:None, Loss:184.9451, Loss-ae:184.3922, Loss-topo:0.0006\n",
      "Epoch:177, P:None, Loss:189.2948, Loss-ae:188.8490, Loss-topo:0.0004\n",
      "Epoch:178, P:None, Loss:186.1915, Loss-ae:185.5580, Loss-topo:0.0006\n",
      "Epoch:179, P:None, Loss:189.2036, Loss-ae:188.6141, Loss-topo:0.0006\n",
      "Epoch:180, P:None, Loss:189.8622, Loss-ae:189.4156, Loss-topo:0.0004\n",
      "Epoch:181, P:None, Loss:188.4080, Loss-ae:187.3621, Loss-topo:0.0010\n",
      "Epoch:182, P:None, Loss:187.3044, Loss-ae:187.0245, Loss-topo:0.0003\n",
      "Epoch:183, P:None, Loss:198.9949, Loss-ae:198.2835, Loss-topo:0.0007\n",
      "Epoch:184, P:None, Loss:184.2247, Loss-ae:183.5882, Loss-topo:0.0006\n",
      "Epoch:185, P:None, Loss:190.1022, Loss-ae:189.6325, Loss-topo:0.0005\n",
      "Epoch:186, P:None, Loss:184.6190, Loss-ae:184.0269, Loss-topo:0.0006\n",
      "Epoch:187, P:None, Loss:189.3939, Loss-ae:188.8857, Loss-topo:0.0005\n",
      "Epoch:188, P:None, Loss:183.2511, Loss-ae:182.8115, Loss-topo:0.0004\n",
      "Epoch:189, P:None, Loss:193.1816, Loss-ae:192.4267, Loss-topo:0.0008\n",
      "Epoch:190, P:None, Loss:191.0701, Loss-ae:190.6088, Loss-topo:0.0005\n",
      "Epoch:191, P:None, Loss:181.9952, Loss-ae:181.6228, Loss-topo:0.0004\n",
      "Epoch:192, P:None, Loss:183.7271, Loss-ae:183.3221, Loss-topo:0.0004\n",
      "Epoch:193, P:None, Loss:181.9082, Loss-ae:181.2719, Loss-topo:0.0006\n",
      "Epoch:194, P:None, Loss:178.6996, Loss-ae:178.2491, Loss-topo:0.0005\n",
      "Epoch:195, P:None, Loss:178.0622, Loss-ae:177.5637, Loss-topo:0.0005\n",
      "Epoch:196, P:None, Loss:178.0056, Loss-ae:177.6498, Loss-topo:0.0004\n",
      "Epoch:197, P:None, Loss:182.8550, Loss-ae:182.4859, Loss-topo:0.0004\n",
      "Epoch:198, P:None, Loss:177.9626, Loss-ae:177.3063, Loss-topo:0.0007\n",
      "Epoch:199, P:None, Loss:178.9314, Loss-ae:178.4087, Loss-topo:0.0005\n",
      "Epoch:200, P:None, Loss:182.3189, Loss-ae:182.0107, Loss-topo:0.0003\n",
      "Epoch:201, P:None, Loss:186.8670, Loss-ae:186.5458, Loss-topo:0.0003\n",
      "Epoch:202, P:None, Loss:179.4132, Loss-ae:179.0820, Loss-topo:0.0003\n",
      "Epoch:203, P:None, Loss:181.4970, Loss-ae:180.7529, Loss-topo:0.0007\n",
      "Epoch:204, P:None, Loss:183.5261, Loss-ae:183.1544, Loss-topo:0.0004\n",
      "Epoch:205, P:None, Loss:184.9719, Loss-ae:184.6605, Loss-topo:0.0003\n",
      "Epoch:206, P:None, Loss:179.8981, Loss-ae:179.5916, Loss-topo:0.0003\n",
      "Epoch:207, P:None, Loss:181.0616, Loss-ae:180.6228, Loss-topo:0.0004\n",
      "Epoch:208, P:None, Loss:181.8498, Loss-ae:181.4644, Loss-topo:0.0004\n",
      "Epoch:209, P:None, Loss:186.8042, Loss-ae:186.2885, Loss-topo:0.0005\n",
      "Epoch:210, P:None, Loss:180.5546, Loss-ae:179.8727, Loss-topo:0.0007\n",
      "Epoch:211, P:None, Loss:187.4257, Loss-ae:186.6796, Loss-topo:0.0007\n",
      "Epoch:212, P:None, Loss:198.1775, Loss-ae:197.6697, Loss-topo:0.0005\n",
      "Epoch:213, P:None, Loss:180.6441, Loss-ae:180.1061, Loss-topo:0.0005\n",
      "Epoch:214, P:None, Loss:182.5274, Loss-ae:181.9869, Loss-topo:0.0005\n",
      "Epoch:215, P:None, Loss:185.7277, Loss-ae:185.3911, Loss-topo:0.0003\n",
      "Epoch:216, P:None, Loss:183.7366, Loss-ae:183.2376, Loss-topo:0.0005\n",
      "Epoch:217, P:None, Loss:183.4440, Loss-ae:182.9328, Loss-topo:0.0005\n",
      "Epoch:218, P:None, Loss:185.1790, Loss-ae:184.9527, Loss-topo:0.0002\n",
      "Epoch:219, P:None, Loss:185.8320, Loss-ae:185.4959, Loss-topo:0.0003\n",
      "Epoch:220, P:None, Loss:181.3275, Loss-ae:180.9421, Loss-topo:0.0004\n",
      "Epoch:221, P:None, Loss:173.9165, Loss-ae:173.4337, Loss-topo:0.0005\n",
      "Epoch:222, P:None, Loss:179.1294, Loss-ae:178.7982, Loss-topo:0.0003\n",
      "Epoch:223, P:None, Loss:177.0815, Loss-ae:176.7009, Loss-topo:0.0004\n",
      "Epoch:224, P:None, Loss:178.7361, Loss-ae:178.3887, Loss-topo:0.0003\n",
      "Epoch:225, P:None, Loss:182.2884, Loss-ae:181.6836, Loss-topo:0.0006\n",
      "Epoch:226, P:None, Loss:172.8339, Loss-ae:172.5525, Loss-topo:0.0003\n",
      "Epoch:227, P:None, Loss:175.5544, Loss-ae:175.1075, Loss-topo:0.0004\n",
      "Epoch:228, P:None, Loss:180.3151, Loss-ae:179.9387, Loss-topo:0.0004\n",
      "Epoch:229, P:None, Loss:175.5102, Loss-ae:175.0821, Loss-topo:0.0004\n",
      "Epoch:230, P:None, Loss:175.5057, Loss-ae:175.2149, Loss-topo:0.0003\n",
      "Epoch:231, P:None, Loss:178.7625, Loss-ae:178.3158, Loss-topo:0.0004\n",
      "Epoch:232, P:None, Loss:180.0454, Loss-ae:179.7365, Loss-topo:0.0003\n",
      "Epoch:233, P:None, Loss:174.7481, Loss-ae:174.3477, Loss-topo:0.0004\n",
      "Epoch:234, P:None, Loss:174.9859, Loss-ae:174.6575, Loss-topo:0.0003\n",
      "Epoch:235, P:None, Loss:178.3305, Loss-ae:177.9576, Loss-topo:0.0004\n",
      "Epoch:236, P:None, Loss:176.8575, Loss-ae:176.5760, Loss-topo:0.0003\n",
      "Epoch:237, P:None, Loss:172.0950, Loss-ae:171.7504, Loss-topo:0.0003\n",
      "Epoch:238, P:None, Loss:177.3509, Loss-ae:176.9964, Loss-topo:0.0004\n",
      "Epoch:239, P:None, Loss:169.2886, Loss-ae:168.8824, Loss-topo:0.0004\n",
      "Epoch:240, P:None, Loss:178.1589, Loss-ae:177.7065, Loss-topo:0.0005\n",
      "Epoch:241, P:None, Loss:169.3597, Loss-ae:169.0272, Loss-topo:0.0003\n",
      "Epoch:242, P:None, Loss:170.9467, Loss-ae:170.5517, Loss-topo:0.0004\n",
      "Epoch:243, P:None, Loss:168.5246, Loss-ae:168.2494, Loss-topo:0.0003\n",
      "Epoch:244, P:None, Loss:171.7505, Loss-ae:171.3964, Loss-topo:0.0004\n",
      "Epoch:245, P:None, Loss:173.8368, Loss-ae:173.5399, Loss-topo:0.0003\n",
      "Epoch:246, P:None, Loss:172.2740, Loss-ae:171.9628, Loss-topo:0.0003\n",
      "Epoch:247, P:None, Loss:181.0609, Loss-ae:180.5888, Loss-topo:0.0005\n",
      "Epoch:248, P:None, Loss:173.4392, Loss-ae:173.1910, Loss-topo:0.0002\n",
      "Epoch:249, P:None, Loss:171.0464, Loss-ae:170.6867, Loss-topo:0.0004\n",
      "Epoch:250, P:None, Loss:172.1982, Loss-ae:171.8415, Loss-topo:0.0004\n",
      "Epoch:251, P:None, Loss:170.1772, Loss-ae:169.8066, Loss-topo:0.0004\n",
      "Epoch:252, P:None, Loss:171.8054, Loss-ae:171.4005, Loss-topo:0.0004\n",
      "Epoch:253, P:None, Loss:173.0854, Loss-ae:172.3942, Loss-topo:0.0007\n",
      "Epoch:254, P:None, Loss:163.8962, Loss-ae:163.4329, Loss-topo:0.0005\n",
      "Epoch:255, P:None, Loss:174.4405, Loss-ae:174.0408, Loss-topo:0.0004\n",
      "Epoch:256, P:None, Loss:168.5671, Loss-ae:168.3316, Loss-topo:0.0002\n",
      "Epoch:257, P:None, Loss:172.4544, Loss-ae:172.0216, Loss-topo:0.0004\n",
      "Epoch:258, P:None, Loss:175.9223, Loss-ae:175.4645, Loss-topo:0.0005\n",
      "Epoch:259, P:None, Loss:168.6336, Loss-ae:168.4286, Loss-topo:0.0002\n",
      "Epoch:260, P:None, Loss:171.2254, Loss-ae:170.9006, Loss-topo:0.0003\n",
      "Epoch:261, P:None, Loss:170.4335, Loss-ae:170.1019, Loss-topo:0.0003\n",
      "Epoch:262, P:None, Loss:167.1118, Loss-ae:166.7944, Loss-topo:0.0003\n",
      "Epoch:263, P:None, Loss:168.1547, Loss-ae:167.8859, Loss-topo:0.0003\n",
      "Epoch:264, P:None, Loss:166.0129, Loss-ae:165.7820, Loss-topo:0.0002\n",
      "Epoch:265, P:None, Loss:169.9451, Loss-ae:169.5858, Loss-topo:0.0004\n",
      "Epoch:266, P:None, Loss:167.4775, Loss-ae:167.0859, Loss-topo:0.0004\n",
      "Epoch:267, P:None, Loss:163.8411, Loss-ae:163.5496, Loss-topo:0.0003\n",
      "Epoch:268, P:None, Loss:165.3177, Loss-ae:164.9438, Loss-topo:0.0004\n",
      "Epoch:269, P:None, Loss:166.8863, Loss-ae:166.4714, Loss-topo:0.0004\n",
      "Epoch:270, P:None, Loss:170.5023, Loss-ae:170.2694, Loss-topo:0.0002\n",
      "Epoch:271, P:None, Loss:167.3310, Loss-ae:167.0266, Loss-topo:0.0003\n",
      "Epoch:272, P:None, Loss:171.1455, Loss-ae:170.6815, Loss-topo:0.0005\n",
      "Epoch:273, P:None, Loss:173.3231, Loss-ae:173.0211, Loss-topo:0.0003\n",
      "Epoch:274, P:None, Loss:175.4455, Loss-ae:174.9196, Loss-topo:0.0005\n",
      "Epoch:275, P:None, Loss:175.7731, Loss-ae:175.4105, Loss-topo:0.0004\n",
      "Epoch:276, P:None, Loss:162.2451, Loss-ae:161.9120, Loss-topo:0.0003\n",
      "Epoch:277, P:None, Loss:162.9243, Loss-ae:162.6043, Loss-topo:0.0003\n",
      "Epoch:278, P:None, Loss:163.2932, Loss-ae:163.0286, Loss-topo:0.0003\n",
      "Epoch:279, P:None, Loss:168.4033, Loss-ae:168.1086, Loss-topo:0.0003\n",
      "Epoch:280, P:None, Loss:163.8143, Loss-ae:163.4720, Loss-topo:0.0003\n",
      "Epoch:281, P:None, Loss:171.2534, Loss-ae:170.9978, Loss-topo:0.0003\n",
      "Epoch:282, P:None, Loss:161.5871, Loss-ae:161.2433, Loss-topo:0.0003\n",
      "Epoch:283, P:None, Loss:171.6335, Loss-ae:171.3702, Loss-topo:0.0003\n",
      "Epoch:284, P:None, Loss:161.0122, Loss-ae:160.5795, Loss-topo:0.0004\n",
      "Epoch:285, P:None, Loss:162.7169, Loss-ae:162.3303, Loss-topo:0.0004\n",
      "Epoch:286, P:None, Loss:173.8107, Loss-ae:173.4386, Loss-topo:0.0004\n",
      "Epoch:287, P:None, Loss:163.7328, Loss-ae:163.3305, Loss-topo:0.0004\n",
      "Epoch:288, P:None, Loss:158.1180, Loss-ae:157.7608, Loss-topo:0.0004\n",
      "Epoch:289, P:None, Loss:172.8803, Loss-ae:172.4895, Loss-topo:0.0004\n",
      "Epoch:290, P:None, Loss:168.8211, Loss-ae:168.4214, Loss-topo:0.0004\n",
      "Epoch:291, P:None, Loss:166.0648, Loss-ae:165.7202, Loss-topo:0.0003\n",
      "Epoch:292, P:None, Loss:166.8382, Loss-ae:166.4270, Loss-topo:0.0004\n",
      "Epoch:293, P:None, Loss:163.8040, Loss-ae:163.4797, Loss-topo:0.0003\n",
      "Epoch:294, P:None, Loss:168.4842, Loss-ae:168.2303, Loss-topo:0.0003\n",
      "Epoch:295, P:None, Loss:164.1772, Loss-ae:163.9076, Loss-topo:0.0003\n",
      "Epoch:296, P:None, Loss:160.2609, Loss-ae:159.9945, Loss-topo:0.0003\n",
      "Epoch:297, P:None, Loss:171.5272, Loss-ae:171.2475, Loss-topo:0.0003\n",
      "Epoch:298, P:None, Loss:164.6573, Loss-ae:164.3324, Loss-topo:0.0003\n",
      "Epoch:299, P:None, Loss:171.5220, Loss-ae:170.9006, Loss-topo:0.0006\n",
      "Epoch:300, P:None, Loss:162.6705, Loss-ae:162.4062, Loss-topo:0.0003\n",
      "Epoch:301, P:None, Loss:172.4938, Loss-ae:172.0307, Loss-topo:0.0005\n",
      "Epoch:302, P:None, Loss:166.1926, Loss-ae:165.8423, Loss-topo:0.0004\n",
      "Epoch:303, P:None, Loss:161.5202, Loss-ae:161.1636, Loss-topo:0.0004\n",
      "Epoch:304, P:None, Loss:166.5851, Loss-ae:166.3998, Loss-topo:0.0002\n",
      "Epoch:305, P:None, Loss:171.2249, Loss-ae:170.9696, Loss-topo:0.0003\n",
      "Epoch:306, P:None, Loss:160.0595, Loss-ae:159.7571, Loss-topo:0.0003\n",
      "Epoch:307, P:None, Loss:159.3454, Loss-ae:158.9185, Loss-topo:0.0004\n",
      "Epoch:308, P:None, Loss:161.6417, Loss-ae:161.4066, Loss-topo:0.0002\n",
      "Epoch:309, P:None, Loss:163.7407, Loss-ae:163.4761, Loss-topo:0.0003\n",
      "Epoch:310, P:None, Loss:162.5333, Loss-ae:162.2020, Loss-topo:0.0003\n",
      "Epoch:311, P:None, Loss:172.5057, Loss-ae:172.1995, Loss-topo:0.0003\n",
      "Epoch:312, P:None, Loss:166.4634, Loss-ae:166.1709, Loss-topo:0.0003\n",
      "Epoch:313, P:None, Loss:160.1608, Loss-ae:159.8907, Loss-topo:0.0003\n",
      "Epoch:314, P:None, Loss:169.3522, Loss-ae:168.7245, Loss-topo:0.0006\n",
      "Epoch:315, P:None, Loss:163.1713, Loss-ae:162.9237, Loss-topo:0.0002\n",
      "Epoch:316, P:None, Loss:161.6955, Loss-ae:161.4207, Loss-topo:0.0003\n",
      "Epoch:317, P:None, Loss:167.9444, Loss-ae:167.5715, Loss-topo:0.0004\n",
      "Epoch:318, P:None, Loss:166.0692, Loss-ae:165.6110, Loss-topo:0.0005\n",
      "Epoch:319, P:None, Loss:167.0072, Loss-ae:166.4265, Loss-topo:0.0006\n",
      "Epoch:320, P:None, Loss:166.0465, Loss-ae:165.7459, Loss-topo:0.0003\n",
      "Epoch:321, P:None, Loss:161.8633, Loss-ae:161.6347, Loss-topo:0.0002\n",
      "Epoch:322, P:None, Loss:163.5778, Loss-ae:163.2667, Loss-topo:0.0003\n",
      "Epoch:323, P:None, Loss:162.0129, Loss-ae:161.7575, Loss-topo:0.0003\n",
      "Epoch:324, P:None, Loss:158.3208, Loss-ae:158.1026, Loss-topo:0.0002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m plot_object \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRF-ACC\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRF-F1\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCo-k-NNs\u001b[39m\u001b[38;5;124m'\u001b[39m: []   \n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lambda_val \u001b[38;5;129;01min\u001b[39;00m lambdas_to_explore:\n\u001b[0;32m---> 14\u001b[0m     lambda_exploration \u001b[38;5;241m=\u001b[39m \u001b[43mexplore_lambda\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_HD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_Y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_HD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_Y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlambda_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimes_to_execute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutions_per_model\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# print(lambda_val, lambda_exploration)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj_property \u001b[38;5;129;01min\u001b[39;00m plot_object:\n",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36mexplore_lambda\u001b[0;34m(train_HD, train_Y, test_HD, test_Y, topoae_lambda, times_to_execute)\u001b[0m\n\u001b[1;32m     20\u001b[0m topo_reducer \u001b[38;5;241m=\u001b[39m CustomTopoDimRedTransform(\n\u001b[1;32m     21\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepAE_custom_dim2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m     model_lambda\u001b[38;5;241m=\u001b[39mtopoae_lambda,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     to_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m title_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMotionSense 20Hz\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTopoAE lambda \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(topoae_lambda)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtopo_reducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_HD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle_plot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle_plot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m train_LD \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(topo_reducer\u001b[38;5;241m.\u001b[39mtransform(train_HD), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     31\u001b[0m test_LD \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(topo_reducer\u001b[38;5;241m.\u001b[39mtransform(test_HD), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/Topological_ae/MotionSense20Hz/../../../librep/transforms/topo_ae.py:71\u001b[0m, in \u001b[0;36mTopologicalDimensionalityReduction.fit\u001b[0;34m(self, X, y, title_plot)\u001b[0m\n\u001b[1;32m     69\u001b[0m reshaped_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape)\n\u001b[1;32m     70\u001b[0m in_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(reshaped_data)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 71\u001b[0m loss, loss_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     73\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/Topological_ae/MotionSense20Hz/../../../librep/estimators/ae/torch/models/topological_ae/topological_ae.py:44\u001b[0m, in \u001b[0;36mTopologicallyRegularizedAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the loss of the Topologically regularized autoencoder.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m        x: Input data\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m        Tuple of final_loss, (...loss components...)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     x_distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_distance_matrix(x)\n\u001b[1;32m     48\u001b[0m     dimensions \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/Topological_ae/MotionSense20Hz/../../../librep/estimators/ae/torch/models/topological_ae/model_submodules.py:545\u001b[0m, in \u001b[0;36mDeepAE_custom_dim2.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute latent representation using convolutional autoencoder.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plot_object = {\n",
    "    'RF-ACC': [],\n",
    "    'RF-F1': [],\n",
    "    'SVC-ACC': [],\n",
    "    'SVC-F1': [],\n",
    "    'KNN-ACC': [],\n",
    "    'KNN-F1': [],\n",
    "    'Trustworthiness': [],\n",
    "    'Continuity': [],\n",
    "    'Co-k-NNs': []   \n",
    "}\n",
    "\n",
    "for lambda_val in lambdas_to_explore:\n",
    "    lambda_exploration = explore_lambda(\n",
    "        train_HD, train_Y,\n",
    "        test_HD, test_Y,\n",
    "        lambda_val,\n",
    "        times_to_execute=executions_per_model\n",
    "    )\n",
    "    # print(lambda_val, lambda_exploration)\n",
    "    for obj_property in plot_object:\n",
    "        plot_object[obj_property].append(lambda_exploration[obj_property])\n",
    "    \n",
    "# print(plot_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef0ac1-32bc-47a4-a1ca-84e6857a310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd7a5e-e570-4976-a119-60417b64053d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
