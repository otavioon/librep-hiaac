{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1630ed-63c8-4ee7-86c9-62dfb528b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da911e3-e82f-47c9-8090-f71f453956ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f067405-ab8c-4f22-b056-ab9e1febe71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5007e3-ef19-4658-ac7d-2a6f617bb9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 13:51:19.507334: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-18 13:51:19.507358: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from librep.datasets.har.loaders import (\n",
    "    MotionSense_BalancedView20HZ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9f66b2-7b1c-4d4e-a0e3-cde9552a595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librep.transforms.topo_ae import (\n",
    "    TopologicalDimensionalityReduction,\n",
    "    CustomTopoDimRedTransform\n",
    ")\n",
    "from librep.transforms import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "from experiments.Topological_ae.Experiment_utils import *\n",
    "from librep.datasets.multimodal import TransformMultiModalDataset\n",
    "from librep.transforms.fft import FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85bea424-07ff-47ec-945a-aef385afe7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Balanced MotionSense View Resampled to 20Hz with Gravity - Multiplied acc by 9.81m/sÂ²\n",
       "\n",
       "This is a view from [MotionSense] that was spllited into 3s windows and was resampled to 20Hz using the [FFT method](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.resample.html#scipy.signal.resample). \n",
       "\n",
       "The data was first splitted in three sets: train, validation and test. Each one with the following proportions:\n",
       "- Train: 70% of samples\n",
       "- Validation: 10% of samples\n",
       "- Test: 20% of samples\n",
       "\n",
       "After splits, the datasets were balanced in relation to the activity code column, that is, each subset have the same number of activitiy samples.\n",
       "\n",
       "**NOTE**: Each subset contain samples from distinct users, that is, samples of one user belongs exclusivelly to one of three subsets.\n",
       "\n",
       "## Activity codes\n",
       "- 0: downstairs (569 train, 101 validation, 170 test) \n",
       "- 1: upstairs (569 train, 101 validation, 170 test) \n",
       "- 2: sitting (569 train, 101 validation, 170 test) \n",
       "- 3: standing (569 train, 101 validation, 170 test) \n",
       "- 4: walking (569 train, 101 validation, 170 test) \n",
       "- 5: jogging (569 train, 101 validation, 170 test) \n",
       " \n",
       "\n",
       "## Standartized activity codes\n",
       "- 0: sit (569 train, 101 validation, 170 test) \n",
       "- 1: stand (569 train, 101 validation, 170 test) \n",
       "- 2: walk (569 train, 101 validation, 170 test) \n",
       "- 3: stair up (569 train, 101 validation, 170 test) \n",
       "- 4: stair down (569 train, 101 validation, 170 test) \n",
       "- 5: run (569 train, 101 validation, 170 test) \n",
       "      \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MotionSense Loader\n",
    "loader = MotionSense_BalancedView20HZ(\n",
    "    root_dir=\"../../../data/views/MotionSense/balanced_view_20Hz_with_gravity_9.81_acc_standard\", \n",
    "    download=False\n",
    ")\n",
    "\n",
    "# Print the readme (optional)\n",
    "loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d817b0c-4e6b-4227-9b48-cd23c3df190d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=4020, features=360, no. window=6, label_columns='standard activity code',\n",
       " PandasMultiModalDataset: samples=1020, features=360, no. window=6, label_columns='standard activity code')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# If concat_train_validation is true, return a tuple (train+validation, test)\n",
    "train_val, test = loader.load(concat_train_validation=True, label=loader.standard_label)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9491b1e0-0b60-439c-9bed-ea0ee7918b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_HD = np.array(train_val[:][0])\n",
    "train_Y = np.array(train_val[:][1])\n",
    "test_HD = np.array(test[:][0])\n",
    "test_Y = np.array(test[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc4810ac-c560-4b23-9b72-481a2d0f1a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4020, 360) (4020,) (1020, 360) (1020,)\n"
     ]
    }
   ],
   "source": [
    "print(train_HD.shape, train_Y.shape, test_HD.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93d061-6ea0-418b-b690-2d977f6297c9",
   "metadata": {},
   "source": [
    "# Aplicar FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5255de5-0259-404a-a2d8-507ad4f2ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_transform = FFT(centered = True)\n",
    "transformer = TransformMultiModalDataset(\n",
    "    transforms=[fft_transform],\n",
    "    new_window_name_prefix=\"fft.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29195f38-f8a9-4167-937f-d3f98ddc7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_fft = transformer(train_val)\n",
    "test_dataset_fft = transformer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8cfc25-0323-498a-931d-1249078f2796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4020, 180)\n",
      "(1020, 180)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_fft.X.shape)\n",
    "print(test_dataset_fft.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d413cc20-e5b5-4594-b1f7-f11153c00c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797.3368276806895\n"
     ]
    }
   ],
   "source": [
    "print(np.max(train_dataset_fft.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fea2e105-a571-4a2f-ac76-096b08a6ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_HD = train_dataset_fft.X\n",
    "train_LD = None\n",
    "train_Y = train_dataset_fft.y\n",
    "test_HD = test_dataset_fft.X\n",
    "test_LD = None\n",
    "test_Y = test_dataset_fft.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee13aef1-77c7-42a9-930b-b1a56f980d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4020, 180) (4020,) (1020, 180) (1020,)\n"
     ]
    }
   ],
   "source": [
    "print(train_HD.shape, train_Y.shape, test_HD.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adbc2e-4af6-40a4-85dc-99b2336d656c",
   "metadata": {},
   "source": [
    "# Visualization helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d56d7ec7-6bf5-4c32-91aa-d48a8746a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: sit (569 train, 101 validation, 170 test)\n",
    "# 1: stand (569 train, 101 validation, 170 test)\n",
    "# 2: walk (569 train, 101 validation, 170 test)\n",
    "# 3: stair up (569 train, 101 validation, 170 test)\n",
    "# 4: stair down (569 train, 101 validation, 170 test)\n",
    "# 5: run (569 train, 101 validation, 170 test)\n",
    "def visualize(X, Y):\n",
    "    labels = ['sit', 'stand', 'walk', 'stair up', 'stair down', 'run']\n",
    "    df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=Y))\n",
    "    groups = df.groupby('label')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.margins(0.05)\n",
    "    for name, group in groups:\n",
    "        ax.plot(group.x, group.y, marker='.', linestyle='', ms=8, label=labels[name])\n",
    "    # Shrink current axis by 20%\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "    # Put a legend to the right of the current axis\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80f4771a-da3f-407b-9fa9-b0619a129e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to reuse\n",
    "model_dim = 10\n",
    "model_epc = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8264b9-7756-4a8d-a8c0-18a1645b8e28",
   "metadata": {},
   "source": [
    "# Reducing with Generic Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d80ae63d-92f7-49c6-9527-212af7500573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topologically Regularized DeepAE_custom_dim2\n",
      "Using python to compute signatures\n",
      "DeepAE_custom_dim, Input: (1, 180) Inner dim: 10\n"
     ]
    }
   ],
   "source": [
    "model_lam = 0\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='DeepAE_custom_dim2',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d46f8ff8-e7a8-42e1-9fd2-d1e9a17b8cad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, P:None, Loss:501.6854, Loss-ae:501.6854, Loss-topo:727.3604\n",
      "Epoch:2, P:None, Loss:380.0453, Loss-ae:380.0453, Loss-topo:590.1054\n",
      "Epoch:3, P:None, Loss:328.0547, Loss-ae:328.0547, Loss-topo:582.1986\n",
      "Epoch:4, P:None, Loss:317.5555, Loss-ae:317.5555, Loss-topo:643.3878\n",
      "Epoch:5, P:None, Loss:313.0299, Loss-ae:313.0299, Loss-topo:993.6992\n",
      "Epoch:6, P:None, Loss:279.5524, Loss-ae:279.5524, Loss-topo:1663.7698\n",
      "Epoch:7, P:None, Loss:237.0799, Loss-ae:237.0799, Loss-topo:2254.0603\n",
      "Epoch:8, P:None, Loss:229.9885, Loss-ae:229.9885, Loss-topo:2198.6821\n",
      "Epoch:9, P:None, Loss:210.4836, Loss-ae:210.4836, Loss-topo:2772.3740\n",
      "Epoch:10, P:None, Loss:204.5083, Loss-ae:204.5083, Loss-topo:2437.5754\n",
      "Epoch:11, P:None, Loss:192.7030, Loss-ae:192.7030, Loss-topo:2477.9075\n",
      "Epoch:12, P:None, Loss:187.3277, Loss-ae:187.3277, Loss-topo:2576.0890\n",
      "Epoch:13, P:None, Loss:176.9867, Loss-ae:176.9867, Loss-topo:2917.8407\n",
      "Epoch:14, P:None, Loss:170.5403, Loss-ae:170.5403, Loss-topo:2752.3389\n",
      "Epoch:15, P:None, Loss:166.8215, Loss-ae:166.8215, Loss-topo:3015.1564\n",
      "Epoch:16, P:None, Loss:168.3432, Loss-ae:168.3432, Loss-topo:3308.8977\n",
      "Epoch:17, P:None, Loss:163.6534, Loss-ae:163.6534, Loss-topo:3128.9713\n",
      "Epoch:18, P:None, Loss:161.4490, Loss-ae:161.4490, Loss-topo:3042.3049\n",
      "Epoch:19, P:None, Loss:163.0954, Loss-ae:163.0954, Loss-topo:3130.5717\n",
      "Epoch:20, P:None, Loss:155.7972, Loss-ae:155.7972, Loss-topo:3808.1625\n",
      "Epoch:21, P:None, Loss:145.7685, Loss-ae:145.7685, Loss-topo:3223.3093\n",
      "Epoch:22, P:None, Loss:147.4147, Loss-ae:147.4147, Loss-topo:3845.3839\n",
      "Epoch:23, P:None, Loss:152.6092, Loss-ae:152.6092, Loss-topo:4454.1824\n",
      "Epoch:24, P:None, Loss:151.4321, Loss-ae:151.4321, Loss-topo:4073.2927\n",
      "Epoch:25, P:None, Loss:146.9589, Loss-ae:146.9589, Loss-topo:3591.2310\n",
      "Epoch:26, P:None, Loss:140.0854, Loss-ae:140.0854, Loss-topo:4196.3070\n",
      "Epoch:27, P:None, Loss:145.8852, Loss-ae:145.8852, Loss-topo:4842.7122\n",
      "Epoch:28, P:None, Loss:142.9547, Loss-ae:142.9547, Loss-topo:4789.3890\n",
      "Epoch:29, P:None, Loss:139.4082, Loss-ae:139.4082, Loss-topo:4469.1970\n",
      "Epoch:30, P:None, Loss:140.3115, Loss-ae:140.3115, Loss-topo:4845.2636\n",
      "Epoch:31, P:None, Loss:136.6702, Loss-ae:136.6702, Loss-topo:4700.3523\n",
      "Epoch:32, P:None, Loss:133.2038, Loss-ae:133.2038, Loss-topo:5149.2943\n",
      "Epoch:33, P:None, Loss:135.9263, Loss-ae:135.9263, Loss-topo:5192.2480\n",
      "Epoch:34, P:None, Loss:137.8897, Loss-ae:137.8897, Loss-topo:4873.3678\n",
      "Epoch:35, P:None, Loss:135.8812, Loss-ae:135.8812, Loss-topo:5548.9088\n",
      "Epoch:36, P:None, Loss:131.8300, Loss-ae:131.8300, Loss-topo:6075.0575\n",
      "Epoch:37, P:None, Loss:128.4211, Loss-ae:128.4211, Loss-topo:5502.3961\n",
      "Epoch:38, P:None, Loss:130.3662, Loss-ae:130.3662, Loss-topo:5800.2227\n",
      "Epoch:39, P:None, Loss:129.0638, Loss-ae:129.0638, Loss-topo:5832.2215\n",
      "Epoch:40, P:None, Loss:134.1096, Loss-ae:134.1096, Loss-topo:6579.5938\n",
      "Epoch:41, P:None, Loss:128.5869, Loss-ae:128.5869, Loss-topo:6003.2176\n",
      "Epoch:42, P:None, Loss:126.1691, Loss-ae:126.1691, Loss-topo:6004.3212\n",
      "Epoch:43, P:None, Loss:123.7524, Loss-ae:123.7524, Loss-topo:6404.6895\n",
      "Epoch:44, P:None, Loss:127.7431, Loss-ae:127.7431, Loss-topo:6917.5708\n",
      "Epoch:45, P:None, Loss:129.9082, Loss-ae:129.9082, Loss-topo:6920.3914\n",
      "Epoch:46, P:None, Loss:125.4220, Loss-ae:125.4220, Loss-topo:7185.7723\n",
      "Epoch:47, P:None, Loss:123.3970, Loss-ae:123.3970, Loss-topo:7090.3973\n",
      "Epoch:48, P:None, Loss:121.8534, Loss-ae:121.8534, Loss-topo:7362.0426\n",
      "Epoch:49, P:None, Loss:121.1187, Loss-ae:121.1187, Loss-topo:7744.3017\n",
      "Epoch:50, P:None, Loss:120.3020, Loss-ae:120.3020, Loss-topo:7569.5256\n",
      "Epoch:51, P:None, Loss:125.9123, Loss-ae:125.9123, Loss-topo:8758.0190\n",
      "Epoch:52, P:None, Loss:123.0234, Loss-ae:123.0234, Loss-topo:7880.1832\n",
      "Epoch:53, P:None, Loss:128.8065, Loss-ae:128.8065, Loss-topo:7928.1685\n",
      "Epoch:54, P:None, Loss:120.4598, Loss-ae:120.4598, Loss-topo:7901.8756\n",
      "Epoch:55, P:None, Loss:120.8336, Loss-ae:120.8336, Loss-topo:7522.1689\n",
      "Epoch:56, P:None, Loss:119.6828, Loss-ae:119.6828, Loss-topo:7743.6046\n",
      "Epoch:57, P:None, Loss:126.6955, Loss-ae:126.6955, Loss-topo:8162.8759\n",
      "Epoch:58, P:None, Loss:122.4801, Loss-ae:122.4801, Loss-topo:7588.4112\n",
      "Epoch:59, P:None, Loss:120.8562, Loss-ae:120.8562, Loss-topo:8236.2899\n",
      "Epoch:60, P:None, Loss:116.9276, Loss-ae:116.9276, Loss-topo:7943.0871\n",
      "Epoch:61, P:None, Loss:118.7042, Loss-ae:118.7042, Loss-topo:8005.3703\n",
      "Epoch:62, P:None, Loss:121.5667, Loss-ae:121.5667, Loss-topo:8472.1277\n",
      "Epoch:63, P:None, Loss:123.3690, Loss-ae:123.3690, Loss-topo:8628.4637\n",
      "Epoch:64, P:None, Loss:123.0626, Loss-ae:123.0626, Loss-topo:9109.3522\n",
      "Epoch:65, P:None, Loss:115.8812, Loss-ae:115.8812, Loss-topo:8358.8369\n",
      "Epoch:66, P:None, Loss:117.8596, Loss-ae:117.8596, Loss-topo:9370.8311\n",
      "Epoch:67, P:None, Loss:117.6102, Loss-ae:117.6102, Loss-topo:9164.4212\n",
      "Epoch:68, P:None, Loss:114.6063, Loss-ae:114.6063, Loss-topo:8138.7139\n",
      "Epoch:69, P:None, Loss:117.8861, Loss-ae:117.8861, Loss-topo:8779.1193\n",
      "Epoch:70, P:None, Loss:114.8047, Loss-ae:114.8047, Loss-topo:8532.5255\n",
      "Epoch:71, P:None, Loss:118.5980, Loss-ae:118.5980, Loss-topo:9130.9788\n",
      "Epoch:72, P:None, Loss:121.2419, Loss-ae:121.2419, Loss-topo:8866.6609\n",
      "Epoch:73, P:None, Loss:117.5493, Loss-ae:117.5493, Loss-topo:9576.3643\n",
      "Epoch:74, P:None, Loss:117.9861, Loss-ae:117.9861, Loss-topo:9150.3164\n",
      "Epoch:75, P:None, Loss:115.5845, Loss-ae:115.5845, Loss-topo:9449.1982\n",
      "Epoch:76, P:None, Loss:112.3089, Loss-ae:112.3089, Loss-topo:8772.0323\n",
      "Epoch:77, P:None, Loss:113.0777, Loss-ae:113.0777, Loss-topo:9327.3093\n",
      "Epoch:78, P:None, Loss:113.3036, Loss-ae:113.3036, Loss-topo:10279.0169\n",
      "Epoch:79, P:None, Loss:114.4445, Loss-ae:114.4445, Loss-topo:9512.9396\n",
      "Epoch:80, P:None, Loss:113.6294, Loss-ae:113.6294, Loss-topo:9913.2663\n",
      "Epoch:81, P:None, Loss:115.2391, Loss-ae:115.2391, Loss-topo:9507.3937\n",
      "Epoch:82, P:None, Loss:113.2544, Loss-ae:113.2544, Loss-topo:9685.8015\n",
      "Epoch:83, P:None, Loss:109.7838, Loss-ae:109.7838, Loss-topo:9219.6879\n",
      "Epoch:84, P:None, Loss:110.9985, Loss-ae:110.9985, Loss-topo:9751.8994\n",
      "Epoch:85, P:None, Loss:119.9369, Loss-ae:119.9369, Loss-topo:10294.4607\n",
      "Epoch:86, P:None, Loss:121.5204, Loss-ae:121.5204, Loss-topo:9345.6416\n",
      "Epoch:87, P:None, Loss:115.0712, Loss-ae:115.0712, Loss-topo:10141.4552\n",
      "Epoch:88, P:None, Loss:111.8879, Loss-ae:111.8879, Loss-topo:10256.7595\n",
      "Epoch:89, P:None, Loss:108.1936, Loss-ae:108.1936, Loss-topo:9459.5938\n",
      "Epoch:90, P:None, Loss:112.4270, Loss-ae:112.4270, Loss-topo:10028.4503\n",
      "Epoch:91, P:None, Loss:115.3239, Loss-ae:115.3239, Loss-topo:11396.2602\n",
      "Epoch:92, P:None, Loss:119.8406, Loss-ae:119.8406, Loss-topo:10727.9890\n",
      "Epoch:93, P:None, Loss:119.9368, Loss-ae:119.9368, Loss-topo:10909.7141\n",
      "Epoch:94, P:None, Loss:108.6687, Loss-ae:108.6687, Loss-topo:9442.0330\n",
      "Epoch:95, P:None, Loss:113.7808, Loss-ae:113.7808, Loss-topo:11246.0525\n",
      "Epoch:96, P:None, Loss:109.1402, Loss-ae:109.1402, Loss-topo:10080.2864\n",
      "Epoch:97, P:None, Loss:111.2393, Loss-ae:111.2393, Loss-topo:10758.1381\n",
      "Epoch:98, P:None, Loss:110.9781, Loss-ae:110.9781, Loss-topo:10627.1038\n",
      "Epoch:99, P:None, Loss:114.4780, Loss-ae:114.4780, Loss-topo:11748.4976\n",
      "Epoch:100, P:None, Loss:112.6485, Loss-ae:112.6485, Loss-topo:11205.3612\n",
      "Epoch:101, P:None, Loss:111.7893, Loss-ae:111.7893, Loss-topo:11315.6879\n",
      "Epoch:102, P:None, Loss:110.5334, Loss-ae:110.5334, Loss-topo:11183.4801\n",
      "Epoch:103, P:None, Loss:110.9946, Loss-ae:110.9946, Loss-topo:11627.1276\n",
      "Epoch:104, P:None, Loss:113.7608, Loss-ae:113.7608, Loss-topo:11058.3973\n",
      "Epoch:105, P:None, Loss:108.7781, Loss-ae:108.7781, Loss-topo:11331.7457\n",
      "Epoch:106, P:None, Loss:114.0509, Loss-ae:114.0509, Loss-topo:11287.9764\n",
      "Epoch:107, P:None, Loss:109.8103, Loss-ae:109.8103, Loss-topo:11046.8973\n",
      "Epoch:108, P:None, Loss:112.7564, Loss-ae:112.7564, Loss-topo:11618.7741\n",
      "Epoch:109, P:None, Loss:110.7391, Loss-ae:110.7391, Loss-topo:12059.0557\n",
      "Epoch:110, P:None, Loss:116.1432, Loss-ae:116.1432, Loss-topo:12170.3701\n",
      "Epoch:111, P:None, Loss:112.8645, Loss-ae:112.8645, Loss-topo:12755.6369\n",
      "Epoch:112, P:None, Loss:108.5417, Loss-ae:108.5417, Loss-topo:12069.5095\n",
      "Epoch:113, P:None, Loss:109.7613, Loss-ae:109.7613, Loss-topo:12244.5409\n",
      "Epoch:114, P:None, Loss:109.1153, Loss-ae:109.1153, Loss-topo:12091.1807\n",
      "Epoch:115, P:None, Loss:112.1994, Loss-ae:112.1994, Loss-topo:11857.3577\n",
      "Epoch:116, P:None, Loss:107.8013, Loss-ae:107.8013, Loss-topo:12222.0483\n",
      "Epoch:117, P:None, Loss:111.3443, Loss-ae:111.3443, Loss-topo:13109.5716\n",
      "Epoch:118, P:None, Loss:109.4302, Loss-ae:109.4302, Loss-topo:11697.2527\n",
      "Epoch:119, P:None, Loss:112.3624, Loss-ae:112.3624, Loss-topo:11704.2415\n",
      "Epoch:120, P:None, Loss:109.3141, Loss-ae:109.3141, Loss-topo:13066.2959\n",
      "Epoch:121, P:None, Loss:108.4230, Loss-ae:108.4230, Loss-topo:11976.0855\n",
      "Epoch:122, P:None, Loss:111.6163, Loss-ae:111.6163, Loss-topo:12853.7507\n",
      "Epoch:123, P:None, Loss:107.2099, Loss-ae:107.2099, Loss-topo:11838.9576\n",
      "Epoch:124, P:None, Loss:110.9718, Loss-ae:110.9718, Loss-topo:12845.4392\n",
      "Epoch:125, P:None, Loss:106.8624, Loss-ae:106.8624, Loss-topo:12266.7913\n",
      "Epoch:126, P:None, Loss:127.6352, Loss-ae:127.6352, Loss-topo:12860.7314\n",
      "Epoch:127, P:None, Loss:108.8336, Loss-ae:108.8336, Loss-topo:13350.0770\n",
      "Epoch:128, P:None, Loss:108.3606, Loss-ae:108.3606, Loss-topo:12899.3290\n",
      "Epoch:129, P:None, Loss:107.5286, Loss-ae:107.5286, Loss-topo:12854.8718\n",
      "Epoch:130, P:None, Loss:108.2097, Loss-ae:108.2097, Loss-topo:13073.6865\n",
      "Epoch:131, P:None, Loss:103.3225, Loss-ae:103.3225, Loss-topo:13126.6343\n",
      "Epoch:132, P:None, Loss:107.5269, Loss-ae:107.5269, Loss-topo:12807.0929\n",
      "Epoch:133, P:None, Loss:107.8163, Loss-ae:107.8163, Loss-topo:13212.7963\n",
      "Epoch:134, P:None, Loss:107.5441, Loss-ae:107.5441, Loss-topo:13840.9118\n",
      "Epoch:135, P:None, Loss:103.5396, Loss-ae:103.5396, Loss-topo:12253.6000\n",
      "Epoch:136, P:None, Loss:107.3260, Loss-ae:107.3260, Loss-topo:14500.4477\n",
      "Epoch:137, P:None, Loss:115.2018, Loss-ae:115.2018, Loss-topo:13540.0691\n",
      "Epoch:138, P:None, Loss:106.3350, Loss-ae:106.3350, Loss-topo:13305.4584\n",
      "Epoch:139, P:None, Loss:104.6903, Loss-ae:104.6903, Loss-topo:13796.2098\n",
      "Epoch:140, P:None, Loss:103.6135, Loss-ae:103.6135, Loss-topo:14413.7709\n",
      "Epoch:141, P:None, Loss:104.9748, Loss-ae:104.9748, Loss-topo:14479.7605\n",
      "Epoch:142, P:None, Loss:109.0177, Loss-ae:109.0177, Loss-topo:14924.7845\n",
      "Epoch:143, P:None, Loss:102.9515, Loss-ae:102.9515, Loss-topo:14745.5882\n",
      "Epoch:144, P:None, Loss:105.1750, Loss-ae:105.1750, Loss-topo:15011.0359\n",
      "Epoch:145, P:None, Loss:106.9166, Loss-ae:106.9166, Loss-topo:14836.3878\n",
      "Epoch:146, P:None, Loss:104.3415, Loss-ae:104.3415, Loss-topo:14201.8708\n",
      "Epoch:147, P:None, Loss:108.0188, Loss-ae:108.0188, Loss-topo:15264.0287\n",
      "Epoch:148, P:None, Loss:104.3096, Loss-ae:104.3096, Loss-topo:14929.9452\n",
      "Epoch:149, P:None, Loss:111.1480, Loss-ae:111.1480, Loss-topo:13926.1445\n",
      "Epoch:150, P:None, Loss:111.3505, Loss-ae:111.3505, Loss-topo:15681.9466\n",
      "Epoch:151, P:None, Loss:105.4564, Loss-ae:105.4564, Loss-topo:14493.4309\n",
      "Epoch:152, P:None, Loss:111.3184, Loss-ae:111.3184, Loss-topo:14397.6237\n",
      "Epoch:153, P:None, Loss:104.2152, Loss-ae:104.2152, Loss-topo:13384.9332\n",
      "Epoch:154, P:None, Loss:105.1237, Loss-ae:105.1237, Loss-topo:14850.0333\n",
      "Epoch:155, P:None, Loss:108.8744, Loss-ae:108.8744, Loss-topo:15025.2722\n",
      "Epoch:156, P:None, Loss:99.9571, Loss-ae:99.9571, Loss-topo:14135.7938\n",
      "Epoch:157, P:None, Loss:102.1812, Loss-ae:102.1812, Loss-topo:15258.8523\n",
      "Epoch:158, P:None, Loss:106.2518, Loss-ae:106.2518, Loss-topo:15848.9824\n",
      "Epoch:159, P:None, Loss:102.7093, Loss-ae:102.7093, Loss-topo:14638.6332\n",
      "Epoch:160, P:None, Loss:104.7351, Loss-ae:104.7351, Loss-topo:16315.9710\n",
      "Epoch:161, P:None, Loss:105.0208, Loss-ae:105.0208, Loss-topo:16534.9867\n",
      "Epoch:162, P:None, Loss:101.3130, Loss-ae:101.3130, Loss-topo:14794.8457\n",
      "Epoch:163, P:None, Loss:103.8397, Loss-ae:103.8397, Loss-topo:15041.2831\n",
      "Epoch:164, P:None, Loss:104.3951, Loss-ae:104.3951, Loss-topo:15649.6844\n",
      "Epoch:165, P:None, Loss:108.9164, Loss-ae:108.9164, Loss-topo:16185.5950\n",
      "Epoch:166, P:None, Loss:104.1968, Loss-ae:104.1968, Loss-topo:15016.5949\n",
      "Epoch:167, P:None, Loss:103.4761, Loss-ae:103.4761, Loss-topo:15336.1339\n",
      "Epoch:168, P:None, Loss:103.1503, Loss-ae:103.1503, Loss-topo:15626.4856\n",
      "Epoch:169, P:None, Loss:101.5204, Loss-ae:101.5204, Loss-topo:14506.4171\n",
      "Epoch:170, P:None, Loss:107.2500, Loss-ae:107.2500, Loss-topo:15494.1671\n",
      "Epoch:171, P:None, Loss:103.6994, Loss-ae:103.6994, Loss-topo:14867.7199\n",
      "Epoch:172, P:None, Loss:103.5929, Loss-ae:103.5929, Loss-topo:16908.2147\n",
      "Epoch:173, P:None, Loss:101.2044, Loss-ae:101.2044, Loss-topo:15645.4941\n",
      "Epoch:174, P:None, Loss:103.2313, Loss-ae:103.2313, Loss-topo:15634.6274\n",
      "Epoch:175, P:None, Loss:105.3462, Loss-ae:105.3462, Loss-topo:17012.6556\n",
      "Epoch:176, P:None, Loss:105.0691, Loss-ae:105.0691, Loss-topo:17152.9290\n",
      "Epoch:177, P:None, Loss:104.6573, Loss-ae:104.6573, Loss-topo:16186.1473\n",
      "Epoch:178, P:None, Loss:103.3116, Loss-ae:103.3116, Loss-topo:16620.1762\n",
      "Epoch:179, P:None, Loss:103.0669, Loss-ae:103.0669, Loss-topo:17341.2628\n",
      "Epoch:180, P:None, Loss:103.0780, Loss-ae:103.0780, Loss-topo:16208.1514\n",
      "Epoch:181, P:None, Loss:104.4366, Loss-ae:104.4366, Loss-topo:18092.4192\n",
      "Epoch:182, P:None, Loss:106.2792, Loss-ae:106.2792, Loss-topo:16823.0946\n",
      "Epoch:183, P:None, Loss:108.2563, Loss-ae:108.2563, Loss-topo:17103.5963\n",
      "Epoch:184, P:None, Loss:110.6922, Loss-ae:110.6922, Loss-topo:16975.2919\n",
      "Epoch:185, P:None, Loss:102.9775, Loss-ae:102.9775, Loss-topo:16249.5020\n",
      "Epoch:186, P:None, Loss:100.5269, Loss-ae:100.5269, Loss-topo:16734.5262\n",
      "Epoch:187, P:None, Loss:104.3572, Loss-ae:104.3572, Loss-topo:18226.7969\n",
      "Epoch:188, P:None, Loss:103.3767, Loss-ae:103.3767, Loss-topo:17046.6768\n",
      "Epoch:189, P:None, Loss:103.7749, Loss-ae:103.7749, Loss-topo:17762.5777\n",
      "Epoch:190, P:None, Loss:103.5391, Loss-ae:103.5391, Loss-topo:16434.5508\n",
      "Epoch:191, P:None, Loss:105.7337, Loss-ae:105.7337, Loss-topo:17418.2874\n",
      "Epoch:192, P:None, Loss:103.2682, Loss-ae:103.2682, Loss-topo:17384.2157\n",
      "Epoch:193, P:None, Loss:108.3535, Loss-ae:108.3535, Loss-topo:17298.6941\n",
      "Epoch:194, P:None, Loss:101.4013, Loss-ae:101.4013, Loss-topo:17494.2959\n",
      "Epoch:195, P:None, Loss:103.8501, Loss-ae:103.8501, Loss-topo:17488.1892\n",
      "Epoch:196, P:None, Loss:104.3773, Loss-ae:104.3773, Loss-topo:17186.9358\n",
      "Epoch:197, P:None, Loss:104.3212, Loss-ae:104.3212, Loss-topo:17446.8313\n",
      "Epoch:198, P:None, Loss:101.8885, Loss-ae:101.8885, Loss-topo:16962.9958\n",
      "Epoch:199, P:None, Loss:110.6900, Loss-ae:110.6900, Loss-topo:18312.2462\n",
      "Epoch:200, P:None, Loss:105.8902, Loss-ae:105.8902, Loss-topo:16814.3322\n",
      "Epoch:201, P:None, Loss:103.1566, Loss-ae:103.1566, Loss-topo:18431.3315\n",
      "Epoch:202, P:None, Loss:105.6026, Loss-ae:105.6026, Loss-topo:18419.5151\n",
      "Epoch:203, P:None, Loss:109.5948, Loss-ae:109.5948, Loss-topo:18363.2917\n",
      "Epoch:204, P:None, Loss:104.3162, Loss-ae:104.3162, Loss-topo:17933.2633\n",
      "Epoch:205, P:None, Loss:105.6900, Loss-ae:105.6900, Loss-topo:17335.9388\n",
      "Epoch:206, P:None, Loss:103.0319, Loss-ae:103.0319, Loss-topo:18297.7033\n",
      "Epoch:207, P:None, Loss:107.5500, Loss-ae:107.5500, Loss-topo:18780.1067\n",
      "Epoch:208, P:None, Loss:106.1069, Loss-ae:106.1069, Loss-topo:18802.5788\n",
      "Epoch:209, P:None, Loss:106.4834, Loss-ae:106.4834, Loss-topo:19056.9508\n",
      "Epoch:210, P:None, Loss:101.7398, Loss-ae:101.7398, Loss-topo:17552.8387\n",
      "Epoch:211, P:None, Loss:105.8832, Loss-ae:105.8832, Loss-topo:18376.2109\n",
      "Epoch:212, P:None, Loss:108.1136, Loss-ae:108.1136, Loss-topo:18008.3983\n",
      "Epoch:213, P:None, Loss:107.0287, Loss-ae:107.0287, Loss-topo:19128.1699\n",
      "Epoch:214, P:None, Loss:104.4396, Loss-ae:104.4396, Loss-topo:18976.1938\n",
      "Epoch:215, P:None, Loss:103.6142, Loss-ae:103.6142, Loss-topo:17968.6403\n",
      "Epoch:216, P:None, Loss:105.6223, Loss-ae:105.6223, Loss-topo:18680.0081\n",
      "Epoch:217, P:None, Loss:104.2868, Loss-ae:104.2868, Loss-topo:18903.6318\n",
      "Epoch:218, P:None, Loss:105.5399, Loss-ae:105.5399, Loss-topo:20109.9166\n",
      "Epoch:219, P:None, Loss:105.0810, Loss-ae:105.0810, Loss-topo:20548.1507\n",
      "Epoch:220, P:None, Loss:106.2219, Loss-ae:106.2219, Loss-topo:19007.8030\n",
      "Epoch:221, P:None, Loss:105.9631, Loss-ae:105.9631, Loss-topo:19138.5092\n",
      "Epoch:222, P:None, Loss:107.9253, Loss-ae:107.9253, Loss-topo:19550.9858\n",
      "Epoch:223, P:None, Loss:104.9052, Loss-ae:104.9052, Loss-topo:18989.6918\n",
      "Epoch:224, P:None, Loss:105.1100, Loss-ae:105.1100, Loss-topo:19392.2414\n",
      "Epoch:225, P:None, Loss:99.5929, Loss-ae:99.5929, Loss-topo:18583.1609\n",
      "Epoch:226, P:None, Loss:108.5217, Loss-ae:108.5217, Loss-topo:22300.3061\n",
      "Epoch:227, P:None, Loss:109.6217, Loss-ae:109.6217, Loss-topo:22878.8797\n",
      "Epoch:228, P:None, Loss:110.0680, Loss-ae:110.0680, Loss-topo:19671.4122\n",
      "Epoch:229, P:None, Loss:109.6539, Loss-ae:109.6539, Loss-topo:20882.3684\n",
      "Epoch:230, P:None, Loss:108.7091, Loss-ae:108.7091, Loss-topo:18517.0541\n",
      "Epoch:231, P:None, Loss:108.8082, Loss-ae:108.8082, Loss-topo:17586.6812\n",
      "Epoch:232, P:None, Loss:111.1696, Loss-ae:111.1696, Loss-topo:19664.0483\n",
      "Epoch:233, P:None, Loss:105.2681, Loss-ae:105.2681, Loss-topo:18982.3549\n",
      "Epoch:234, P:None, Loss:109.6481, Loss-ae:109.6481, Loss-topo:19312.7655\n",
      "Epoch:235, P:None, Loss:103.4858, Loss-ae:103.4858, Loss-topo:19354.9422\n",
      "Epoch:236, P:None, Loss:103.3216, Loss-ae:103.3216, Loss-topo:19495.9428\n",
      "Epoch:237, P:None, Loss:107.2378, Loss-ae:107.2378, Loss-topo:20338.0141\n",
      "Epoch:238, P:None, Loss:104.8109, Loss-ae:104.8109, Loss-topo:19541.6576\n",
      "Epoch:239, P:None, Loss:100.6629, Loss-ae:100.6629, Loss-topo:18954.7808\n",
      "Epoch:240, P:None, Loss:107.8621, Loss-ae:107.8621, Loss-topo:21492.7128\n",
      "Epoch:241, P:None, Loss:105.0121, Loss-ae:105.0121, Loss-topo:20157.9467\n",
      "Epoch:242, P:None, Loss:106.9817, Loss-ae:106.9817, Loss-topo:22778.6479\n",
      "Epoch:243, P:None, Loss:104.3242, Loss-ae:104.3242, Loss-topo:19862.2720\n",
      "Epoch:244, P:None, Loss:107.4776, Loss-ae:107.4776, Loss-topo:21454.7093\n",
      "Epoch:245, P:None, Loss:102.9518, Loss-ae:102.9518, Loss-topo:20455.2497\n",
      "Epoch:246, P:None, Loss:107.7206, Loss-ae:107.7206, Loss-topo:21923.1044\n",
      "Epoch:247, P:None, Loss:107.7630, Loss-ae:107.7630, Loss-topo:20720.5622\n",
      "Epoch:248, P:None, Loss:103.6638, Loss-ae:103.6638, Loss-topo:19557.9503\n",
      "Epoch:249, P:None, Loss:102.6422, Loss-ae:102.6422, Loss-topo:20586.4322\n",
      "Epoch:250, P:None, Loss:113.1848, Loss-ae:113.1848, Loss-topo:23475.6490\n",
      "Epoch:251, P:None, Loss:132.9377, Loss-ae:132.9377, Loss-topo:17493.8037\n",
      "Epoch:252, P:None, Loss:128.8766, Loss-ae:128.8766, Loss-topo:18799.3167\n",
      "Epoch:253, P:None, Loss:108.1782, Loss-ae:108.1782, Loss-topo:17903.5710\n",
      "Epoch:254, P:None, Loss:102.0738, Loss-ae:102.0738, Loss-topo:17913.2087\n",
      "Epoch:255, P:None, Loss:101.7518, Loss-ae:101.7518, Loss-topo:17633.7423\n",
      "Epoch:256, P:None, Loss:100.7314, Loss-ae:100.7314, Loss-topo:19147.6201\n",
      "Epoch:257, P:None, Loss:102.6206, Loss-ae:102.6206, Loss-topo:18659.8742\n",
      "Epoch:258, P:None, Loss:103.8768, Loss-ae:103.8768, Loss-topo:19177.3239\n",
      "Epoch:259, P:None, Loss:100.3404, Loss-ae:100.3404, Loss-topo:19082.1410\n",
      "Epoch:260, P:None, Loss:99.8258, Loss-ae:99.8258, Loss-topo:18895.1205\n",
      "Epoch:261, P:None, Loss:98.4609, Loss-ae:98.4609, Loss-topo:18777.3920\n",
      "Epoch:262, P:None, Loss:102.9950, Loss-ae:102.9950, Loss-topo:20891.0765\n",
      "Epoch:263, P:None, Loss:103.0317, Loss-ae:103.0317, Loss-topo:21046.6007\n",
      "Epoch:264, P:None, Loss:102.3245, Loss-ae:102.3245, Loss-topo:20801.2808\n",
      "Epoch:265, P:None, Loss:101.6253, Loss-ae:101.6253, Loss-topo:19445.1055\n",
      "Epoch:266, P:None, Loss:102.3087, Loss-ae:102.3087, Loss-topo:20624.6878\n",
      "Epoch:267, P:None, Loss:100.7863, Loss-ae:100.7863, Loss-topo:20523.7419\n",
      "Epoch:268, P:None, Loss:102.5276, Loss-ae:102.5276, Loss-topo:20235.6077\n",
      "Epoch:269, P:None, Loss:104.6105, Loss-ae:104.6105, Loss-topo:21567.2857\n",
      "Epoch:270, P:None, Loss:101.7517, Loss-ae:101.7517, Loss-topo:21272.7312\n",
      "Epoch:271, P:None, Loss:100.1524, Loss-ae:100.1524, Loss-topo:20897.1588\n",
      "Epoch:272, P:None, Loss:101.8591, Loss-ae:101.8591, Loss-topo:21948.7665\n",
      "Epoch:273, P:None, Loss:105.8717, Loss-ae:105.8717, Loss-topo:22735.4782\n",
      "Epoch:274, P:None, Loss:105.9287, Loss-ae:105.9287, Loss-topo:22258.4333\n",
      "Epoch:275, P:None, Loss:100.1335, Loss-ae:100.1335, Loss-topo:22073.0725\n",
      "Epoch:276, P:None, Loss:101.7113, Loss-ae:101.7113, Loss-topo:21882.7302\n",
      "Epoch:277, P:None, Loss:102.4166, Loss-ae:102.4166, Loss-topo:21791.5315\n",
      "Epoch:278, P:None, Loss:99.3186, Loss-ae:99.3186, Loss-topo:21122.2441\n",
      "Epoch:279, P:None, Loss:106.0868, Loss-ae:106.0868, Loss-topo:23792.9763\n",
      "Epoch:280, P:None, Loss:102.3444, Loss-ae:102.3444, Loss-topo:21190.9196\n",
      "Epoch:281, P:None, Loss:104.6456, Loss-ae:104.6456, Loss-topo:23027.6105\n",
      "Epoch:282, P:None, Loss:103.6763, Loss-ae:103.6763, Loss-topo:22245.8831\n",
      "Epoch:283, P:None, Loss:102.2454, Loss-ae:102.2454, Loss-topo:21841.0416\n",
      "Epoch:284, P:None, Loss:103.6498, Loss-ae:103.6498, Loss-topo:24205.2804\n",
      "Epoch:285, P:None, Loss:106.3144, Loss-ae:106.3144, Loss-topo:22765.8675\n",
      "Epoch:286, P:None, Loss:104.3340, Loss-ae:104.3340, Loss-topo:23427.9701\n",
      "Epoch:287, P:None, Loss:103.2498, Loss-ae:103.2498, Loss-topo:22010.5781\n",
      "Epoch:288, P:None, Loss:105.2196, Loss-ae:105.2196, Loss-topo:23482.9076\n",
      "Epoch:289, P:None, Loss:102.1428, Loss-ae:102.1428, Loss-topo:22491.3697\n",
      "Epoch:290, P:None, Loss:102.5980, Loss-ae:102.5980, Loss-topo:21523.9074\n",
      "Epoch:291, P:None, Loss:103.8745, Loss-ae:103.8745, Loss-topo:21507.5377\n",
      "Epoch:292, P:None, Loss:99.7328, Loss-ae:99.7328, Loss-topo:21504.6345\n",
      "Epoch:293, P:None, Loss:103.8377, Loss-ae:103.8377, Loss-topo:22634.4849\n",
      "Epoch:294, P:None, Loss:102.9487, Loss-ae:102.9487, Loss-topo:24042.5120\n",
      "Epoch:295, P:None, Loss:100.9306, Loss-ae:100.9306, Loss-topo:22643.8856\n",
      "Epoch:296, P:None, Loss:106.9851, Loss-ae:106.9851, Loss-topo:22054.5151\n",
      "Epoch:297, P:None, Loss:102.4613, Loss-ae:102.4613, Loss-topo:21928.5413\n",
      "Epoch:298, P:None, Loss:106.2252, Loss-ae:106.2252, Loss-topo:22962.7684\n",
      "Epoch:299, P:None, Loss:109.8542, Loss-ae:109.8542, Loss-topo:23088.4690\n",
      "Epoch:300, P:None, Loss:106.9616, Loss-ae:106.9616, Loss-topo:20677.9914\n",
      "Epoch:301, P:None, Loss:105.2962, Loss-ae:105.2962, Loss-topo:21629.1967\n",
      "Epoch:302, P:None, Loss:104.8414, Loss-ae:104.8414, Loss-topo:20198.2528\n",
      "Epoch:303, P:None, Loss:106.7978, Loss-ae:106.7978, Loss-topo:21983.7720\n",
      "Epoch:304, P:None, Loss:108.8826, Loss-ae:108.8826, Loss-topo:20645.1777\n",
      "Epoch:305, P:None, Loss:121.3319, Loss-ae:121.3319, Loss-topo:22175.9498\n",
      "Epoch:306, P:None, Loss:105.3920, Loss-ae:105.3920, Loss-topo:20158.5458\n",
      "Epoch:307, P:None, Loss:104.8466, Loss-ae:104.8466, Loss-topo:20956.1360\n",
      "Epoch:308, P:None, Loss:106.3550, Loss-ae:106.3550, Loss-topo:22870.5753\n",
      "Epoch:309, P:None, Loss:107.3043, Loss-ae:107.3043, Loss-topo:22179.6311\n",
      "Epoch:310, P:None, Loss:106.0181, Loss-ae:106.0181, Loss-topo:23464.2547\n",
      "Epoch:311, P:None, Loss:100.8623, Loss-ae:100.8623, Loss-topo:21732.1652\n",
      "Epoch:312, P:None, Loss:102.2829, Loss-ae:102.2829, Loss-topo:22365.3228\n",
      "Epoch:313, P:None, Loss:101.5418, Loss-ae:101.5418, Loss-topo:21174.3814\n",
      "Epoch:314, P:None, Loss:106.6855, Loss-ae:106.6855, Loss-topo:23232.8945\n",
      "Epoch:315, P:None, Loss:105.3221, Loss-ae:105.3221, Loss-topo:21856.6861\n",
      "Epoch:316, P:None, Loss:105.9820, Loss-ae:105.9820, Loss-topo:24229.2414\n",
      "Epoch:317, P:None, Loss:102.9749, Loss-ae:102.9749, Loss-topo:22969.8111\n",
      "Epoch:318, P:None, Loss:102.8601, Loss-ae:102.8601, Loss-topo:23612.7801\n",
      "Epoch:319, P:None, Loss:102.3696, Loss-ae:102.3696, Loss-topo:22751.9501\n",
      "Epoch:320, P:None, Loss:101.6609, Loss-ae:101.6609, Loss-topo:22464.0343\n",
      "Epoch:321, P:None, Loss:100.6513, Loss-ae:100.6513, Loss-topo:20948.2899\n",
      "Epoch:322, P:None, Loss:101.9248, Loss-ae:101.9248, Loss-topo:22276.0061\n",
      "Epoch:323, P:None, Loss:102.9243, Loss-ae:102.9243, Loss-topo:22762.6864\n",
      "Epoch:324, P:None, Loss:106.9713, Loss-ae:106.9713, Loss-topo:24664.0193\n",
      "Epoch:325, P:None, Loss:104.1214, Loss-ae:104.1214, Loss-topo:21943.5293\n",
      "Epoch:326, P:None, Loss:108.2020, Loss-ae:108.2020, Loss-topo:23807.0815\n",
      "Epoch:327, P:None, Loss:104.0519, Loss-ae:104.0519, Loss-topo:23263.9381\n",
      "Epoch:328, P:None, Loss:107.5954, Loss-ae:107.5954, Loss-topo:22572.7347\n",
      "Epoch:329, P:None, Loss:106.8891, Loss-ae:106.8891, Loss-topo:23545.2765\n",
      "Epoch:330, P:None, Loss:120.4923, Loss-ae:120.4923, Loss-topo:22011.5050\n",
      "Epoch:331, P:None, Loss:108.5081, Loss-ae:108.5081, Loss-topo:22052.4922\n",
      "Epoch:332, P:None, Loss:104.4237, Loss-ae:104.4237, Loss-topo:20522.4040\n",
      "Epoch:333, P:None, Loss:107.0331, Loss-ae:107.0331, Loss-topo:22425.0790\n",
      "Epoch:334, P:None, Loss:108.2450, Loss-ae:108.2450, Loss-topo:22538.1362\n",
      "Epoch:335, P:None, Loss:102.7723, Loss-ae:102.7723, Loss-topo:20252.5647\n",
      "Epoch:336, P:None, Loss:104.5934, Loss-ae:104.5934, Loss-topo:21174.7866\n",
      "Epoch:337, P:None, Loss:111.2946, Loss-ae:111.2946, Loss-topo:21638.1283\n",
      "Epoch:338, P:None, Loss:105.9403, Loss-ae:105.9403, Loss-topo:21657.8066\n",
      "Epoch:339, P:None, Loss:106.0303, Loss-ae:106.0303, Loss-topo:22275.3594\n",
      "Epoch:340, P:None, Loss:101.9703, Loss-ae:101.9703, Loss-topo:21361.3387\n",
      "Epoch:341, P:None, Loss:102.1307, Loss-ae:102.1307, Loss-topo:21085.8876\n",
      "Epoch:342, P:None, Loss:109.7514, Loss-ae:109.7514, Loss-topo:22880.7888\n",
      "Epoch:343, P:None, Loss:103.4886, Loss-ae:103.4886, Loss-topo:21451.2168\n",
      "Epoch:344, P:None, Loss:102.9807, Loss-ae:102.9807, Loss-topo:21253.3770\n",
      "Epoch:345, P:None, Loss:102.2406, Loss-ae:102.2406, Loss-topo:21832.8463\n",
      "Epoch:346, P:None, Loss:103.2318, Loss-ae:103.2318, Loss-topo:22972.1150\n",
      "Epoch:347, P:None, Loss:106.2941, Loss-ae:106.2941, Loss-topo:22890.3362\n",
      "Epoch:348, P:None, Loss:107.1335, Loss-ae:107.1335, Loss-topo:23263.8730\n",
      "Epoch:349, P:None, Loss:104.3876, Loss-ae:104.3876, Loss-topo:21697.9520\n",
      "Epoch:350, P:None, Loss:106.1441, Loss-ae:106.1441, Loss-topo:23772.2999\n",
      "Epoch:351, P:None, Loss:108.1982, Loss-ae:108.1982, Loss-topo:23694.8530\n",
      "Epoch:352, P:None, Loss:103.8275, Loss-ae:103.8275, Loss-topo:22613.9353\n",
      "Epoch:353, P:None, Loss:108.6999, Loss-ae:108.6999, Loss-topo:24556.1529\n",
      "Epoch:354, P:None, Loss:104.3891, Loss-ae:104.3891, Loss-topo:23294.6738\n",
      "Epoch:355, P:None, Loss:108.9640, Loss-ae:108.9640, Loss-topo:24959.2288\n",
      "Epoch:356, P:None, Loss:108.2148, Loss-ae:108.2148, Loss-topo:23651.3103\n",
      "Epoch:357, P:None, Loss:104.9938, Loss-ae:104.9938, Loss-topo:22962.8990\n",
      "Epoch:358, P:None, Loss:104.7154, Loss-ae:104.7154, Loss-topo:23631.9925\n",
      "Epoch:359, P:None, Loss:107.7347, Loss-ae:107.7347, Loss-topo:22398.2087\n",
      "Epoch:360, P:None, Loss:105.3139, Loss-ae:105.3139, Loss-topo:22460.2905\n",
      "Epoch:361, P:None, Loss:104.0321, Loss-ae:104.0321, Loss-topo:20573.6392\n",
      "Epoch:362, P:None, Loss:105.5530, Loss-ae:105.5530, Loss-topo:21226.8867\n",
      "Epoch:363, P:None, Loss:103.3165, Loss-ae:103.3165, Loss-topo:21465.5053\n",
      "Epoch:364, P:None, Loss:105.7914, Loss-ae:105.7914, Loss-topo:23041.2570\n",
      "Epoch:365, P:None, Loss:103.6476, Loss-ae:103.6476, Loss-topo:21933.4992\n",
      "Epoch:366, P:None, Loss:105.2154, Loss-ae:105.2154, Loss-topo:22777.2441\n",
      "Epoch:367, P:None, Loss:107.6611, Loss-ae:107.6611, Loss-topo:24753.8675\n",
      "Epoch:368, P:None, Loss:106.3134, Loss-ae:106.3134, Loss-topo:22853.2640\n",
      "Epoch:369, P:None, Loss:107.0660, Loss-ae:107.0660, Loss-topo:24558.3189\n",
      "Epoch:370, P:None, Loss:105.7274, Loss-ae:105.7274, Loss-topo:24920.5137\n",
      "Epoch:371, P:None, Loss:104.1017, Loss-ae:104.1017, Loss-topo:24361.8733\n",
      "Epoch:372, P:None, Loss:107.7901, Loss-ae:107.7901, Loss-topo:24470.4640\n",
      "Epoch:373, P:None, Loss:110.9936, Loss-ae:110.9936, Loss-topo:23337.1761\n",
      "Epoch:374, P:None, Loss:106.6454, Loss-ae:106.6454, Loss-topo:23388.1724\n",
      "Epoch:375, P:None, Loss:106.6122, Loss-ae:106.6122, Loss-topo:23919.5826\n",
      "Epoch:376, P:None, Loss:110.9662, Loss-ae:110.9662, Loss-topo:22776.4562\n",
      "Epoch:377, P:None, Loss:109.5094, Loss-ae:109.5094, Loss-topo:24002.8052\n",
      "Epoch:378, P:None, Loss:104.4563, Loss-ae:104.4563, Loss-topo:21873.0597\n",
      "Epoch:379, P:None, Loss:104.9407, Loss-ae:104.9407, Loss-topo:21894.2408\n",
      "Epoch:380, P:None, Loss:112.6243, Loss-ae:112.6243, Loss-topo:23791.7801\n",
      "Epoch:381, P:None, Loss:108.3473, Loss-ae:108.3473, Loss-topo:23263.8824\n",
      "Epoch:382, P:None, Loss:103.8428, Loss-ae:103.8428, Loss-topo:22180.2171\n",
      "Epoch:383, P:None, Loss:106.5373, Loss-ae:106.5373, Loss-topo:23583.7480\n",
      "Epoch:384, P:None, Loss:104.0470, Loss-ae:104.0470, Loss-topo:22430.1306\n",
      "Epoch:385, P:None, Loss:106.5004, Loss-ae:106.5004, Loss-topo:22308.9674\n",
      "Epoch:386, P:None, Loss:109.0840, Loss-ae:109.0840, Loss-topo:23209.4936\n",
      "Epoch:387, P:None, Loss:114.3197, Loss-ae:114.3197, Loss-topo:23282.7746\n",
      "Epoch:388, P:None, Loss:106.3820, Loss-ae:106.3820, Loss-topo:23146.5678\n",
      "Epoch:389, P:None, Loss:109.5438, Loss-ae:109.5438, Loss-topo:23568.0140\n",
      "Epoch:390, P:None, Loss:107.0922, Loss-ae:107.0922, Loss-topo:21834.7581\n",
      "Epoch:391, P:None, Loss:109.4300, Loss-ae:109.4300, Loss-topo:24222.0067\n",
      "Epoch:392, P:None, Loss:104.7461, Loss-ae:104.7461, Loss-topo:23320.1900\n",
      "Epoch:393, P:None, Loss:109.7742, Loss-ae:109.7742, Loss-topo:24089.9568\n",
      "Epoch:394, P:None, Loss:107.0248, Loss-ae:107.0248, Loss-topo:24648.9618\n",
      "Epoch:395, P:None, Loss:108.4964, Loss-ae:108.4964, Loss-topo:24906.8672\n",
      "Epoch:396, P:None, Loss:108.6156, Loss-ae:108.6156, Loss-topo:23791.6518\n",
      "Epoch:397, P:None, Loss:108.3846, Loss-ae:108.3846, Loss-topo:22853.5181\n",
      "Epoch:398, P:None, Loss:111.1479, Loss-ae:111.1479, Loss-topo:24662.2123\n",
      "Epoch:399, P:None, Loss:111.2975, Loss-ae:111.2975, Loss-topo:23330.5667\n",
      "Epoch:400, P:None, Loss:109.9998, Loss-ae:109.9998, Loss-topo:24374.7266\n",
      "Epoch:401, P:None, Loss:112.3895, Loss-ae:112.3895, Loss-topo:22960.5075\n",
      "Epoch:402, P:None, Loss:108.1813, Loss-ae:108.1813, Loss-topo:24160.8666\n",
      "Epoch:403, P:None, Loss:103.4570, Loss-ae:103.4570, Loss-topo:21690.0092\n",
      "Epoch:404, P:None, Loss:108.2899, Loss-ae:108.2899, Loss-topo:23486.1543\n",
      "Epoch:405, P:None, Loss:116.5582, Loss-ae:116.5582, Loss-topo:22303.4146\n",
      "Epoch:406, P:None, Loss:109.0513, Loss-ae:109.0513, Loss-topo:21657.9713\n",
      "Epoch:407, P:None, Loss:105.4599, Loss-ae:105.4599, Loss-topo:21337.4062\n",
      "Epoch:408, P:None, Loss:108.4257, Loss-ae:108.4257, Loss-topo:23334.7785\n",
      "Epoch:409, P:None, Loss:106.8316, Loss-ae:106.8316, Loss-topo:22046.4372\n",
      "Epoch:410, P:None, Loss:112.6972, Loss-ae:112.6972, Loss-topo:23383.2921\n",
      "Epoch:411, P:None, Loss:104.8552, Loss-ae:104.8552, Loss-topo:22830.1373\n",
      "Epoch:412, P:None, Loss:109.5556, Loss-ae:109.5556, Loss-topo:22894.6362\n",
      "Epoch:413, P:None, Loss:105.6215, Loss-ae:105.6215, Loss-topo:23287.0050\n",
      "Epoch:414, P:None, Loss:106.9125, Loss-ae:106.9125, Loss-topo:23106.6281\n",
      "Epoch:415, P:None, Loss:104.7656, Loss-ae:104.7656, Loss-topo:23963.3435\n",
      "Epoch:416, P:None, Loss:106.9332, Loss-ae:106.9332, Loss-topo:23433.7087\n",
      "Epoch:417, P:None, Loss:108.0382, Loss-ae:108.0382, Loss-topo:22828.3496\n",
      "Epoch:418, P:None, Loss:115.1459, Loss-ae:115.1459, Loss-topo:24070.4810\n",
      "Epoch:419, P:None, Loss:107.8158, Loss-ae:107.8158, Loss-topo:22608.4958\n",
      "Epoch:420, P:None, Loss:105.9853, Loss-ae:105.9853, Loss-topo:22510.7573\n",
      "Epoch:421, P:None, Loss:109.2613, Loss-ae:109.2613, Loss-topo:23153.2729\n",
      "Epoch:422, P:None, Loss:108.4288, Loss-ae:108.4288, Loss-topo:22249.6797\n",
      "Epoch:423, P:None, Loss:105.8697, Loss-ae:105.8697, Loss-topo:22060.9425\n",
      "Epoch:424, P:None, Loss:105.9363, Loss-ae:105.9363, Loss-topo:22020.0349\n",
      "Epoch:425, P:None, Loss:105.5208, Loss-ae:105.5208, Loss-topo:22141.7154\n",
      "Epoch:426, P:None, Loss:110.5049, Loss-ae:110.5049, Loss-topo:23768.6744\n",
      "Epoch:427, P:None, Loss:108.9011, Loss-ae:108.9011, Loss-topo:22463.6532\n",
      "Epoch:428, P:None, Loss:105.4245, Loss-ae:105.4245, Loss-topo:22716.6116\n",
      "Epoch:429, P:None, Loss:109.1694, Loss-ae:109.1694, Loss-topo:22469.7720\n",
      "Epoch:430, P:None, Loss:107.0727, Loss-ae:107.0727, Loss-topo:22688.1217\n",
      "Epoch:431, P:None, Loss:109.7558, Loss-ae:109.7558, Loss-topo:23703.6571\n",
      "Epoch:432, P:None, Loss:107.1821, Loss-ae:107.1821, Loss-topo:23153.6197\n",
      "Epoch:433, P:None, Loss:103.8614, Loss-ae:103.8614, Loss-topo:21169.6618\n",
      "Epoch:434, P:None, Loss:109.8884, Loss-ae:109.8884, Loss-topo:24182.2503\n",
      "Epoch:435, P:None, Loss:108.0219, Loss-ae:108.0219, Loss-topo:22315.3061\n",
      "Epoch:436, P:None, Loss:109.4864, Loss-ae:109.4864, Loss-topo:22789.3348\n",
      "Epoch:437, P:None, Loss:106.8722, Loss-ae:106.8722, Loss-topo:22265.2478\n",
      "Epoch:438, P:None, Loss:110.2516, Loss-ae:110.2516, Loss-topo:23586.0326\n",
      "Epoch:439, P:None, Loss:105.4670, Loss-ae:105.4670, Loss-topo:21942.6069\n",
      "Epoch:440, P:None, Loss:109.5741, Loss-ae:109.5741, Loss-topo:23747.1844\n",
      "Epoch:441, P:None, Loss:107.6606, Loss-ae:107.6606, Loss-topo:22872.1883\n",
      "Epoch:442, P:None, Loss:108.8136, Loss-ae:108.8136, Loss-topo:23469.3577\n",
      "Epoch:443, P:None, Loss:107.6204, Loss-ae:107.6204, Loss-topo:21995.8142\n",
      "Epoch:444, P:None, Loss:107.6648, Loss-ae:107.6648, Loss-topo:21646.8504\n",
      "Epoch:445, P:None, Loss:109.3810, Loss-ae:109.3810, Loss-topo:23040.6099\n",
      "Epoch:446, P:None, Loss:108.2897, Loss-ae:108.2897, Loss-topo:23168.4102\n",
      "Epoch:447, P:None, Loss:108.0043, Loss-ae:108.0043, Loss-topo:22687.5193\n",
      "Epoch:448, P:None, Loss:109.4411, Loss-ae:109.4411, Loss-topo:23599.4665\n",
      "Epoch:449, P:None, Loss:110.5267, Loss-ae:110.5267, Loss-topo:23283.7028\n",
      "Epoch:450, P:None, Loss:103.9081, Loss-ae:103.9081, Loss-topo:22806.4129\n",
      "Epoch:451, P:None, Loss:102.8203, Loss-ae:102.8203, Loss-topo:22826.6657\n",
      "Epoch:452, P:None, Loss:102.6817, Loss-ae:102.6817, Loss-topo:22308.1936\n",
      "Epoch:453, P:None, Loss:109.9473, Loss-ae:109.9473, Loss-topo:23509.1242\n",
      "Epoch:454, P:None, Loss:111.3415, Loss-ae:111.3415, Loss-topo:23238.1426\n",
      "Epoch:455, P:None, Loss:104.9444, Loss-ae:104.9444, Loss-topo:22501.8873\n",
      "Epoch:456, P:None, Loss:103.4044, Loss-ae:103.4044, Loss-topo:22507.6696\n",
      "Epoch:457, P:None, Loss:105.0608, Loss-ae:105.0608, Loss-topo:22772.3555\n",
      "Epoch:458, P:None, Loss:104.8389, Loss-ae:104.8389, Loss-topo:22670.2411\n",
      "Epoch:459, P:None, Loss:101.4283, Loss-ae:101.4283, Loss-topo:22168.1772\n",
      "Epoch:460, P:None, Loss:109.6762, Loss-ae:109.6762, Loss-topo:23209.4554\n",
      "Epoch:461, P:None, Loss:105.4526, Loss-ae:105.4526, Loss-topo:22338.9819\n",
      "Epoch:462, P:None, Loss:108.2308, Loss-ae:108.2308, Loss-topo:22269.8050\n",
      "Epoch:463, P:None, Loss:103.1540, Loss-ae:103.1540, Loss-topo:22271.1579\n",
      "Epoch:464, P:None, Loss:108.8822, Loss-ae:108.8822, Loss-topo:22528.8345\n",
      "Epoch:465, P:None, Loss:101.6288, Loss-ae:101.6288, Loss-topo:21033.8806\n",
      "Epoch:466, P:None, Loss:107.9074, Loss-ae:107.9074, Loss-topo:23455.6356\n",
      "Epoch:467, P:None, Loss:103.1169, Loss-ae:103.1169, Loss-topo:21753.9693\n",
      "Epoch:468, P:None, Loss:104.2127, Loss-ae:104.2127, Loss-topo:22534.4749\n",
      "Epoch:469, P:None, Loss:107.3689, Loss-ae:107.3689, Loss-topo:22767.8348\n",
      "Epoch:470, P:None, Loss:108.7029, Loss-ae:108.7029, Loss-topo:22810.2408\n",
      "Epoch:471, P:None, Loss:107.7127, Loss-ae:107.7127, Loss-topo:21916.2006\n",
      "Epoch:472, P:None, Loss:105.4873, Loss-ae:105.4873, Loss-topo:22300.6858\n",
      "Epoch:473, P:None, Loss:106.9955, Loss-ae:106.9955, Loss-topo:22967.9927\n",
      "Epoch:474, P:None, Loss:106.7858, Loss-ae:106.7858, Loss-topo:23161.3256\n",
      "Epoch:475, P:None, Loss:104.7003, Loss-ae:104.7003, Loss-topo:21765.4124\n",
      "Epoch:476, P:None, Loss:105.8765, Loss-ae:105.8765, Loss-topo:20918.3407\n",
      "Epoch:477, P:None, Loss:109.6535, Loss-ae:109.6535, Loss-topo:23344.7743\n",
      "Epoch:478, P:None, Loss:109.7978, Loss-ae:109.7978, Loss-topo:22506.7824\n",
      "Epoch:479, P:None, Loss:105.0946, Loss-ae:105.0946, Loss-topo:21516.7737\n",
      "Epoch:480, P:None, Loss:104.6757, Loss-ae:104.6757, Loss-topo:21126.6928\n",
      "Epoch:481, P:None, Loss:103.6476, Loss-ae:103.6476, Loss-topo:20840.4626\n",
      "Epoch:482, P:None, Loss:105.5028, Loss-ae:105.5028, Loss-topo:21942.7098\n",
      "Epoch:483, P:None, Loss:104.3519, Loss-ae:104.3519, Loss-topo:21589.7748\n",
      "Epoch:484, P:None, Loss:104.0402, Loss-ae:104.0402, Loss-topo:21159.5158\n",
      "Epoch:485, P:None, Loss:102.3680, Loss-ae:102.3680, Loss-topo:21345.4699\n",
      "Epoch:486, P:None, Loss:104.6305, Loss-ae:104.6305, Loss-topo:20843.5075\n",
      "Epoch:487, P:None, Loss:107.2054, Loss-ae:107.2054, Loss-topo:21488.7693\n",
      "Epoch:488, P:None, Loss:105.4511, Loss-ae:105.4511, Loss-topo:21342.1264\n",
      "Epoch:489, P:None, Loss:106.2679, Loss-ae:106.2679, Loss-topo:22122.0725\n",
      "Epoch:490, P:None, Loss:105.1163, Loss-ae:105.1163, Loss-topo:21157.9953\n",
      "Epoch:491, P:None, Loss:103.0958, Loss-ae:103.0958, Loss-topo:21478.3463\n",
      "Epoch:492, P:None, Loss:106.1496, Loss-ae:106.1496, Loss-topo:22108.2388\n",
      "Epoch:493, P:None, Loss:103.5986, Loss-ae:103.5986, Loss-topo:21986.9866\n",
      "Epoch:494, P:None, Loss:108.2135, Loss-ae:108.2135, Loss-topo:22813.6401\n",
      "Epoch:495, P:None, Loss:107.3684, Loss-ae:107.3684, Loss-topo:23221.9252\n",
      "Epoch:496, P:None, Loss:102.0021, Loss-ae:102.0021, Loss-topo:21173.6032\n",
      "Epoch:497, P:None, Loss:101.6751, Loss-ae:101.6751, Loss-topo:21427.3315\n",
      "Epoch:498, P:None, Loss:106.8122, Loss-ae:106.8122, Loss-topo:21119.1864\n",
      "Epoch:499, P:None, Loss:106.4469, Loss-ae:106.4469, Loss-topo:22188.6861\n",
      "Epoch:500, P:None, Loss:107.8079, Loss-ae:107.8079, Loss-topo:21566.7665\n",
      "Epoch:501, P:None, Loss:109.3048, Loss-ae:109.3048, Loss-topo:22355.0053\n",
      "Epoch:502, P:None, Loss:107.4472, Loss-ae:107.4472, Loss-topo:21756.4191\n",
      "Epoch:503, P:None, Loss:108.2681, Loss-ae:108.2681, Loss-topo:20624.2024\n",
      "Epoch:504, P:None, Loss:110.3116, Loss-ae:110.3116, Loss-topo:21723.4690\n",
      "Epoch:505, P:None, Loss:107.2631, Loss-ae:107.2631, Loss-topo:21398.4980\n",
      "Epoch:506, P:None, Loss:107.3262, Loss-ae:107.3262, Loss-topo:21570.2877\n",
      "Epoch:507, P:None, Loss:105.6563, Loss-ae:105.6563, Loss-topo:21781.6415\n",
      "Epoch:508, P:None, Loss:103.7670, Loss-ae:103.7670, Loss-topo:20400.4436\n",
      "Epoch:509, P:None, Loss:106.2678, Loss-ae:106.2678, Loss-topo:19687.6800\n",
      "Epoch:510, P:None, Loss:106.9679, Loss-ae:106.9679, Loss-topo:20683.7768\n",
      "Epoch:511, P:None, Loss:105.0083, Loss-ae:105.0083, Loss-topo:19351.0304\n",
      "Epoch:512, P:None, Loss:105.0965, Loss-ae:105.0965, Loss-topo:19311.2801\n",
      "Epoch:513, P:None, Loss:105.6056, Loss-ae:105.6056, Loss-topo:20541.0491\n",
      "Epoch:514, P:None, Loss:104.7411, Loss-ae:104.7411, Loss-topo:20024.3742\n",
      "Epoch:515, P:None, Loss:110.3925, Loss-ae:110.3925, Loss-topo:21425.1585\n",
      "Epoch:516, P:None, Loss:103.8502, Loss-ae:103.8502, Loss-topo:19847.1881\n",
      "Epoch:517, P:None, Loss:106.2028, Loss-ae:106.2028, Loss-topo:20813.5047\n",
      "Epoch:518, P:None, Loss:103.8260, Loss-ae:103.8260, Loss-topo:20314.4441\n",
      "Epoch:519, P:None, Loss:104.1316, Loss-ae:104.1316, Loss-topo:19423.6271\n",
      "Epoch:520, P:None, Loss:108.1489, Loss-ae:108.1489, Loss-topo:20431.0929\n",
      "Epoch:521, P:None, Loss:105.8595, Loss-ae:105.8595, Loss-topo:21363.6230\n",
      "Epoch:522, P:None, Loss:105.0542, Loss-ae:105.0542, Loss-topo:21052.9138\n",
      "Epoch:523, P:None, Loss:107.6045, Loss-ae:107.6045, Loss-topo:21672.7785\n",
      "Epoch:524, P:None, Loss:106.1293, Loss-ae:106.1293, Loss-topo:21618.5544\n",
      "Epoch:525, P:None, Loss:110.6807, Loss-ae:110.6807, Loss-topo:21027.2914\n",
      "Epoch:526, P:None, Loss:105.5270, Loss-ae:105.5270, Loss-topo:21146.7645\n",
      "Epoch:527, P:None, Loss:103.4839, Loss-ae:103.4839, Loss-topo:21513.2408\n",
      "Epoch:528, P:None, Loss:107.3138, Loss-ae:107.3138, Loss-topo:21859.6616\n",
      "Epoch:529, P:None, Loss:103.6791, Loss-ae:103.6791, Loss-topo:21430.5859\n",
      "Epoch:530, P:None, Loss:108.8754, Loss-ae:108.8754, Loss-topo:21654.5402\n",
      "Epoch:531, P:None, Loss:102.9341, Loss-ae:102.9341, Loss-topo:20868.6805\n",
      "Epoch:532, P:None, Loss:109.4693, Loss-ae:109.4693, Loss-topo:21428.0946\n",
      "Epoch:533, P:None, Loss:104.5018, Loss-ae:104.5018, Loss-topo:21516.2732\n",
      "Epoch:534, P:None, Loss:106.6860, Loss-ae:106.6860, Loss-topo:21479.7377\n",
      "Epoch:535, P:None, Loss:106.9164, Loss-ae:106.9164, Loss-topo:21864.4481\n",
      "Epoch:536, P:None, Loss:106.5479, Loss-ae:106.5479, Loss-topo:21043.2238\n",
      "Epoch:537, P:None, Loss:104.9732, Loss-ae:104.9732, Loss-topo:21972.4701\n",
      "Epoch:538, P:None, Loss:106.9003, Loss-ae:106.9003, Loss-topo:22082.8491\n",
      "Epoch:539, P:None, Loss:102.7518, Loss-ae:102.7518, Loss-topo:19873.2944\n",
      "Epoch:540, P:None, Loss:103.7768, Loss-ae:103.7768, Loss-topo:20911.0541\n",
      "Epoch:541, P:None, Loss:108.1268, Loss-ae:108.1268, Loss-topo:20839.9152\n",
      "Epoch:542, P:None, Loss:101.5487, Loss-ae:101.5487, Loss-topo:20434.6727\n",
      "Epoch:543, P:None, Loss:102.4776, Loss-ae:102.4776, Loss-topo:20187.0353\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m title_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMotionSense 20Hz\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTopoAE lambda \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_lam)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtopo_reducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_HD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle_plot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle_plot\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/Topological_ae/MotionSense20Hz/../../../librep/transforms/topo_ae.py:73\u001b[0m, in \u001b[0;36mTopologicalDimensionalityReduction.fit\u001b[0;34m(self, X, y, title_plot)\u001b[0m\n\u001b[1;32m     71\u001b[0m loss, loss_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(in_tensor)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 73\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     75\u001b[0m epoch_train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:484\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    476\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    477\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 484\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:191\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98d66c-b630-493e-a3d0-6aee364a1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456c1f2-f856-44b5-b4c3-1c1b59781951",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52697960-6cb0-4f76-a033-21828d9effbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97126520-db06-4926-9f62-b8382a68d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d394d-adf1-4b47-90dc-47d402fc389b",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a246d2-1000-4a0b-a833-e13b05bfaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 1\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='DeepAE_custom_dim2',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd10d4c-3f50-46a1-89e7-5d03513b1908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3455110-7d0f-4ae3-981b-dfc4d60b390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a5042-2a17-405a-9050-d4db3696d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc9431-db7f-4d90-842a-5973855211e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db0e416-dedc-42bf-ae22-3de332d7f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc62e6a4-6962-47e2-84f9-29fff5c662f3",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb29ec-76b3-4f98-bdd5-404e64da5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 5\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='DeepAE_custom_dim2',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8782b60-204a-4e91-a8fd-a56ab3d50a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a924330-75ac-40f1-9756-b7f56c2df3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a64ca-2da3-41bb-b20c-3566ae56876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8dac9c-8a14-41a9-8974-27708e65a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f9fad-ac7a-4e0c-986b-9b4488e9f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9003d3-7f20-4458-bcf2-29e3e96eeb2d",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58053da5-5f27-4cc1-b960-a1091462f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 10\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='DeepAE_custom_dim2',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90b324-05ae-46ea-871b-cd1bf602cf57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f32e46-bfa8-4fd9-97e4-becfe3c0c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2948fd-3ff5-4055-bd5d-1ea460ece54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f48f0c-8167-4f92-b5e3-41026aeca3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a7cfa-75b3-453e-b1d9-ded339fb22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b63bc06-eb93-4755-8ee3-6add8e40f7c1",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454ea1e-b392-4867-8f6d-8d04ffcee910",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 100\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='DeepAE_custom_dim2',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122769a9-ea11-49de-8476-cd57135a5f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bb0dd-d189-4e9b-83de-2d9371e450cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a42d610-e5ac-4c50-97cd-830abea35003",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae177f50-7f6e-4af4-85c4-94684b3557aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4c244-97bd-4d17-b263-88ec61dcfef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c976d2-0a0e-4182-bd33-2b80770c1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing with Convolutional Topological Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852209ec-7f7b-4f21-8f46-a49b400908bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lam = 1\n",
    "# model_dim = 10\n",
    "# model_epc = 200\n",
    "# topo_reducer = TopologicalDimensionalityReduction(\n",
    "#     ae_model='ConvolutionalAutoencoder_custom_dim',\n",
    "#     lam = model_lam,\n",
    "#     ae_kwargs = {'input_dims':(1, 360), 'custom_dim':model_dim},\n",
    "#     input_shape = (-1, 1, 1, 360),\n",
    "#     patience = None,\n",
    "#     num_epochs = model_epc\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b59a5-053a-421e-b61f-50209c36a08e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "# topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf2efc3-cff5-4500-b9d3-270d3bcf66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_LD = topo_reducer.transform(train_HD)\n",
    "# test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f8f6d-a630-4f07-92bd-aba4c7da436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "# print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "# test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "# print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5672350-3d30-4e77-b43a-79271ebf2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "# experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e07fc9-8512-434f-84da-53e7366cccfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
