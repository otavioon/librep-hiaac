{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1630ed-63c8-4ee7-86c9-62dfb528b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da911e3-e82f-47c9-8090-f71f453956ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f067405-ab8c-4f22-b056-ab9e1febe71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5007e3-ef19-4658-ac7d-2a6f617bb9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 16:45:46.504973: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-19 16:45:46.520091: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from librep.datasets.har.loaders import (\n",
    "    MotionSense_BalancedView20HZ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9f66b2-7b1c-4d4e-a0e3-cde9552a595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librep.transforms.topo_ae import (\n",
    "    TopologicalDimensionalityReduction,\n",
    "    CustomTopoDimRedTransform\n",
    ")\n",
    "from librep.transforms import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "from experiments.Topological_ae.Experiment_utils import *\n",
    "from librep.datasets.multimodal import TransformMultiModalDataset\n",
    "from librep.transforms.fft import FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85bea424-07ff-47ec-945a-aef385afe7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Balanced MotionSense View Resampled to 20Hz with Gravity - Multiplied acc by 9.81m/sÂ²\n",
       "\n",
       "This is a view from [MotionSense] that was spllited into 3s windows and was resampled to 20Hz using the [FFT method](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.resample.html#scipy.signal.resample). \n",
       "\n",
       "The data was first splitted in three sets: train, validation and test. Each one with the following proportions:\n",
       "- Train: 70% of samples\n",
       "- Validation: 10% of samples\n",
       "- Test: 20% of samples\n",
       "\n",
       "After splits, the datasets were balanced in relation to the activity code column, that is, each subset have the same number of activitiy samples.\n",
       "\n",
       "**NOTE**: Each subset contain samples from distinct users, that is, samples of one user belongs exclusivelly to one of three subsets.\n",
       "\n",
       "## Activity codes\n",
       "- 0: downstairs (569 train, 101 validation, 170 test) \n",
       "- 1: upstairs (569 train, 101 validation, 170 test) \n",
       "- 2: sitting (569 train, 101 validation, 170 test) \n",
       "- 3: standing (569 train, 101 validation, 170 test) \n",
       "- 4: walking (569 train, 101 validation, 170 test) \n",
       "- 5: jogging (569 train, 101 validation, 170 test) \n",
       " \n",
       "\n",
       "## Standartized activity codes\n",
       "- 0: sit (569 train, 101 validation, 170 test) \n",
       "- 1: stand (569 train, 101 validation, 170 test) \n",
       "- 2: walk (569 train, 101 validation, 170 test) \n",
       "- 3: stair up (569 train, 101 validation, 170 test) \n",
       "- 4: stair down (569 train, 101 validation, 170 test) \n",
       "- 5: run (569 train, 101 validation, 170 test) \n",
       "      \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MotionSense Loader\n",
    "loader = MotionSense_BalancedView20HZ(\n",
    "    root_dir=\"../../../data/views/MotionSense/balanced_view_20Hz_with_gravity_9.81_acc_standard\", \n",
    "    download=False\n",
    ")\n",
    "\n",
    "# Print the readme (optional)\n",
    "loader.print_readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d817b0c-4e6b-4227-9b48-cd23c3df190d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PandasMultiModalDataset: samples=4020, features=360, no. window=6, label_columns='standard activity code',\n",
       " PandasMultiModalDataset: samples=1020, features=360, no. window=6, label_columns='standard activity code')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# If concat_train_validation is true, return a tuple (train+validation, test)\n",
    "train_val, test = loader.load(concat_train_validation=True, label=loader.standard_label)\n",
    "train_val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9491b1e0-0b60-439c-9bed-ea0ee7918b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_HD = np.array(train_val[:][0])\n",
    "train_Y = np.array(train_val[:][1])\n",
    "test_HD = np.array(test[:][0])\n",
    "test_Y = np.array(test[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc4810ac-c560-4b23-9b72-481a2d0f1a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4020, 360) (4020,) (1020, 360) (1020,)\n"
     ]
    }
   ],
   "source": [
    "print(train_HD.shape, train_Y.shape, test_HD.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93d061-6ea0-418b-b690-2d977f6297c9",
   "metadata": {},
   "source": [
    "# Aplicar FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5255de5-0259-404a-a2d8-507ad4f2ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_transform = FFT(centered = True)\n",
    "transformer = TransformMultiModalDataset(\n",
    "    transforms=[fft_transform],\n",
    "    new_window_name_prefix=\"fft.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29195f38-f8a9-4167-937f-d3f98ddc7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_fft = transformer(train_val)\n",
    "test_dataset_fft = transformer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8cfc25-0323-498a-931d-1249078f2796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4020, 180)\n",
      "(1020, 180)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_fft.X.shape)\n",
    "print(test_dataset_fft.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d413cc20-e5b5-4594-b1f7-f11153c00c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797.3368276806895\n"
     ]
    }
   ],
   "source": [
    "print(np.max(train_dataset_fft.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fea2e105-a571-4a2f-ac76-096b08a6ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_HD = train_dataset_fft.X\n",
    "train_LD = None\n",
    "train_Y = train_dataset_fft.y\n",
    "test_HD = test_dataset_fft.X\n",
    "test_LD = None\n",
    "test_Y = test_dataset_fft.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee13aef1-77c7-42a9-930b-b1a56f980d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4020, 180) (4020,) (1020, 180) (1020,)\n"
     ]
    }
   ],
   "source": [
    "print(train_HD.shape, train_Y.shape, test_HD.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adbc2e-4af6-40a4-85dc-99b2336d656c",
   "metadata": {},
   "source": [
    "# Visualization helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d56d7ec7-6bf5-4c32-91aa-d48a8746a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: sit (569 train, 101 validation, 170 test)\n",
    "# 1: stand (569 train, 101 validation, 170 test)\n",
    "# 2: walk (569 train, 101 validation, 170 test)\n",
    "# 3: stair up (569 train, 101 validation, 170 test)\n",
    "# 4: stair down (569 train, 101 validation, 170 test)\n",
    "# 5: run (569 train, 101 validation, 170 test)\n",
    "def visualize(X, Y):\n",
    "    labels = ['sit', 'stand', 'walk', 'stair up', 'stair down', 'run']\n",
    "    df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=Y))\n",
    "    groups = df.groupby('label')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.margins(0.05)\n",
    "    for name, group in groups:\n",
    "        ax.plot(group.x, group.y, marker='.', linestyle='', ms=8, label=labels[name])\n",
    "    # Shrink current axis by 20%\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "    # Put a legend to the right of the current axis\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80f4771a-da3f-407b-9fa9-b0619a129e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to reuse\n",
    "model_dim = 10\n",
    "model_epc = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8264b9-7756-4a8d-a8c0-18a1645b8e28",
   "metadata": {},
   "source": [
    "# Reducing with Convolutional Topological Autoencoders (L=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eefd1ec-28b6-4937-872a-ce26dc99d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lam = 1\n",
    "# model_dim = 10\n",
    "# model_epc = 1500\n",
    "# topo_reducer = TopologicalDimensionalityReduction(\n",
    "#     ae_model='ConvolutionalAutoencoder_custom_dim2',\n",
    "#     lam = model_lam,\n",
    "#     ae_kwargs = {'input_dims':(1, 180), 'custom_dim':model_dim},\n",
    "#     input_shape = (-1, 1, 1, 180),\n",
    "#     patience = None,\n",
    "#     num_epochs = model_epc\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d80ae63d-92f7-49c6-9527-212af7500573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topologically Regularized ConvolutionalAutoencoder_custom_dim3\n",
      "Using python to compute signatures\n",
      "ConvAECustomDim, Input: (1, 180) Inner dim: 10\n",
      "ENCODER STRUCT torch.Size([4, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "model_lam = 0\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f8ff8-e7a8-42e1-9fd2-d1e9a17b8cad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, P:None, Loss:2442.5205, Loss-ae:2442.5205, Loss-topo:34.1982\n",
      "Epoch:2, P:None, Loss:1782.0148, Loss-ae:1782.0148, Loss-topo:430.1654\n",
      "Epoch:3, P:None, Loss:683.2181, Loss-ae:683.2181, Loss-topo:1048.2666\n",
      "Epoch:4, P:None, Loss:465.9476, Loss-ae:465.9476, Loss-topo:2186.6462\n",
      "Epoch:5, P:None, Loss:396.5730, Loss-ae:396.5730, Loss-topo:2759.6106\n",
      "Epoch:6, P:None, Loss:335.5221, Loss-ae:335.5221, Loss-topo:4672.4887\n",
      "Epoch:7, P:None, Loss:310.5788, Loss-ae:310.5788, Loss-topo:7606.8613\n",
      "Epoch:8, P:None, Loss:291.7676, Loss-ae:291.7676, Loss-topo:10290.2036\n",
      "Epoch:9, P:None, Loss:270.2848, Loss-ae:270.2848, Loss-topo:13498.3991\n",
      "Epoch:10, P:None, Loss:252.7556, Loss-ae:252.7556, Loss-topo:14198.0485\n",
      "Epoch:11, P:None, Loss:246.3988, Loss-ae:246.3988, Loss-topo:14799.4371\n",
      "Epoch:12, P:None, Loss:232.2044, Loss-ae:232.2044, Loss-topo:19377.0665\n",
      "Epoch:13, P:None, Loss:218.5465, Loss-ae:218.5465, Loss-topo:21524.9590\n",
      "Epoch:14, P:None, Loss:197.7622, Loss-ae:197.7622, Loss-topo:25102.1002\n",
      "Epoch:15, P:None, Loss:191.6178, Loss-ae:191.6178, Loss-topo:23107.1049\n",
      "Epoch:16, P:None, Loss:184.8200, Loss-ae:184.8200, Loss-topo:25594.3577\n",
      "Epoch:17, P:None, Loss:170.4548, Loss-ae:170.4548, Loss-topo:27230.4693\n",
      "Epoch:18, P:None, Loss:165.6356, Loss-ae:165.6356, Loss-topo:27331.3438\n",
      "Epoch:19, P:None, Loss:165.1864, Loss-ae:165.1864, Loss-topo:32995.9760\n",
      "Epoch:20, P:None, Loss:160.0858, Loss-ae:160.0858, Loss-topo:35380.5438\n",
      "Epoch:21, P:None, Loss:169.4633, Loss-ae:169.4633, Loss-topo:38603.1077\n",
      "Epoch:22, P:None, Loss:168.7209, Loss-ae:168.7209, Loss-topo:36531.0979\n",
      "Epoch:23, P:None, Loss:156.4220, Loss-ae:156.4220, Loss-topo:36215.0073\n",
      "Epoch:24, P:None, Loss:152.9738, Loss-ae:152.9738, Loss-topo:40001.4375\n",
      "Epoch:25, P:None, Loss:158.4379, Loss-ae:158.4379, Loss-topo:45095.9503\n",
      "Epoch:26, P:None, Loss:154.7758, Loss-ae:154.7758, Loss-topo:46244.2095\n",
      "Epoch:27, P:None, Loss:146.6222, Loss-ae:146.6222, Loss-topo:44245.4967\n",
      "Epoch:28, P:None, Loss:150.6843, Loss-ae:150.6843, Loss-topo:47397.3571\n",
      "Epoch:29, P:None, Loss:144.6763, Loss-ae:144.6763, Loss-topo:43982.1334\n",
      "Epoch:30, P:None, Loss:153.3955, Loss-ae:153.3955, Loss-topo:52488.6049\n",
      "Epoch:31, P:None, Loss:140.1063, Loss-ae:140.1063, Loss-topo:50305.6680\n",
      "Epoch:32, P:None, Loss:150.1499, Loss-ae:150.1499, Loss-topo:51537.7896\n",
      "Epoch:33, P:None, Loss:136.8304, Loss-ae:136.8304, Loss-topo:51041.4721\n",
      "Epoch:34, P:None, Loss:142.3843, Loss-ae:142.3843, Loss-topo:54917.5921\n",
      "Epoch:35, P:None, Loss:141.0890, Loss-ae:141.0890, Loss-topo:60362.4107\n",
      "Epoch:36, P:None, Loss:140.3811, Loss-ae:140.3811, Loss-topo:54242.6417\n",
      "Epoch:37, P:None, Loss:146.2814, Loss-ae:146.2814, Loss-topo:55506.1049\n",
      "Epoch:38, P:None, Loss:138.4131, Loss-ae:138.4131, Loss-topo:65963.8259\n",
      "Epoch:39, P:None, Loss:133.6848, Loss-ae:133.6848, Loss-topo:61275.5469\n",
      "Epoch:40, P:None, Loss:140.9245, Loss-ae:140.9245, Loss-topo:56747.8454\n",
      "Epoch:41, P:None, Loss:135.4682, Loss-ae:135.4682, Loss-topo:62850.3488\n",
      "Epoch:42, P:None, Loss:137.3384, Loss-ae:137.3384, Loss-topo:63547.3772\n",
      "Epoch:43, P:None, Loss:131.7903, Loss-ae:131.7903, Loss-topo:69596.7852\n",
      "Epoch:44, P:None, Loss:138.5545, Loss-ae:138.5545, Loss-topo:66192.7924\n",
      "Epoch:45, P:None, Loss:138.0824, Loss-ae:138.0824, Loss-topo:63069.1484\n",
      "Epoch:46, P:None, Loss:130.5727, Loss-ae:130.5727, Loss-topo:64198.7288\n",
      "Epoch:47, P:None, Loss:132.0427, Loss-ae:132.0427, Loss-topo:70355.1378\n",
      "Epoch:48, P:None, Loss:131.5329, Loss-ae:131.5329, Loss-topo:66389.7003\n",
      "Epoch:49, P:None, Loss:128.7557, Loss-ae:128.7557, Loss-topo:72519.1105\n",
      "Epoch:50, P:None, Loss:132.6789, Loss-ae:132.6789, Loss-topo:67123.5391\n",
      "Epoch:51, P:None, Loss:129.4566, Loss-ae:129.4566, Loss-topo:72813.5647\n",
      "Epoch:52, P:None, Loss:128.6802, Loss-ae:128.6802, Loss-topo:77455.6077\n",
      "Epoch:53, P:None, Loss:131.0828, Loss-ae:131.0828, Loss-topo:73820.2623\n",
      "Epoch:54, P:None, Loss:123.0371, Loss-ae:123.0371, Loss-topo:62921.3013\n",
      "Epoch:55, P:None, Loss:127.7097, Loss-ae:127.7097, Loss-topo:72580.3281\n",
      "Epoch:56, P:None, Loss:103.3147, Loss-ae:103.3147, Loss-topo:71737.0965\n",
      "Epoch:57, P:None, Loss:98.0844, Loss-ae:98.0844, Loss-topo:75018.0670\n",
      "Epoch:58, P:None, Loss:95.7345, Loss-ae:95.7345, Loss-topo:83640.4626\n",
      "Epoch:59, P:None, Loss:93.5202, Loss-ae:93.5202, Loss-topo:78079.8650\n",
      "Epoch:60, P:None, Loss:87.2601, Loss-ae:87.2601, Loss-topo:80044.5273\n",
      "Epoch:61, P:None, Loss:88.4759, Loss-ae:88.4759, Loss-topo:78487.9023\n",
      "Epoch:62, P:None, Loss:88.7369, Loss-ae:88.7369, Loss-topo:83242.6629\n",
      "Epoch:63, P:None, Loss:86.9564, Loss-ae:86.9564, Loss-topo:86391.6021\n",
      "Epoch:64, P:None, Loss:86.9916, Loss-ae:86.9916, Loss-topo:81355.3711\n",
      "Epoch:65, P:None, Loss:88.5657, Loss-ae:88.5657, Loss-topo:87977.3761\n",
      "Epoch:66, P:None, Loss:85.7352, Loss-ae:85.7352, Loss-topo:79192.6786\n",
      "Epoch:67, P:None, Loss:86.7308, Loss-ae:86.7308, Loss-topo:84801.4230\n",
      "Epoch:68, P:None, Loss:89.5612, Loss-ae:89.5612, Loss-topo:96473.8449\n",
      "Epoch:69, P:None, Loss:86.8940, Loss-ae:86.8940, Loss-topo:97884.9314\n",
      "Epoch:70, P:None, Loss:87.0863, Loss-ae:87.0863, Loss-topo:89265.9308\n",
      "Epoch:71, P:None, Loss:84.8199, Loss-ae:84.8199, Loss-topo:89179.4118\n",
      "Epoch:72, P:None, Loss:86.4809, Loss-ae:86.4809, Loss-topo:92349.9487\n",
      "Epoch:73, P:None, Loss:82.7969, Loss-ae:82.7969, Loss-topo:81942.7578\n",
      "Epoch:74, P:None, Loss:84.8743, Loss-ae:84.8743, Loss-topo:91037.9872\n",
      "Epoch:75, P:None, Loss:85.4230, Loss-ae:85.4230, Loss-topo:91879.1864\n",
      "Epoch:76, P:None, Loss:81.3140, Loss-ae:81.3140, Loss-topo:84677.3956\n",
      "Epoch:77, P:None, Loss:83.3280, Loss-ae:83.3280, Loss-topo:91032.1830\n",
      "Epoch:78, P:None, Loss:83.0250, Loss-ae:83.0250, Loss-topo:87617.9693\n",
      "Epoch:79, P:None, Loss:83.5680, Loss-ae:83.5680, Loss-topo:95960.7109\n",
      "Epoch:80, P:None, Loss:81.8945, Loss-ae:81.8945, Loss-topo:89014.4576\n",
      "Epoch:81, P:None, Loss:84.3619, Loss-ae:84.3619, Loss-topo:103550.1987\n",
      "Epoch:82, P:None, Loss:83.2161, Loss-ae:83.2161, Loss-topo:90091.4152\n",
      "Epoch:83, P:None, Loss:83.1358, Loss-ae:83.1358, Loss-topo:95312.3940\n",
      "Epoch:84, P:None, Loss:82.6180, Loss-ae:82.6180, Loss-topo:91665.8549\n",
      "Epoch:85, P:None, Loss:83.6770, Loss-ae:83.6770, Loss-topo:102140.0949\n",
      "Epoch:86, P:None, Loss:83.2521, Loss-ae:83.2521, Loss-topo:90572.5536\n",
      "Epoch:87, P:None, Loss:79.4140, Loss-ae:79.4140, Loss-topo:95785.0781\n",
      "Epoch:88, P:None, Loss:80.2975, Loss-ae:80.2975, Loss-topo:90059.8560\n",
      "Epoch:89, P:None, Loss:83.8640, Loss-ae:83.8640, Loss-topo:97440.6830\n",
      "Epoch:90, P:None, Loss:81.3297, Loss-ae:81.3297, Loss-topo:95731.7031\n",
      "Epoch:91, P:None, Loss:81.3951, Loss-ae:81.3951, Loss-topo:95486.2154\n",
      "Epoch:92, P:None, Loss:80.5459, Loss-ae:80.5459, Loss-topo:97709.3527\n",
      "Epoch:93, P:None, Loss:80.4699, Loss-ae:80.4699, Loss-topo:104214.8984\n",
      "Epoch:94, P:None, Loss:80.6118, Loss-ae:80.6118, Loss-topo:105739.9353\n",
      "Epoch:95, P:None, Loss:82.7157, Loss-ae:82.7157, Loss-topo:98642.6897\n",
      "Epoch:96, P:None, Loss:81.0730, Loss-ae:81.0730, Loss-topo:99279.5837\n",
      "Epoch:97, P:None, Loss:81.1000, Loss-ae:81.1000, Loss-topo:107787.0971\n",
      "Epoch:98, P:None, Loss:79.2434, Loss-ae:79.2434, Loss-topo:104089.0324\n",
      "Epoch:99, P:None, Loss:79.9827, Loss-ae:79.9827, Loss-topo:97274.9018\n",
      "Epoch:100, P:None, Loss:79.1156, Loss-ae:79.1156, Loss-topo:97948.5402\n",
      "Epoch:101, P:None, Loss:79.4121, Loss-ae:79.4121, Loss-topo:102296.4208\n",
      "Epoch:102, P:None, Loss:79.6092, Loss-ae:79.6092, Loss-topo:99509.1105\n",
      "Epoch:103, P:None, Loss:80.6730, Loss-ae:80.6730, Loss-topo:101789.8694\n",
      "Epoch:104, P:None, Loss:78.8564, Loss-ae:78.8564, Loss-topo:95308.2098\n",
      "Epoch:105, P:None, Loss:79.3682, Loss-ae:79.3682, Loss-topo:97341.9598\n",
      "Epoch:106, P:None, Loss:80.4659, Loss-ae:80.4659, Loss-topo:105535.3873\n",
      "Epoch:107, P:None, Loss:79.6097, Loss-ae:79.6097, Loss-topo:104018.8817\n",
      "Epoch:108, P:None, Loss:79.1592, Loss-ae:79.1592, Loss-topo:106372.1172\n",
      "Epoch:109, P:None, Loss:78.7164, Loss-ae:78.7164, Loss-topo:105604.6786\n",
      "Epoch:110, P:None, Loss:84.3192, Loss-ae:84.3192, Loss-topo:119704.3795\n",
      "Epoch:111, P:None, Loss:77.9822, Loss-ae:77.9822, Loss-topo:114076.0000\n",
      "Epoch:112, P:None, Loss:78.0101, Loss-ae:78.0101, Loss-topo:103203.5056\n",
      "Epoch:113, P:None, Loss:77.0067, Loss-ae:77.0067, Loss-topo:105540.0658\n",
      "Epoch:114, P:None, Loss:75.7879, Loss-ae:75.7879, Loss-topo:97707.7723\n",
      "Epoch:115, P:None, Loss:77.3360, Loss-ae:77.3360, Loss-topo:108537.5592\n",
      "Epoch:116, P:None, Loss:79.0433, Loss-ae:79.0433, Loss-topo:105823.2891\n",
      "Epoch:117, P:None, Loss:76.5958, Loss-ae:76.5958, Loss-topo:106544.1998\n",
      "Epoch:118, P:None, Loss:74.8661, Loss-ae:74.8661, Loss-topo:100565.6317\n",
      "Epoch:119, P:None, Loss:78.8343, Loss-ae:78.8343, Loss-topo:100106.0458\n",
      "Epoch:120, P:None, Loss:77.9180, Loss-ae:77.9180, Loss-topo:112251.1395\n",
      "Epoch:121, P:None, Loss:80.4923, Loss-ae:80.4923, Loss-topo:114553.9933\n",
      "Epoch:122, P:None, Loss:79.1548, Loss-ae:79.1548, Loss-topo:103132.5324\n",
      "Epoch:123, P:None, Loss:78.2240, Loss-ae:78.2240, Loss-topo:113497.1150\n",
      "Epoch:124, P:None, Loss:75.6889, Loss-ae:75.6889, Loss-topo:105983.8460\n",
      "Epoch:125, P:None, Loss:77.3474, Loss-ae:77.3474, Loss-topo:108253.1362\n",
      "Epoch:126, P:None, Loss:77.0816, Loss-ae:77.0816, Loss-topo:109369.1830\n",
      "Epoch:127, P:None, Loss:75.6519, Loss-ae:75.6519, Loss-topo:118698.6373\n",
      "Epoch:128, P:None, Loss:74.9128, Loss-ae:74.9128, Loss-topo:109208.1607\n",
      "Epoch:129, P:None, Loss:80.0920, Loss-ae:80.0920, Loss-topo:115515.6194\n",
      "Epoch:130, P:None, Loss:75.6130, Loss-ae:75.6130, Loss-topo:111276.3304\n",
      "Epoch:131, P:None, Loss:75.3311, Loss-ae:75.3311, Loss-topo:106925.5301\n",
      "Epoch:132, P:None, Loss:78.5524, Loss-ae:78.5524, Loss-topo:111017.9453\n",
      "Epoch:133, P:None, Loss:75.5953, Loss-ae:75.5953, Loss-topo:109391.4911\n",
      "Epoch:134, P:None, Loss:74.7936, Loss-ae:74.7936, Loss-topo:110182.2690\n",
      "Epoch:135, P:None, Loss:73.8826, Loss-ae:73.8826, Loss-topo:104022.9029\n",
      "Epoch:136, P:None, Loss:74.4922, Loss-ae:74.4922, Loss-topo:119176.2623\n",
      "Epoch:137, P:None, Loss:75.6998, Loss-ae:75.6998, Loss-topo:110845.5402\n",
      "Epoch:138, P:None, Loss:79.3703, Loss-ae:79.3703, Loss-topo:120054.4040\n",
      "Epoch:139, P:None, Loss:75.6861, Loss-ae:75.6861, Loss-topo:108827.0335\n",
      "Epoch:140, P:None, Loss:74.5910, Loss-ae:74.5910, Loss-topo:119478.4118\n",
      "Epoch:141, P:None, Loss:77.2780, Loss-ae:77.2780, Loss-topo:133379.6295\n",
      "Epoch:142, P:None, Loss:74.0477, Loss-ae:74.0477, Loss-topo:105937.1339\n",
      "Epoch:143, P:None, Loss:77.4905, Loss-ae:77.4905, Loss-topo:123929.2556\n",
      "Epoch:144, P:None, Loss:76.1199, Loss-ae:76.1199, Loss-topo:120226.1518\n",
      "Epoch:145, P:None, Loss:75.9035, Loss-ae:75.9035, Loss-topo:116197.6641\n",
      "Epoch:146, P:None, Loss:74.7886, Loss-ae:74.7886, Loss-topo:117305.6908\n",
      "Epoch:147, P:None, Loss:76.7496, Loss-ae:76.7496, Loss-topo:116919.1641\n",
      "Epoch:148, P:None, Loss:73.8001, Loss-ae:73.8001, Loss-topo:110071.1741\n",
      "Epoch:149, P:None, Loss:79.9957, Loss-ae:79.9957, Loss-topo:126950.8862\n",
      "Epoch:150, P:None, Loss:75.9200, Loss-ae:75.9200, Loss-topo:119751.5714\n",
      "Epoch:151, P:None, Loss:75.6941, Loss-ae:75.6941, Loss-topo:120007.3940\n",
      "Epoch:152, P:None, Loss:72.9216, Loss-ae:72.9216, Loss-topo:110759.7266\n",
      "Epoch:153, P:None, Loss:72.9967, Loss-ae:72.9967, Loss-topo:108343.8136\n",
      "Epoch:154, P:None, Loss:74.3161, Loss-ae:74.3161, Loss-topo:118842.5268\n",
      "Epoch:155, P:None, Loss:74.8942, Loss-ae:74.8942, Loss-topo:114879.8214\n",
      "Epoch:156, P:None, Loss:75.5794, Loss-ae:75.5794, Loss-topo:120860.9710\n",
      "Epoch:157, P:None, Loss:73.1439, Loss-ae:73.1439, Loss-topo:112835.5346\n",
      "Epoch:158, P:None, Loss:73.2048, Loss-ae:73.2048, Loss-topo:110337.5781\n",
      "Epoch:159, P:None, Loss:74.1008, Loss-ae:74.1008, Loss-topo:128988.4955\n",
      "Epoch:160, P:None, Loss:72.9197, Loss-ae:72.9197, Loss-topo:115934.9732\n",
      "Epoch:161, P:None, Loss:74.6365, Loss-ae:74.6365, Loss-topo:117162.8248\n",
      "Epoch:162, P:None, Loss:74.9639, Loss-ae:74.9639, Loss-topo:115489.0625\n",
      "Epoch:163, P:None, Loss:73.9387, Loss-ae:73.9387, Loss-topo:121006.4252\n",
      "Epoch:164, P:None, Loss:73.3931, Loss-ae:73.3931, Loss-topo:120423.9007\n",
      "Epoch:165, P:None, Loss:75.1544, Loss-ae:75.1544, Loss-topo:126521.1551\n",
      "Epoch:166, P:None, Loss:73.9606, Loss-ae:73.9606, Loss-topo:125144.8337\n",
      "Epoch:167, P:None, Loss:72.7488, Loss-ae:72.7488, Loss-topo:118792.9688\n",
      "Epoch:168, P:None, Loss:75.0514, Loss-ae:75.0514, Loss-topo:119052.8131\n",
      "Epoch:169, P:None, Loss:73.9317, Loss-ae:73.9317, Loss-topo:119087.4777\n",
      "Epoch:170, P:None, Loss:73.1661, Loss-ae:73.1661, Loss-topo:112449.7645\n",
      "Epoch:171, P:None, Loss:73.9744, Loss-ae:73.9744, Loss-topo:123498.8326\n",
      "Epoch:172, P:None, Loss:72.8448, Loss-ae:72.8448, Loss-topo:119180.1808\n",
      "Epoch:173, P:None, Loss:74.4745, Loss-ae:74.4745, Loss-topo:122740.9967\n",
      "Epoch:174, P:None, Loss:73.0319, Loss-ae:73.0319, Loss-topo:121000.8560\n",
      "Epoch:175, P:None, Loss:73.5345, Loss-ae:73.5345, Loss-topo:115168.2143\n",
      "Epoch:176, P:None, Loss:74.9738, Loss-ae:74.9738, Loss-topo:127589.3504\n",
      "Epoch:177, P:None, Loss:73.4245, Loss-ae:73.4245, Loss-topo:112807.3482\n",
      "Epoch:178, P:None, Loss:71.2325, Loss-ae:71.2325, Loss-topo:126242.3672\n",
      "Epoch:179, P:None, Loss:70.8401, Loss-ae:70.8401, Loss-topo:120195.0625\n",
      "Epoch:180, P:None, Loss:73.2684, Loss-ae:73.2684, Loss-topo:120784.8839\n",
      "Epoch:181, P:None, Loss:74.7849, Loss-ae:74.7849, Loss-topo:122572.2801\n",
      "Epoch:182, P:None, Loss:74.1568, Loss-ae:74.1568, Loss-topo:137473.0078\n",
      "Epoch:183, P:None, Loss:74.2676, Loss-ae:74.2676, Loss-topo:129153.8471\n",
      "Epoch:184, P:None, Loss:70.3773, Loss-ae:70.3773, Loss-topo:120628.0871\n",
      "Epoch:185, P:None, Loss:71.6067, Loss-ae:71.6067, Loss-topo:118190.8929\n",
      "Epoch:186, P:None, Loss:71.3369, Loss-ae:71.3369, Loss-topo:116647.5134\n",
      "Epoch:187, P:None, Loss:72.4998, Loss-ae:72.4998, Loss-topo:115820.6406\n",
      "Epoch:188, P:None, Loss:73.3081, Loss-ae:73.3081, Loss-topo:124819.0904\n",
      "Epoch:189, P:None, Loss:71.1848, Loss-ae:71.1848, Loss-topo:118845.5257\n",
      "Epoch:190, P:None, Loss:73.6291, Loss-ae:73.6291, Loss-topo:126986.6138\n",
      "Epoch:191, P:None, Loss:72.6407, Loss-ae:72.6407, Loss-topo:110803.9364\n",
      "Epoch:192, P:None, Loss:73.1284, Loss-ae:73.1284, Loss-topo:118474.5435\n",
      "Epoch:193, P:None, Loss:75.3644, Loss-ae:75.3644, Loss-topo:124460.5179\n",
      "Epoch:194, P:None, Loss:70.9903, Loss-ae:70.9903, Loss-topo:122687.7835\n",
      "Epoch:195, P:None, Loss:70.9702, Loss-ae:70.9702, Loss-topo:110533.8192\n",
      "Epoch:196, P:None, Loss:74.9575, Loss-ae:74.9575, Loss-topo:122553.5625\n",
      "Epoch:197, P:None, Loss:71.7197, Loss-ae:71.7197, Loss-topo:124067.3158\n",
      "Epoch:198, P:None, Loss:71.2347, Loss-ae:71.2347, Loss-topo:119188.4699\n",
      "Epoch:199, P:None, Loss:70.6779, Loss-ae:70.6779, Loss-topo:119814.9297\n",
      "Epoch:200, P:None, Loss:71.9500, Loss-ae:71.9500, Loss-topo:116430.4810\n",
      "Epoch:201, P:None, Loss:73.1025, Loss-ae:73.1025, Loss-topo:135640.0670\n",
      "Epoch:202, P:None, Loss:71.2154, Loss-ae:71.2154, Loss-topo:121502.5748\n",
      "Epoch:203, P:None, Loss:73.6833, Loss-ae:73.6833, Loss-topo:125902.0558\n",
      "Epoch:204, P:None, Loss:70.6556, Loss-ae:70.6556, Loss-topo:139211.3415\n",
      "Epoch:205, P:None, Loss:72.7485, Loss-ae:72.7485, Loss-topo:134422.5982\n",
      "Epoch:206, P:None, Loss:71.9536, Loss-ae:71.9536, Loss-topo:124739.5502\n",
      "Epoch:207, P:None, Loss:71.1519, Loss-ae:71.1519, Loss-topo:122518.7746\n",
      "Epoch:208, P:None, Loss:73.0274, Loss-ae:73.0274, Loss-topo:138031.0636\n",
      "Epoch:209, P:None, Loss:71.7069, Loss-ae:71.7069, Loss-topo:133560.3560\n",
      "Epoch:210, P:None, Loss:72.5508, Loss-ae:72.5508, Loss-topo:128618.5413\n",
      "Epoch:211, P:None, Loss:71.4574, Loss-ae:71.4574, Loss-topo:123320.4453\n",
      "Epoch:212, P:None, Loss:73.5902, Loss-ae:73.5902, Loss-topo:127611.9096\n",
      "Epoch:213, P:None, Loss:72.8504, Loss-ae:72.8504, Loss-topo:131402.5781\n",
      "Epoch:214, P:None, Loss:72.3507, Loss-ae:72.3507, Loss-topo:125138.4487\n",
      "Epoch:215, P:None, Loss:71.5829, Loss-ae:71.5829, Loss-topo:130349.2333\n",
      "Epoch:216, P:None, Loss:71.3064, Loss-ae:71.3064, Loss-topo:131562.1161\n",
      "Epoch:217, P:None, Loss:70.1166, Loss-ae:70.1166, Loss-topo:124368.2188\n",
      "Epoch:218, P:None, Loss:70.6121, Loss-ae:70.6121, Loss-topo:118967.9297\n",
      "Epoch:219, P:None, Loss:71.7715, Loss-ae:71.7715, Loss-topo:146049.0971\n",
      "Epoch:220, P:None, Loss:71.0593, Loss-ae:71.0593, Loss-topo:122114.8873\n",
      "Epoch:221, P:None, Loss:73.4719, Loss-ae:73.4719, Loss-topo:132415.3214\n",
      "Epoch:222, P:None, Loss:71.0587, Loss-ae:71.0587, Loss-topo:133021.4531\n",
      "Epoch:223, P:None, Loss:71.0501, Loss-ae:71.0501, Loss-topo:127928.1719\n",
      "Epoch:224, P:None, Loss:72.3304, Loss-ae:72.3304, Loss-topo:127687.3772\n",
      "Epoch:225, P:None, Loss:70.6058, Loss-ae:70.6058, Loss-topo:132953.5558\n",
      "Epoch:226, P:None, Loss:71.2473, Loss-ae:71.2473, Loss-topo:122618.4487\n",
      "Epoch:227, P:None, Loss:68.8176, Loss-ae:68.8176, Loss-topo:120804.0480\n",
      "Epoch:228, P:None, Loss:73.1343, Loss-ae:73.1343, Loss-topo:131269.6362\n",
      "Epoch:229, P:None, Loss:69.5966, Loss-ae:69.5966, Loss-topo:132314.9442\n",
      "Epoch:230, P:None, Loss:75.9182, Loss-ae:75.9182, Loss-topo:141699.0279\n",
      "Epoch:231, P:None, Loss:70.4242, Loss-ae:70.4242, Loss-topo:131542.5145\n",
      "Epoch:232, P:None, Loss:71.5983, Loss-ae:71.5983, Loss-topo:121639.4888\n",
      "Epoch:233, P:None, Loss:70.3891, Loss-ae:70.3891, Loss-topo:144345.3650\n",
      "Epoch:234, P:None, Loss:70.2452, Loss-ae:70.2452, Loss-topo:126460.1540\n",
      "Epoch:235, P:None, Loss:72.1667, Loss-ae:72.1667, Loss-topo:136324.3415\n",
      "Epoch:236, P:None, Loss:69.1730, Loss-ae:69.1730, Loss-topo:133180.4275\n",
      "Epoch:237, P:None, Loss:71.2622, Loss-ae:71.2622, Loss-topo:124222.7656\n",
      "Epoch:238, P:None, Loss:71.2156, Loss-ae:71.2156, Loss-topo:123853.1261\n",
      "Epoch:239, P:None, Loss:70.9124, Loss-ae:70.9124, Loss-topo:122933.8114\n",
      "Epoch:240, P:None, Loss:72.0938, Loss-ae:72.0938, Loss-topo:130788.0379\n",
      "Epoch:241, P:None, Loss:70.1807, Loss-ae:70.1807, Loss-topo:127692.8929\n",
      "Epoch:242, P:None, Loss:71.2324, Loss-ae:71.2324, Loss-topo:127156.8125\n",
      "Epoch:243, P:None, Loss:72.9229, Loss-ae:72.9229, Loss-topo:130236.5938\n",
      "Epoch:244, P:None, Loss:70.4263, Loss-ae:70.4263, Loss-topo:125494.9241\n",
      "Epoch:245, P:None, Loss:69.3381, Loss-ae:69.3381, Loss-topo:119043.8549\n",
      "Epoch:246, P:None, Loss:73.4453, Loss-ae:73.4453, Loss-topo:130610.9040\n",
      "Epoch:247, P:None, Loss:72.0365, Loss-ae:72.0365, Loss-topo:133316.7489\n",
      "Epoch:248, P:None, Loss:73.3474, Loss-ae:73.3474, Loss-topo:135970.1295\n",
      "Epoch:249, P:None, Loss:70.5550, Loss-ae:70.5550, Loss-topo:129786.9386\n",
      "Epoch:250, P:None, Loss:71.5611, Loss-ae:71.5611, Loss-topo:141026.2790\n",
      "Epoch:251, P:None, Loss:70.0216, Loss-ae:70.0216, Loss-topo:121904.8471\n",
      "Epoch:252, P:None, Loss:72.4744, Loss-ae:72.4744, Loss-topo:142496.2433\n",
      "Epoch:253, P:None, Loss:70.7964, Loss-ae:70.7964, Loss-topo:144280.1529\n",
      "Epoch:254, P:None, Loss:70.6072, Loss-ae:70.6072, Loss-topo:134706.9833\n",
      "Epoch:255, P:None, Loss:72.9720, Loss-ae:72.9720, Loss-topo:130762.3650\n",
      "Epoch:256, P:None, Loss:73.7558, Loss-ae:73.7558, Loss-topo:140754.7578\n",
      "Epoch:257, P:None, Loss:69.0239, Loss-ae:69.0239, Loss-topo:132185.8449\n",
      "Epoch:258, P:None, Loss:70.4333, Loss-ae:70.4333, Loss-topo:132109.8092\n",
      "Epoch:259, P:None, Loss:73.7929, Loss-ae:73.7929, Loss-topo:145391.9710\n",
      "Epoch:260, P:None, Loss:69.4299, Loss-ae:69.4299, Loss-topo:118925.7288\n",
      "Epoch:261, P:None, Loss:71.3683, Loss-ae:71.3683, Loss-topo:130440.4911\n",
      "Epoch:262, P:None, Loss:71.4526, Loss-ae:71.4526, Loss-topo:127251.7533\n",
      "Epoch:263, P:None, Loss:70.2632, Loss-ae:70.2632, Loss-topo:124465.0681\n",
      "Epoch:264, P:None, Loss:70.9533, Loss-ae:70.9533, Loss-topo:125420.8058\n",
      "Epoch:265, P:None, Loss:69.8300, Loss-ae:69.8300, Loss-topo:127517.6830\n",
      "Epoch:266, P:None, Loss:69.6386, Loss-ae:69.6386, Loss-topo:123979.4364\n",
      "Epoch:267, P:None, Loss:71.1677, Loss-ae:71.1677, Loss-topo:133859.2667\n",
      "Epoch:268, P:None, Loss:69.8778, Loss-ae:69.8778, Loss-topo:135714.6194\n",
      "Epoch:269, P:None, Loss:71.9030, Loss-ae:71.9030, Loss-topo:136184.4676\n",
      "Epoch:270, P:None, Loss:72.0796, Loss-ae:72.0796, Loss-topo:140492.6574\n",
      "Epoch:271, P:None, Loss:70.8563, Loss-ae:70.8563, Loss-topo:134977.0625\n",
      "Epoch:272, P:None, Loss:69.0377, Loss-ae:69.0377, Loss-topo:127190.4107\n",
      "Epoch:273, P:None, Loss:71.0286, Loss-ae:71.0286, Loss-topo:135864.8962\n",
      "Epoch:274, P:None, Loss:70.9890, Loss-ae:70.9890, Loss-topo:145080.3069\n",
      "Epoch:275, P:None, Loss:68.9588, Loss-ae:68.9588, Loss-topo:137560.6629\n",
      "Epoch:276, P:None, Loss:70.1557, Loss-ae:70.1557, Loss-topo:125896.9978\n",
      "Epoch:277, P:None, Loss:69.0946, Loss-ae:69.0946, Loss-topo:126735.7690\n",
      "Epoch:278, P:None, Loss:70.3169, Loss-ae:70.3169, Loss-topo:131368.7835\n",
      "Epoch:279, P:None, Loss:67.6263, Loss-ae:67.6263, Loss-topo:125448.1663\n",
      "Epoch:280, P:None, Loss:69.3613, Loss-ae:69.3613, Loss-topo:134692.1942\n",
      "Epoch:281, P:None, Loss:70.1989, Loss-ae:70.1989, Loss-topo:117466.3895\n",
      "Epoch:282, P:None, Loss:69.6856, Loss-ae:69.6856, Loss-topo:124501.1830\n",
      "Epoch:283, P:None, Loss:69.1137, Loss-ae:69.1137, Loss-topo:124122.8862\n",
      "Epoch:284, P:None, Loss:71.7888, Loss-ae:71.7888, Loss-topo:145805.2321\n",
      "Epoch:285, P:None, Loss:71.1168, Loss-ae:71.1168, Loss-topo:134625.4062\n",
      "Epoch:286, P:None, Loss:71.6540, Loss-ae:71.6540, Loss-topo:128847.3192\n",
      "Epoch:287, P:None, Loss:68.7429, Loss-ae:68.7429, Loss-topo:124093.0156\n",
      "Epoch:288, P:None, Loss:69.5726, Loss-ae:69.5726, Loss-topo:134668.4609\n",
      "Epoch:289, P:None, Loss:70.3170, Loss-ae:70.3170, Loss-topo:134414.4967\n",
      "Epoch:290, P:None, Loss:70.4748, Loss-ae:70.4748, Loss-topo:130673.9308\n",
      "Epoch:291, P:None, Loss:71.4127, Loss-ae:71.4127, Loss-topo:128669.5915\n",
      "Epoch:292, P:None, Loss:70.6100, Loss-ae:70.6100, Loss-topo:137647.7634\n",
      "Epoch:293, P:None, Loss:71.9650, Loss-ae:71.9650, Loss-topo:132711.7980\n",
      "Epoch:294, P:None, Loss:69.2699, Loss-ae:69.2699, Loss-topo:134547.0815\n",
      "Epoch:295, P:None, Loss:68.5894, Loss-ae:68.5894, Loss-topo:121535.6808\n",
      "Epoch:296, P:None, Loss:72.2065, Loss-ae:72.2065, Loss-topo:153377.1797\n",
      "Epoch:297, P:None, Loss:69.5667, Loss-ae:69.5667, Loss-topo:128307.6741\n",
      "Epoch:298, P:None, Loss:69.0403, Loss-ae:69.0403, Loss-topo:127191.7969\n",
      "Epoch:299, P:None, Loss:70.2652, Loss-ae:70.2652, Loss-topo:140034.3471\n",
      "Epoch:300, P:None, Loss:73.7113, Loss-ae:73.7113, Loss-topo:132087.2790\n",
      "Epoch:301, P:None, Loss:69.7499, Loss-ae:69.7499, Loss-topo:134064.3136\n",
      "Epoch:302, P:None, Loss:69.7866, Loss-ae:69.7866, Loss-topo:133609.8449\n",
      "Epoch:303, P:None, Loss:68.6495, Loss-ae:68.6495, Loss-topo:122927.3114\n",
      "Epoch:304, P:None, Loss:68.2076, Loss-ae:68.2076, Loss-topo:123151.0491\n",
      "Epoch:305, P:None, Loss:72.6931, Loss-ae:72.6931, Loss-topo:130985.9029\n",
      "Epoch:306, P:None, Loss:69.9841, Loss-ae:69.9841, Loss-topo:131925.6395\n",
      "Epoch:307, P:None, Loss:70.1225, Loss-ae:70.1225, Loss-topo:140641.3192\n",
      "Epoch:308, P:None, Loss:72.2719, Loss-ae:72.2719, Loss-topo:131732.7879\n",
      "Epoch:309, P:None, Loss:70.0566, Loss-ae:70.0566, Loss-topo:125167.1473\n",
      "Epoch:310, P:None, Loss:70.0069, Loss-ae:70.0069, Loss-topo:110203.4252\n",
      "Epoch:311, P:None, Loss:67.6165, Loss-ae:67.6165, Loss-topo:122229.3783\n",
      "Epoch:312, P:None, Loss:69.4288, Loss-ae:69.4288, Loss-topo:130969.1496\n",
      "Epoch:313, P:None, Loss:69.6723, Loss-ae:69.6723, Loss-topo:138327.5569\n",
      "Epoch:314, P:None, Loss:70.4354, Loss-ae:70.4354, Loss-topo:128028.8393\n",
      "Epoch:315, P:None, Loss:70.0946, Loss-ae:70.0946, Loss-topo:127676.9888\n",
      "Epoch:316, P:None, Loss:69.1126, Loss-ae:69.1126, Loss-topo:125810.2667\n",
      "Epoch:317, P:None, Loss:69.4805, Loss-ae:69.4805, Loss-topo:131592.9085\n",
      "Epoch:318, P:None, Loss:71.0506, Loss-ae:71.0506, Loss-topo:135537.9598\n",
      "Epoch:319, P:None, Loss:72.7564, Loss-ae:72.7564, Loss-topo:145565.5904\n",
      "Epoch:320, P:None, Loss:69.0170, Loss-ae:69.0170, Loss-topo:120907.6496\n",
      "Epoch:321, P:None, Loss:71.4402, Loss-ae:71.4402, Loss-topo:142484.7377\n",
      "Epoch:322, P:None, Loss:69.7128, Loss-ae:69.7128, Loss-topo:123641.0670\n",
      "Epoch:323, P:None, Loss:69.1985, Loss-ae:69.1985, Loss-topo:136352.3092\n",
      "Epoch:324, P:None, Loss:71.8698, Loss-ae:71.8698, Loss-topo:138269.9174\n",
      "Epoch:325, P:None, Loss:68.3030, Loss-ae:68.3030, Loss-topo:123058.2790\n",
      "Epoch:326, P:None, Loss:71.8667, Loss-ae:71.8667, Loss-topo:135746.4096\n",
      "Epoch:327, P:None, Loss:69.7723, Loss-ae:69.7723, Loss-topo:127274.3203\n",
      "Epoch:328, P:None, Loss:70.9626, Loss-ae:70.9626, Loss-topo:137841.8170\n",
      "Epoch:329, P:None, Loss:69.6685, Loss-ae:69.6685, Loss-topo:132074.0134\n",
      "Epoch:330, P:None, Loss:71.3045, Loss-ae:71.3045, Loss-topo:133387.2746\n",
      "Epoch:331, P:None, Loss:69.2322, Loss-ae:69.2322, Loss-topo:123896.1897\n",
      "Epoch:332, P:None, Loss:68.7654, Loss-ae:68.7654, Loss-topo:128968.2879\n",
      "Epoch:333, P:None, Loss:72.3026, Loss-ae:72.3026, Loss-topo:129173.7589\n",
      "Epoch:334, P:None, Loss:70.7070, Loss-ae:70.7070, Loss-topo:137358.1920\n",
      "Epoch:335, P:None, Loss:69.1276, Loss-ae:69.1276, Loss-topo:127146.6261\n",
      "Epoch:336, P:None, Loss:67.2599, Loss-ae:67.2599, Loss-topo:119291.7065\n",
      "Epoch:337, P:None, Loss:71.2137, Loss-ae:71.2137, Loss-topo:129649.0938\n",
      "Epoch:338, P:None, Loss:70.5900, Loss-ae:70.5900, Loss-topo:131882.2377\n",
      "Epoch:339, P:None, Loss:70.3491, Loss-ae:70.3491, Loss-topo:134833.3415\n",
      "Epoch:340, P:None, Loss:68.7311, Loss-ae:68.7311, Loss-topo:123413.7723\n",
      "Epoch:341, P:None, Loss:70.0821, Loss-ae:70.0821, Loss-topo:139887.1507\n",
      "Epoch:342, P:None, Loss:69.7816, Loss-ae:69.7816, Loss-topo:139121.4085\n",
      "Epoch:343, P:None, Loss:72.1153, Loss-ae:72.1153, Loss-topo:134056.0882\n",
      "Epoch:344, P:None, Loss:69.9372, Loss-ae:69.9372, Loss-topo:136261.6484\n",
      "Epoch:345, P:None, Loss:68.7820, Loss-ae:68.7820, Loss-topo:125258.6350\n",
      "Epoch:346, P:None, Loss:67.2052, Loss-ae:67.2052, Loss-topo:126534.7969\n",
      "Epoch:347, P:None, Loss:68.4391, Loss-ae:68.4391, Loss-topo:126990.8672\n",
      "Epoch:348, P:None, Loss:68.9574, Loss-ae:68.9574, Loss-topo:134547.1786\n",
      "Epoch:349, P:None, Loss:69.5916, Loss-ae:69.5916, Loss-topo:120308.1808\n",
      "Epoch:350, P:None, Loss:70.1266, Loss-ae:70.1266, Loss-topo:128622.1864\n",
      "Epoch:351, P:None, Loss:66.6423, Loss-ae:66.6423, Loss-topo:127720.1429\n",
      "Epoch:352, P:None, Loss:70.2046, Loss-ae:70.2046, Loss-topo:140289.6875\n",
      "Epoch:353, P:None, Loss:69.1011, Loss-ae:69.1011, Loss-topo:136304.8025\n",
      "Epoch:354, P:None, Loss:69.5406, Loss-ae:69.5406, Loss-topo:132667.8348\n",
      "Epoch:355, P:None, Loss:70.3964, Loss-ae:70.3964, Loss-topo:131853.6808\n",
      "Epoch:356, P:None, Loss:70.1084, Loss-ae:70.1084, Loss-topo:126432.1518\n",
      "Epoch:357, P:None, Loss:71.1612, Loss-ae:71.1612, Loss-topo:130770.5926\n",
      "Epoch:358, P:None, Loss:73.3445, Loss-ae:73.3445, Loss-topo:133319.6373\n",
      "Epoch:359, P:None, Loss:68.9369, Loss-ae:68.9369, Loss-topo:128590.6127\n",
      "Epoch:360, P:None, Loss:69.3152, Loss-ae:69.3152, Loss-topo:134751.7958\n",
      "Epoch:361, P:None, Loss:69.1738, Loss-ae:69.1738, Loss-topo:119901.3594\n",
      "Epoch:362, P:None, Loss:68.7313, Loss-ae:68.7313, Loss-topo:130565.5413\n",
      "Epoch:363, P:None, Loss:72.3747, Loss-ae:72.3747, Loss-topo:133245.4431\n",
      "Epoch:364, P:None, Loss:70.6874, Loss-ae:70.6874, Loss-topo:125672.0536\n",
      "Epoch:365, P:None, Loss:68.0884, Loss-ae:68.0884, Loss-topo:123158.4565\n",
      "Epoch:366, P:None, Loss:70.8454, Loss-ae:70.8454, Loss-topo:141889.8025\n",
      "Epoch:367, P:None, Loss:70.8223, Loss-ae:70.8223, Loss-topo:131992.3627\n",
      "Epoch:368, P:None, Loss:67.8471, Loss-ae:67.8471, Loss-topo:122923.0446\n",
      "Epoch:369, P:None, Loss:68.5461, Loss-ae:68.5461, Loss-topo:128416.5592\n",
      "Epoch:370, P:None, Loss:68.2991, Loss-ae:68.2991, Loss-topo:135767.2612\n",
      "Epoch:371, P:None, Loss:67.8275, Loss-ae:67.8275, Loss-topo:120883.5982\n",
      "Epoch:372, P:None, Loss:67.1824, Loss-ae:67.1824, Loss-topo:129618.1295\n",
      "Epoch:373, P:None, Loss:68.6310, Loss-ae:68.6310, Loss-topo:131802.4520\n",
      "Epoch:374, P:None, Loss:68.6903, Loss-ae:68.6903, Loss-topo:126476.3873\n",
      "Epoch:375, P:None, Loss:68.5071, Loss-ae:68.5071, Loss-topo:130460.6239\n",
      "Epoch:376, P:None, Loss:68.4112, Loss-ae:68.4112, Loss-topo:136897.2277\n",
      "Epoch:377, P:None, Loss:69.7007, Loss-ae:69.7007, Loss-topo:134294.2556\n",
      "Epoch:378, P:None, Loss:70.2919, Loss-ae:70.2919, Loss-topo:134838.5926\n",
      "Epoch:379, P:None, Loss:68.2564, Loss-ae:68.2564, Loss-topo:126586.5491\n",
      "Epoch:380, P:None, Loss:69.6378, Loss-ae:69.6378, Loss-topo:137238.9688\n",
      "Epoch:381, P:None, Loss:68.0200, Loss-ae:68.0200, Loss-topo:127621.8973\n",
      "Epoch:382, P:None, Loss:68.9087, Loss-ae:68.9087, Loss-topo:124651.5112\n",
      "Epoch:383, P:None, Loss:66.3589, Loss-ae:66.3589, Loss-topo:117259.3594\n",
      "Epoch:384, P:None, Loss:67.5772, Loss-ae:67.5772, Loss-topo:134091.2712\n",
      "Epoch:385, P:None, Loss:69.1540, Loss-ae:69.1540, Loss-topo:138676.1451\n",
      "Epoch:386, P:None, Loss:67.6554, Loss-ae:67.6554, Loss-topo:127304.8404\n",
      "Epoch:387, P:None, Loss:67.6232, Loss-ae:67.6232, Loss-topo:133584.0848\n",
      "Epoch:388, P:None, Loss:70.4371, Loss-ae:70.4371, Loss-topo:134769.0915\n",
      "Epoch:389, P:None, Loss:70.5861, Loss-ae:70.5861, Loss-topo:132782.8783\n",
      "Epoch:390, P:None, Loss:69.1287, Loss-ae:69.1287, Loss-topo:135451.5324\n",
      "Epoch:391, P:None, Loss:65.7987, Loss-ae:65.7987, Loss-topo:116683.9342\n",
      "Epoch:392, P:None, Loss:69.1855, Loss-ae:69.1855, Loss-topo:124052.3962\n",
      "Epoch:393, P:None, Loss:68.3935, Loss-ae:68.3935, Loss-topo:133416.5022\n",
      "Epoch:394, P:None, Loss:68.5994, Loss-ae:68.5994, Loss-topo:114597.8884\n",
      "Epoch:395, P:None, Loss:71.1893, Loss-ae:71.1893, Loss-topo:127413.6127\n",
      "Epoch:396, P:None, Loss:67.9626, Loss-ae:67.9626, Loss-topo:122607.4442\n",
      "Epoch:397, P:None, Loss:68.9614, Loss-ae:68.9614, Loss-topo:128044.8471\n",
      "Epoch:398, P:None, Loss:71.9318, Loss-ae:71.9318, Loss-topo:135891.6228\n",
      "Epoch:399, P:None, Loss:68.2934, Loss-ae:68.2934, Loss-topo:132364.4107\n",
      "Epoch:400, P:None, Loss:69.9847, Loss-ae:69.9847, Loss-topo:130505.3292\n",
      "Epoch:401, P:None, Loss:68.7715, Loss-ae:68.7715, Loss-topo:127972.1830\n",
      "Epoch:402, P:None, Loss:69.2183, Loss-ae:69.2183, Loss-topo:131687.4554\n",
      "Epoch:403, P:None, Loss:68.5615, Loss-ae:68.5615, Loss-topo:133536.9598\n",
      "Epoch:404, P:None, Loss:69.2751, Loss-ae:69.2751, Loss-topo:123793.0558\n",
      "Epoch:405, P:None, Loss:66.3048, Loss-ae:66.3048, Loss-topo:120364.0067\n",
      "Epoch:406, P:None, Loss:68.7237, Loss-ae:68.7237, Loss-topo:139508.8382\n",
      "Epoch:407, P:None, Loss:66.3083, Loss-ae:66.3083, Loss-topo:122104.9888\n",
      "Epoch:408, P:None, Loss:69.8654, Loss-ae:69.8654, Loss-topo:143906.3739\n",
      "Epoch:409, P:None, Loss:68.1201, Loss-ae:68.1201, Loss-topo:124568.5145\n",
      "Epoch:410, P:None, Loss:67.4558, Loss-ae:67.4558, Loss-topo:117632.7121\n",
      "Epoch:411, P:None, Loss:69.9338, Loss-ae:69.9338, Loss-topo:131955.5569\n",
      "Epoch:412, P:None, Loss:67.6939, Loss-ae:67.6939, Loss-topo:111955.0045\n",
      "Epoch:413, P:None, Loss:70.1556, Loss-ae:70.1556, Loss-topo:128015.1696\n",
      "Epoch:414, P:None, Loss:70.5437, Loss-ae:70.5437, Loss-topo:143560.8627\n",
      "Epoch:415, P:None, Loss:67.5689, Loss-ae:67.5689, Loss-topo:118374.0993\n",
      "Epoch:416, P:None, Loss:68.4952, Loss-ae:68.4952, Loss-topo:126736.9375\n",
      "Epoch:417, P:None, Loss:69.8078, Loss-ae:69.8078, Loss-topo:139419.1886\n",
      "Epoch:418, P:None, Loss:68.0800, Loss-ae:68.0800, Loss-topo:121472.2868\n",
      "Epoch:419, P:None, Loss:68.4523, Loss-ae:68.4523, Loss-topo:123780.8549\n",
      "Epoch:420, P:None, Loss:67.9559, Loss-ae:67.9559, Loss-topo:116826.8002\n",
      "Epoch:421, P:None, Loss:68.4458, Loss-ae:68.4458, Loss-topo:122743.6607\n",
      "Epoch:422, P:None, Loss:69.0884, Loss-ae:69.0884, Loss-topo:131842.3281\n",
      "Epoch:423, P:None, Loss:68.9534, Loss-ae:68.9534, Loss-topo:133972.6875\n",
      "Epoch:424, P:None, Loss:66.1839, Loss-ae:66.1839, Loss-topo:114513.2969\n",
      "Epoch:425, P:None, Loss:70.5157, Loss-ae:70.5157, Loss-topo:141004.5391\n",
      "Epoch:426, P:None, Loss:67.8464, Loss-ae:67.8464, Loss-topo:131619.3984\n",
      "Epoch:427, P:None, Loss:68.0644, Loss-ae:68.0644, Loss-topo:128655.6607\n",
      "Epoch:428, P:None, Loss:70.5081, Loss-ae:70.5081, Loss-topo:130636.7623\n",
      "Epoch:429, P:None, Loss:68.6603, Loss-ae:68.6603, Loss-topo:118374.9743\n",
      "Epoch:430, P:None, Loss:70.4140, Loss-ae:70.4140, Loss-topo:137651.9029\n",
      "Epoch:431, P:None, Loss:69.3367, Loss-ae:69.3367, Loss-topo:133432.9051\n",
      "Epoch:432, P:None, Loss:67.5492, Loss-ae:67.5492, Loss-topo:126523.3750\n",
      "Epoch:433, P:None, Loss:68.1289, Loss-ae:68.1289, Loss-topo:126551.4509\n",
      "Epoch:434, P:None, Loss:69.3618, Loss-ae:69.3618, Loss-topo:130226.2109\n",
      "Epoch:435, P:None, Loss:71.4206, Loss-ae:71.4206, Loss-topo:142727.0681\n",
      "Epoch:436, P:None, Loss:67.7568, Loss-ae:67.7568, Loss-topo:122759.2444\n",
      "Epoch:437, P:None, Loss:69.4058, Loss-ae:69.4058, Loss-topo:127398.6763\n",
      "Epoch:438, P:None, Loss:69.3896, Loss-ae:69.3896, Loss-topo:123122.8248\n",
      "Epoch:439, P:None, Loss:70.5036, Loss-ae:70.5036, Loss-topo:131928.9319\n",
      "Epoch:440, P:None, Loss:69.0257, Loss-ae:69.0257, Loss-topo:123852.3315\n",
      "Epoch:441, P:None, Loss:67.5283, Loss-ae:67.5283, Loss-topo:122524.3795\n",
      "Epoch:442, P:None, Loss:67.9284, Loss-ae:67.9284, Loss-topo:126010.3650\n",
      "Epoch:443, P:None, Loss:69.6378, Loss-ae:69.6378, Loss-topo:144606.2031\n",
      "Epoch:444, P:None, Loss:68.5266, Loss-ae:68.5266, Loss-topo:127333.9319\n",
      "Epoch:445, P:None, Loss:67.8222, Loss-ae:67.8222, Loss-topo:131799.1239\n",
      "Epoch:446, P:None, Loss:67.9866, Loss-ae:67.9866, Loss-topo:125251.2868\n",
      "Epoch:447, P:None, Loss:70.2771, Loss-ae:70.2771, Loss-topo:134933.8415\n",
      "Epoch:448, P:None, Loss:72.5859, Loss-ae:72.5859, Loss-topo:135313.5458\n",
      "Epoch:449, P:None, Loss:71.5926, Loss-ae:71.5926, Loss-topo:134145.4353\n",
      "Epoch:450, P:None, Loss:67.5017, Loss-ae:67.5017, Loss-topo:125605.6451\n",
      "Epoch:451, P:None, Loss:69.7826, Loss-ae:69.7826, Loss-topo:125330.1440\n",
      "Epoch:452, P:None, Loss:68.6734, Loss-ae:68.6734, Loss-topo:129638.1641\n",
      "Epoch:453, P:None, Loss:70.3304, Loss-ae:70.3304, Loss-topo:133114.7879\n",
      "Epoch:454, P:None, Loss:66.6222, Loss-ae:66.6222, Loss-topo:114751.0737\n",
      "Epoch:455, P:None, Loss:71.0549, Loss-ae:71.0549, Loss-topo:120208.1596\n",
      "Epoch:456, P:None, Loss:68.4592, Loss-ae:68.4592, Loss-topo:127170.5324\n",
      "Epoch:457, P:None, Loss:69.9781, Loss-ae:69.9781, Loss-topo:129528.7121\n",
      "Epoch:458, P:None, Loss:68.0164, Loss-ae:68.0164, Loss-topo:122783.8504\n",
      "Epoch:459, P:None, Loss:66.6047, Loss-ae:66.6047, Loss-topo:121583.9732\n",
      "Epoch:460, P:None, Loss:67.8132, Loss-ae:67.8132, Loss-topo:136622.4989\n",
      "Epoch:461, P:None, Loss:66.4688, Loss-ae:66.4688, Loss-topo:120437.5234\n",
      "Epoch:462, P:None, Loss:70.5809, Loss-ae:70.5809, Loss-topo:135576.8672\n",
      "Epoch:463, P:None, Loss:69.6316, Loss-ae:69.6316, Loss-topo:136771.3203\n",
      "Epoch:464, P:None, Loss:68.0220, Loss-ae:68.0220, Loss-topo:115776.4308\n",
      "Epoch:465, P:None, Loss:68.0419, Loss-ae:68.0419, Loss-topo:122946.0212\n",
      "Epoch:466, P:None, Loss:65.9240, Loss-ae:65.9240, Loss-topo:107336.2076\n",
      "Epoch:467, P:None, Loss:69.2771, Loss-ae:69.2771, Loss-topo:132826.9989\n",
      "Epoch:468, P:None, Loss:69.9950, Loss-ae:69.9950, Loss-topo:123742.0725\n",
      "Epoch:469, P:None, Loss:65.2139, Loss-ae:65.2139, Loss-topo:117686.2199\n",
      "Epoch:470, P:None, Loss:66.1876, Loss-ae:66.1876, Loss-topo:122432.0915\n",
      "Epoch:471, P:None, Loss:68.7343, Loss-ae:68.7343, Loss-topo:124200.4554\n",
      "Epoch:472, P:None, Loss:68.6200, Loss-ae:68.6200, Loss-topo:120495.4665\n",
      "Epoch:473, P:None, Loss:69.3335, Loss-ae:69.3335, Loss-topo:137216.5502\n",
      "Epoch:474, P:None, Loss:68.5889, Loss-ae:68.5889, Loss-topo:120500.2991\n",
      "Epoch:475, P:None, Loss:69.4966, Loss-ae:69.4966, Loss-topo:137107.8359\n",
      "Epoch:476, P:None, Loss:69.1933, Loss-ae:69.1933, Loss-topo:137520.7545\n",
      "Epoch:477, P:None, Loss:70.7233, Loss-ae:70.7233, Loss-topo:125729.6272\n",
      "Epoch:478, P:None, Loss:70.4060, Loss-ae:70.4060, Loss-topo:134402.7377\n",
      "Epoch:479, P:None, Loss:67.5836, Loss-ae:67.5836, Loss-topo:122126.5402\n",
      "Epoch:480, P:None, Loss:68.3355, Loss-ae:68.3355, Loss-topo:125339.4866\n",
      "Epoch:481, P:None, Loss:67.4725, Loss-ae:67.4725, Loss-topo:120607.2857\n",
      "Epoch:482, P:None, Loss:67.1818, Loss-ae:67.1818, Loss-topo:129783.0324\n",
      "Epoch:483, P:None, Loss:66.9679, Loss-ae:66.9679, Loss-topo:130145.8359\n",
      "Epoch:484, P:None, Loss:70.2761, Loss-ae:70.2761, Loss-topo:137090.6384\n",
      "Epoch:485, P:None, Loss:69.0078, Loss-ae:69.0078, Loss-topo:123606.5257\n",
      "Epoch:486, P:None, Loss:68.4545, Loss-ae:68.4545, Loss-topo:125066.1339\n",
      "Epoch:487, P:None, Loss:67.8387, Loss-ae:67.8387, Loss-topo:125413.2266\n",
      "Epoch:488, P:None, Loss:66.9070, Loss-ae:66.9070, Loss-topo:122316.3025\n",
      "Epoch:489, P:None, Loss:68.0694, Loss-ae:68.0694, Loss-topo:141056.9051\n",
      "Epoch:490, P:None, Loss:69.4332, Loss-ae:69.4332, Loss-topo:124276.0703\n",
      "Epoch:491, P:None, Loss:68.2591, Loss-ae:68.2591, Loss-topo:134582.3493\n",
      "Epoch:492, P:None, Loss:66.5754, Loss-ae:66.5754, Loss-topo:125494.1038\n",
      "Epoch:493, P:None, Loss:67.7854, Loss-ae:67.7854, Loss-topo:117185.3315\n",
      "Epoch:494, P:None, Loss:68.4425, Loss-ae:68.4425, Loss-topo:122007.9609\n",
      "Epoch:495, P:None, Loss:67.0047, Loss-ae:67.0047, Loss-topo:118907.6016\n",
      "Epoch:496, P:None, Loss:69.0936, Loss-ae:69.0936, Loss-topo:129012.9074\n",
      "Epoch:497, P:None, Loss:67.2745, Loss-ae:67.2745, Loss-topo:119024.5893\n",
      "Epoch:498, P:None, Loss:68.9035, Loss-ae:68.9035, Loss-topo:123731.3315\n",
      "Epoch:499, P:None, Loss:71.3618, Loss-ae:71.3618, Loss-topo:118324.5938\n",
      "Epoch:500, P:None, Loss:67.4404, Loss-ae:67.4404, Loss-topo:115553.7924\n",
      "Epoch:501, P:None, Loss:67.6664, Loss-ae:67.6664, Loss-topo:121561.4732\n",
      "Epoch:502, P:None, Loss:71.6300, Loss-ae:71.6300, Loss-topo:142855.4185\n",
      "Epoch:503, P:None, Loss:67.6652, Loss-ae:67.6652, Loss-topo:129503.9944\n",
      "Epoch:504, P:None, Loss:68.9666, Loss-ae:68.9666, Loss-topo:127483.4743\n",
      "Epoch:505, P:None, Loss:69.1967, Loss-ae:69.1967, Loss-topo:128579.3292\n",
      "Epoch:506, P:None, Loss:69.6910, Loss-ae:69.6910, Loss-topo:129182.6183\n",
      "Epoch:507, P:None, Loss:65.9495, Loss-ae:65.9495, Loss-topo:122283.3225\n",
      "Epoch:508, P:None, Loss:67.7561, Loss-ae:67.7561, Loss-topo:133475.9342\n",
      "Epoch:509, P:None, Loss:68.3353, Loss-ae:68.3353, Loss-topo:122872.5458\n",
      "Epoch:510, P:None, Loss:67.0271, Loss-ae:67.0271, Loss-topo:123683.1529\n",
      "Epoch:511, P:None, Loss:68.5988, Loss-ae:68.5988, Loss-topo:126124.3638\n",
      "Epoch:512, P:None, Loss:68.9472, Loss-ae:68.9472, Loss-topo:126898.0357\n",
      "Epoch:513, P:None, Loss:69.9407, Loss-ae:69.9407, Loss-topo:120628.7388\n",
      "Epoch:514, P:None, Loss:68.8520, Loss-ae:68.8520, Loss-topo:131264.9855\n",
      "Epoch:515, P:None, Loss:68.9003, Loss-ae:68.9003, Loss-topo:119701.9275\n",
      "Epoch:516, P:None, Loss:68.0597, Loss-ae:68.0597, Loss-topo:125536.4676\n",
      "Epoch:517, P:None, Loss:67.5483, Loss-ae:67.5483, Loss-topo:117713.9487\n",
      "Epoch:518, P:None, Loss:66.3552, Loss-ae:66.3552, Loss-topo:120691.6283\n",
      "Epoch:519, P:None, Loss:70.9865, Loss-ae:70.9865, Loss-topo:129421.4654\n",
      "Epoch:520, P:None, Loss:68.3271, Loss-ae:68.3271, Loss-topo:130149.0491\n",
      "Epoch:521, P:None, Loss:67.0728, Loss-ae:67.0728, Loss-topo:116021.5346\n",
      "Epoch:522, P:None, Loss:67.9639, Loss-ae:67.9639, Loss-topo:126731.7913\n",
      "Epoch:523, P:None, Loss:67.2147, Loss-ae:67.2147, Loss-topo:128647.5491\n",
      "Epoch:524, P:None, Loss:66.9735, Loss-ae:66.9735, Loss-topo:117821.6272\n",
      "Epoch:525, P:None, Loss:66.7401, Loss-ae:66.7401, Loss-topo:122585.8549\n",
      "Epoch:526, P:None, Loss:66.1651, Loss-ae:66.1651, Loss-topo:116643.2400\n",
      "Epoch:527, P:None, Loss:69.9962, Loss-ae:69.9962, Loss-topo:129066.2958\n",
      "Epoch:528, P:None, Loss:67.2196, Loss-ae:67.2196, Loss-topo:119302.2221\n",
      "Epoch:529, P:None, Loss:69.3761, Loss-ae:69.3761, Loss-topo:134745.8895\n",
      "Epoch:530, P:None, Loss:67.2907, Loss-ae:67.2907, Loss-topo:123113.0893\n",
      "Epoch:531, P:None, Loss:67.7398, Loss-ae:67.7398, Loss-topo:121453.2366\n",
      "Epoch:532, P:None, Loss:67.0042, Loss-ae:67.0042, Loss-topo:123449.6217\n",
      "Epoch:533, P:None, Loss:65.5640, Loss-ae:65.5640, Loss-topo:120649.7355\n",
      "Epoch:534, P:None, Loss:68.1257, Loss-ae:68.1257, Loss-topo:123464.2020\n",
      "Epoch:535, P:None, Loss:68.0363, Loss-ae:68.0363, Loss-topo:118752.2600\n",
      "Epoch:536, P:None, Loss:67.6015, Loss-ae:67.6015, Loss-topo:128631.3616\n",
      "Epoch:537, P:None, Loss:66.7409, Loss-ae:66.7409, Loss-topo:122053.6451\n",
      "Epoch:538, P:None, Loss:68.0612, Loss-ae:68.0612, Loss-topo:124371.4040\n",
      "Epoch:539, P:None, Loss:67.9825, Loss-ae:67.9825, Loss-topo:130283.8002\n",
      "Epoch:540, P:None, Loss:67.8227, Loss-ae:67.8227, Loss-topo:128192.0279\n",
      "Epoch:541, P:None, Loss:67.6242, Loss-ae:67.6242, Loss-topo:117551.7444\n",
      "Epoch:542, P:None, Loss:66.4211, Loss-ae:66.4211, Loss-topo:121781.6317\n",
      "Epoch:543, P:None, Loss:68.3241, Loss-ae:68.3241, Loss-topo:142460.8761\n",
      "Epoch:544, P:None, Loss:70.7977, Loss-ae:70.7977, Loss-topo:128427.5792\n",
      "Epoch:545, P:None, Loss:67.8005, Loss-ae:67.8005, Loss-topo:124625.1205\n",
      "Epoch:546, P:None, Loss:65.5883, Loss-ae:65.5883, Loss-topo:109713.7734\n",
      "Epoch:547, P:None, Loss:69.3081, Loss-ae:69.3081, Loss-topo:117152.6150\n",
      "Epoch:548, P:None, Loss:67.7222, Loss-ae:67.7222, Loss-topo:122963.9375\n",
      "Epoch:549, P:None, Loss:67.6639, Loss-ae:67.6639, Loss-topo:117206.3627\n",
      "Epoch:550, P:None, Loss:68.5198, Loss-ae:68.5198, Loss-topo:114846.5357\n",
      "Epoch:551, P:None, Loss:68.2325, Loss-ae:68.2325, Loss-topo:120436.5033\n",
      "Epoch:552, P:None, Loss:68.8916, Loss-ae:68.8916, Loss-topo:129181.0123\n",
      "Epoch:553, P:None, Loss:68.6003, Loss-ae:68.6003, Loss-topo:123752.6272\n",
      "Epoch:554, P:None, Loss:68.6618, Loss-ae:68.6618, Loss-topo:122323.1350\n",
      "Epoch:555, P:None, Loss:70.5961, Loss-ae:70.5961, Loss-topo:132891.6228\n",
      "Epoch:556, P:None, Loss:66.9856, Loss-ae:66.9856, Loss-topo:115136.8940\n",
      "Epoch:557, P:None, Loss:65.7764, Loss-ae:65.7764, Loss-topo:119071.5167\n",
      "Epoch:558, P:None, Loss:67.2235, Loss-ae:67.2235, Loss-topo:109628.5190\n",
      "Epoch:559, P:None, Loss:68.2093, Loss-ae:68.2093, Loss-topo:120867.5748\n",
      "Epoch:560, P:None, Loss:67.7552, Loss-ae:67.7552, Loss-topo:113627.1172\n",
      "Epoch:561, P:None, Loss:68.5391, Loss-ae:68.5391, Loss-topo:116085.6362\n",
      "Epoch:562, P:None, Loss:67.9180, Loss-ae:67.9180, Loss-topo:125421.5848\n",
      "Epoch:563, P:None, Loss:67.0283, Loss-ae:67.0283, Loss-topo:114196.5279\n",
      "Epoch:564, P:None, Loss:68.5090, Loss-ae:68.5090, Loss-topo:120909.3895\n",
      "Epoch:565, P:None, Loss:67.1338, Loss-ae:67.1338, Loss-topo:115488.7746\n",
      "Epoch:566, P:None, Loss:69.6090, Loss-ae:69.6090, Loss-topo:119852.5391\n",
      "Epoch:567, P:None, Loss:68.4811, Loss-ae:68.4811, Loss-topo:128577.0379\n",
      "Epoch:568, P:None, Loss:67.6480, Loss-ae:67.6480, Loss-topo:112943.9252\n",
      "Epoch:569, P:None, Loss:68.6178, Loss-ae:68.6178, Loss-topo:119294.2366\n",
      "Epoch:570, P:None, Loss:67.6042, Loss-ae:67.6042, Loss-topo:111313.1016\n",
      "Epoch:571, P:None, Loss:70.9606, Loss-ae:70.9606, Loss-topo:132489.7411\n",
      "Epoch:572, P:None, Loss:67.0891, Loss-ae:67.0891, Loss-topo:108868.6752\n",
      "Epoch:573, P:None, Loss:69.4629, Loss-ae:69.4629, Loss-topo:119295.0402\n",
      "Epoch:574, P:None, Loss:66.2498, Loss-ae:66.2498, Loss-topo:115467.7746\n",
      "Epoch:575, P:None, Loss:66.9183, Loss-ae:66.9183, Loss-topo:119267.5938\n",
      "Epoch:576, P:None, Loss:69.6622, Loss-ae:69.6622, Loss-topo:118248.5759\n",
      "Epoch:577, P:None, Loss:66.0960, Loss-ae:66.0960, Loss-topo:112412.2612\n",
      "Epoch:578, P:None, Loss:67.9650, Loss-ae:67.9650, Loss-topo:133514.9866\n",
      "Epoch:579, P:None, Loss:66.8904, Loss-ae:66.8904, Loss-topo:125936.1562\n",
      "Epoch:580, P:None, Loss:68.4324, Loss-ae:68.4324, Loss-topo:114162.3929\n",
      "Epoch:581, P:None, Loss:70.6361, Loss-ae:70.6361, Loss-topo:140083.0335\n",
      "Epoch:582, P:None, Loss:67.0499, Loss-ae:67.0499, Loss-topo:118191.9743\n",
      "Epoch:583, P:None, Loss:67.6692, Loss-ae:67.6692, Loss-topo:123485.4442\n",
      "Epoch:584, P:None, Loss:67.6699, Loss-ae:67.6699, Loss-topo:117063.4967\n",
      "Epoch:585, P:None, Loss:68.1510, Loss-ae:68.1510, Loss-topo:119517.5324\n",
      "Epoch:586, P:None, Loss:69.3279, Loss-ae:69.3279, Loss-topo:118390.8025\n",
      "Epoch:587, P:None, Loss:69.2775, Loss-ae:69.2775, Loss-topo:129306.0781\n",
      "Epoch:588, P:None, Loss:66.0042, Loss-ae:66.0042, Loss-topo:121374.9297\n",
      "Epoch:589, P:None, Loss:68.8533, Loss-ae:68.8533, Loss-topo:119718.7098\n",
      "Epoch:590, P:None, Loss:68.3865, Loss-ae:68.3865, Loss-topo:118791.1629\n",
      "Epoch:591, P:None, Loss:66.9997, Loss-ae:66.9997, Loss-topo:120096.7612\n",
      "Epoch:592, P:None, Loss:67.0154, Loss-ae:67.0154, Loss-topo:121634.3783\n",
      "Epoch:593, P:None, Loss:69.7882, Loss-ae:69.7882, Loss-topo:121360.6116\n",
      "Epoch:594, P:None, Loss:66.3994, Loss-ae:66.3994, Loss-topo:121669.4475\n",
      "Epoch:595, P:None, Loss:68.1303, Loss-ae:68.1303, Loss-topo:120291.2500\n",
      "Epoch:596, P:None, Loss:68.8368, Loss-ae:68.8368, Loss-topo:122065.1105\n",
      "Epoch:597, P:None, Loss:66.9720, Loss-ae:66.9720, Loss-topo:122205.1440\n",
      "Epoch:598, P:None, Loss:67.6747, Loss-ae:67.6747, Loss-topo:120586.4018\n",
      "Epoch:599, P:None, Loss:67.8252, Loss-ae:67.8252, Loss-topo:115536.7790\n",
      "Epoch:600, P:None, Loss:67.6082, Loss-ae:67.6082, Loss-topo:109519.3371\n",
      "Epoch:601, P:None, Loss:65.7132, Loss-ae:65.7132, Loss-topo:113358.8080\n",
      "Epoch:602, P:None, Loss:66.5157, Loss-ae:66.5157, Loss-topo:118754.6261\n",
      "Epoch:603, P:None, Loss:66.8289, Loss-ae:66.8289, Loss-topo:130127.1473\n",
      "Epoch:604, P:None, Loss:66.9634, Loss-ae:66.9634, Loss-topo:118188.4777\n",
      "Epoch:605, P:None, Loss:69.4691, Loss-ae:69.4691, Loss-topo:121441.5424\n",
      "Epoch:606, P:None, Loss:67.1821, Loss-ae:67.1821, Loss-topo:107940.4051\n",
      "Epoch:607, P:None, Loss:67.6434, Loss-ae:67.6434, Loss-topo:122579.0580\n",
      "Epoch:608, P:None, Loss:67.2889, Loss-ae:67.2889, Loss-topo:115572.6239\n",
      "Epoch:609, P:None, Loss:68.7448, Loss-ae:68.7448, Loss-topo:125240.9185\n",
      "Epoch:610, P:None, Loss:68.7198, Loss-ae:68.7198, Loss-topo:118721.6138\n",
      "Epoch:611, P:None, Loss:67.4619, Loss-ae:67.4619, Loss-topo:112810.4665\n",
      "Epoch:612, P:None, Loss:68.1943, Loss-ae:68.1943, Loss-topo:118289.5614\n",
      "Epoch:613, P:None, Loss:68.3172, Loss-ae:68.3172, Loss-topo:124589.8170\n",
      "Epoch:614, P:None, Loss:67.7849, Loss-ae:67.7849, Loss-topo:110505.1027\n",
      "Epoch:615, P:None, Loss:66.8598, Loss-ae:66.8598, Loss-topo:109379.9621\n",
      "Epoch:616, P:None, Loss:67.2657, Loss-ae:67.2657, Loss-topo:109740.7277\n",
      "Epoch:617, P:None, Loss:67.1217, Loss-ae:67.1217, Loss-topo:114048.3962\n",
      "Epoch:618, P:None, Loss:67.5200, Loss-ae:67.5200, Loss-topo:114585.2511\n",
      "Epoch:619, P:None, Loss:67.1333, Loss-ae:67.1333, Loss-topo:123843.0893\n",
      "Epoch:620, P:None, Loss:67.1278, Loss-ae:67.1278, Loss-topo:114344.8929\n",
      "Epoch:621, P:None, Loss:65.9477, Loss-ae:65.9477, Loss-topo:112385.4029\n",
      "Epoch:622, P:None, Loss:67.5112, Loss-ae:67.5112, Loss-topo:127045.0379\n",
      "Epoch:623, P:None, Loss:69.1116, Loss-ae:69.1116, Loss-topo:106226.3683\n",
      "Epoch:624, P:None, Loss:64.5363, Loss-ae:64.5363, Loss-topo:109793.0257\n",
      "Epoch:625, P:None, Loss:67.4283, Loss-ae:67.4283, Loss-topo:120503.7667\n",
      "Epoch:626, P:None, Loss:69.3611, Loss-ae:69.3611, Loss-topo:112573.1752\n",
      "Epoch:627, P:None, Loss:67.6098, Loss-ae:67.6098, Loss-topo:121601.8984\n",
      "Epoch:628, P:None, Loss:68.1262, Loss-ae:68.1262, Loss-topo:112654.1462\n",
      "Epoch:629, P:None, Loss:67.7075, Loss-ae:67.7075, Loss-topo:109473.7199\n",
      "Epoch:630, P:None, Loss:70.2843, Loss-ae:70.2843, Loss-topo:122928.9911\n",
      "Epoch:631, P:None, Loss:68.4257, Loss-ae:68.4257, Loss-topo:124063.3650\n",
      "Epoch:632, P:None, Loss:67.8635, Loss-ae:67.8635, Loss-topo:110556.6071\n",
      "Epoch:633, P:None, Loss:68.8245, Loss-ae:68.8245, Loss-topo:117201.3069\n",
      "Epoch:634, P:None, Loss:66.0496, Loss-ae:66.0496, Loss-topo:107331.1306\n",
      "Epoch:635, P:None, Loss:66.5229, Loss-ae:66.5229, Loss-topo:114676.1440\n",
      "Epoch:636, P:None, Loss:67.0511, Loss-ae:67.0511, Loss-topo:116293.3996\n",
      "Epoch:637, P:None, Loss:69.0859, Loss-ae:69.0859, Loss-topo:126528.3181\n",
      "Epoch:638, P:None, Loss:69.0986, Loss-ae:69.0986, Loss-topo:118060.5246\n",
      "Epoch:639, P:None, Loss:68.6986, Loss-ae:68.6986, Loss-topo:117749.7891\n",
      "Epoch:640, P:None, Loss:68.5165, Loss-ae:68.5165, Loss-topo:127939.4096\n",
      "Epoch:641, P:None, Loss:68.4586, Loss-ae:68.4586, Loss-topo:125886.6897\n",
      "Epoch:642, P:None, Loss:68.4136, Loss-ae:68.4136, Loss-topo:119443.6496\n",
      "Epoch:643, P:None, Loss:66.5435, Loss-ae:66.5435, Loss-topo:115795.0402\n",
      "Epoch:644, P:None, Loss:67.6263, Loss-ae:67.6263, Loss-topo:114343.9107\n",
      "Epoch:645, P:None, Loss:65.6007, Loss-ae:65.6007, Loss-topo:96805.2935\n",
      "Epoch:646, P:None, Loss:69.6471, Loss-ae:69.6471, Loss-topo:116025.2076\n",
      "Epoch:647, P:None, Loss:67.5216, Loss-ae:67.5216, Loss-topo:121624.0223\n",
      "Epoch:648, P:None, Loss:67.0071, Loss-ae:67.0071, Loss-topo:108953.6663\n",
      "Epoch:649, P:None, Loss:66.8359, Loss-ae:66.8359, Loss-topo:109497.5826\n",
      "Epoch:650, P:None, Loss:67.4827, Loss-ae:67.4827, Loss-topo:110824.2846\n",
      "Epoch:651, P:None, Loss:67.8096, Loss-ae:67.8096, Loss-topo:117822.8817\n",
      "Epoch:652, P:None, Loss:68.2417, Loss-ae:68.2417, Loss-topo:123212.0480\n",
      "Epoch:653, P:None, Loss:68.1719, Loss-ae:68.1719, Loss-topo:116808.0435\n",
      "Epoch:654, P:None, Loss:67.2414, Loss-ae:67.2414, Loss-topo:108694.0871\n",
      "Epoch:655, P:None, Loss:67.4986, Loss-ae:67.4986, Loss-topo:109962.6897\n",
      "Epoch:656, P:None, Loss:66.0421, Loss-ae:66.0421, Loss-topo:111195.3270\n",
      "Epoch:657, P:None, Loss:69.1246, Loss-ae:69.1246, Loss-topo:110745.2902\n",
      "Epoch:658, P:None, Loss:66.1227, Loss-ae:66.1227, Loss-topo:104817.9408\n",
      "Epoch:659, P:None, Loss:67.0832, Loss-ae:67.0832, Loss-topo:113263.3783\n",
      "Epoch:660, P:None, Loss:67.4435, Loss-ae:67.4435, Loss-topo:110912.7467\n",
      "Epoch:661, P:None, Loss:68.6114, Loss-ae:68.6114, Loss-topo:116883.9676\n",
      "Epoch:662, P:None, Loss:66.7261, Loss-ae:66.7261, Loss-topo:113441.7165\n",
      "Epoch:663, P:None, Loss:68.0631, Loss-ae:68.0631, Loss-topo:109455.7679\n",
      "Epoch:664, P:None, Loss:69.1388, Loss-ae:69.1388, Loss-topo:120356.6317\n",
      "Epoch:665, P:None, Loss:66.0787, Loss-ae:66.0787, Loss-topo:107790.9643\n",
      "Epoch:666, P:None, Loss:68.9065, Loss-ae:68.9065, Loss-topo:113400.8493\n",
      "Epoch:667, P:None, Loss:66.8102, Loss-ae:66.8102, Loss-topo:110657.1406\n",
      "Epoch:668, P:None, Loss:66.1826, Loss-ae:66.1826, Loss-topo:113011.9319\n",
      "Epoch:669, P:None, Loss:69.7776, Loss-ae:69.7776, Loss-topo:115887.5525\n",
      "Epoch:670, P:None, Loss:68.5171, Loss-ae:68.5171, Loss-topo:118181.4688\n",
      "Epoch:671, P:None, Loss:67.9395, Loss-ae:67.9395, Loss-topo:117348.4275\n",
      "Epoch:672, P:None, Loss:67.4592, Loss-ae:67.4592, Loss-topo:119772.0859\n",
      "Epoch:673, P:None, Loss:67.0953, Loss-ae:67.0953, Loss-topo:111018.6998\n",
      "Epoch:674, P:None, Loss:67.7252, Loss-ae:67.7252, Loss-topo:112827.4498\n",
      "Epoch:675, P:None, Loss:69.5273, Loss-ae:69.5273, Loss-topo:112513.3013\n",
      "Epoch:676, P:None, Loss:65.9198, Loss-ae:65.9198, Loss-topo:111432.9699\n",
      "Epoch:677, P:None, Loss:69.2866, Loss-ae:69.2866, Loss-topo:129208.7467\n",
      "Epoch:678, P:None, Loss:66.4867, Loss-ae:66.4867, Loss-topo:109617.3817\n",
      "Epoch:679, P:None, Loss:67.8547, Loss-ae:67.8547, Loss-topo:114372.0357\n",
      "Epoch:680, P:None, Loss:65.3473, Loss-ae:65.3473, Loss-topo:115096.1071\n",
      "Epoch:681, P:None, Loss:69.7731, Loss-ae:69.7731, Loss-topo:112431.6730\n",
      "Epoch:682, P:None, Loss:67.3746, Loss-ae:67.3746, Loss-topo:108756.9777\n",
      "Epoch:683, P:None, Loss:67.2575, Loss-ae:67.2575, Loss-topo:116264.9799\n",
      "Epoch:684, P:None, Loss:67.1461, Loss-ae:67.1461, Loss-topo:111787.2857\n",
      "Epoch:685, P:None, Loss:68.6640, Loss-ae:68.6640, Loss-topo:115704.9074\n",
      "Epoch:686, P:None, Loss:68.3467, Loss-ae:68.3467, Loss-topo:121532.3136\n",
      "Epoch:687, P:None, Loss:66.7483, Loss-ae:66.7483, Loss-topo:113714.7310\n",
      "Epoch:688, P:None, Loss:66.7196, Loss-ae:66.7196, Loss-topo:106042.2277\n",
      "Epoch:689, P:None, Loss:68.1449, Loss-ae:68.1449, Loss-topo:117489.2645\n",
      "Epoch:690, P:None, Loss:69.5510, Loss-ae:69.5510, Loss-topo:114952.2589\n",
      "Epoch:691, P:None, Loss:68.3642, Loss-ae:68.3642, Loss-topo:119535.5636\n",
      "Epoch:692, P:None, Loss:67.8037, Loss-ae:67.8037, Loss-topo:109379.7824\n",
      "Epoch:693, P:None, Loss:66.6066, Loss-ae:66.6066, Loss-topo:112149.1194\n",
      "Epoch:694, P:None, Loss:66.3127, Loss-ae:66.3127, Loss-topo:102974.2444\n",
      "Epoch:695, P:None, Loss:68.7870, Loss-ae:68.7870, Loss-topo:123410.7299\n",
      "Epoch:696, P:None, Loss:68.5797, Loss-ae:68.5797, Loss-topo:122894.3047\n",
      "Epoch:697, P:None, Loss:67.9558, Loss-ae:67.9558, Loss-topo:107974.4922\n",
      "Epoch:698, P:None, Loss:68.0493, Loss-ae:68.0493, Loss-topo:113999.7522\n",
      "Epoch:699, P:None, Loss:70.5451, Loss-ae:70.5451, Loss-topo:115587.2377\n",
      "Epoch:700, P:None, Loss:68.0726, Loss-ae:68.0726, Loss-topo:109234.9587\n",
      "Epoch:701, P:None, Loss:67.1734, Loss-ae:67.1734, Loss-topo:107393.4498\n",
      "Epoch:702, P:None, Loss:67.5918, Loss-ae:67.5918, Loss-topo:106492.1808\n",
      "Epoch:703, P:None, Loss:65.3229, Loss-ae:65.3229, Loss-topo:109730.0781\n",
      "Epoch:704, P:None, Loss:68.5469, Loss-ae:68.5469, Loss-topo:115509.1562\n",
      "Epoch:705, P:None, Loss:67.0032, Loss-ae:67.0032, Loss-topo:110289.5625\n",
      "Epoch:706, P:None, Loss:67.6985, Loss-ae:67.6985, Loss-topo:108364.6730\n",
      "Epoch:707, P:None, Loss:67.2621, Loss-ae:67.2621, Loss-topo:111289.9420\n",
      "Epoch:708, P:None, Loss:66.2731, Loss-ae:66.2731, Loss-topo:109441.1775\n",
      "Epoch:709, P:None, Loss:67.9885, Loss-ae:67.9885, Loss-topo:106301.5179\n",
      "Epoch:710, P:None, Loss:68.6005, Loss-ae:68.6005, Loss-topo:99699.5301\n",
      "Epoch:711, P:None, Loss:67.5962, Loss-ae:67.5962, Loss-topo:121387.7065\n",
      "Epoch:712, P:None, Loss:68.4683, Loss-ae:68.4683, Loss-topo:116242.5078\n",
      "Epoch:713, P:None, Loss:70.0802, Loss-ae:70.0802, Loss-topo:107568.0045\n",
      "Epoch:714, P:None, Loss:68.7069, Loss-ae:68.7069, Loss-topo:121909.9163\n",
      "Epoch:715, P:None, Loss:69.1048, Loss-ae:69.1048, Loss-topo:117844.2746\n",
      "Epoch:716, P:None, Loss:66.7368, Loss-ae:66.7368, Loss-topo:106638.8136\n",
      "Epoch:717, P:None, Loss:67.0975, Loss-ae:67.0975, Loss-topo:109870.6741\n",
      "Epoch:718, P:None, Loss:66.2830, Loss-ae:66.2830, Loss-topo:105374.0603\n",
      "Epoch:719, P:None, Loss:68.2590, Loss-ae:68.2590, Loss-topo:112433.1975\n",
      "Epoch:720, P:None, Loss:67.1187, Loss-ae:67.1187, Loss-topo:104207.9286\n",
      "Epoch:721, P:None, Loss:69.7780, Loss-ae:69.7780, Loss-topo:116540.9989\n",
      "Epoch:722, P:None, Loss:67.2606, Loss-ae:67.2606, Loss-topo:104903.5915\n",
      "Epoch:723, P:None, Loss:68.3909, Loss-ae:68.3909, Loss-topo:111231.4754\n",
      "Epoch:724, P:None, Loss:69.7244, Loss-ae:69.7244, Loss-topo:117413.1362\n",
      "Epoch:725, P:None, Loss:65.9256, Loss-ae:65.9256, Loss-topo:112266.6172\n",
      "Epoch:726, P:None, Loss:68.7564, Loss-ae:68.7564, Loss-topo:103241.7701\n",
      "Epoch:727, P:None, Loss:65.6702, Loss-ae:65.6702, Loss-topo:105743.1953\n",
      "Epoch:728, P:None, Loss:67.4069, Loss-ae:67.4069, Loss-topo:107285.4152\n",
      "Epoch:729, P:None, Loss:67.4226, Loss-ae:67.4226, Loss-topo:108965.8315\n",
      "Epoch:730, P:None, Loss:67.6282, Loss-ae:67.6282, Loss-topo:108434.6931\n",
      "Epoch:731, P:None, Loss:64.8547, Loss-ae:64.8547, Loss-topo:108410.3705\n",
      "Epoch:732, P:None, Loss:66.6076, Loss-ae:66.6076, Loss-topo:106051.1775\n",
      "Epoch:733, P:None, Loss:68.2034, Loss-ae:68.2034, Loss-topo:122050.0368\n",
      "Epoch:734, P:None, Loss:67.9277, Loss-ae:67.9277, Loss-topo:110343.4621\n",
      "Epoch:735, P:None, Loss:66.5458, Loss-ae:66.5458, Loss-topo:108566.4208\n",
      "Epoch:736, P:None, Loss:66.6360, Loss-ae:66.6360, Loss-topo:105143.6004\n",
      "Epoch:737, P:None, Loss:70.0087, Loss-ae:70.0087, Loss-topo:114725.6763\n",
      "Epoch:738, P:None, Loss:66.9578, Loss-ae:66.9578, Loss-topo:108424.7812\n",
      "Epoch:739, P:None, Loss:70.0059, Loss-ae:70.0059, Loss-topo:112199.7400\n",
      "Epoch:740, P:None, Loss:65.0708, Loss-ae:65.0708, Loss-topo:104030.8917\n",
      "Epoch:741, P:None, Loss:68.3943, Loss-ae:68.3943, Loss-topo:103889.6708\n",
      "Epoch:742, P:None, Loss:68.6706, Loss-ae:68.6706, Loss-topo:107237.3203\n",
      "Epoch:743, P:None, Loss:66.8419, Loss-ae:66.8419, Loss-topo:94101.2556\n",
      "Epoch:744, P:None, Loss:68.5534, Loss-ae:68.5534, Loss-topo:107255.4777\n",
      "Epoch:745, P:None, Loss:68.6682, Loss-ae:68.6682, Loss-topo:99634.4955\n",
      "Epoch:746, P:None, Loss:68.0027, Loss-ae:68.0027, Loss-topo:114195.0100\n",
      "Epoch:747, P:None, Loss:69.4090, Loss-ae:69.4090, Loss-topo:110337.1462\n",
      "Epoch:748, P:None, Loss:66.9380, Loss-ae:66.9380, Loss-topo:96901.6217\n",
      "Epoch:749, P:None, Loss:68.5945, Loss-ae:68.5945, Loss-topo:110301.3359\n",
      "Epoch:750, P:None, Loss:66.9597, Loss-ae:66.9597, Loss-topo:108555.7433\n",
      "Epoch:751, P:None, Loss:68.0132, Loss-ae:68.0132, Loss-topo:109899.0000\n",
      "Epoch:752, P:None, Loss:66.4480, Loss-ae:66.4480, Loss-topo:104007.8404\n",
      "Epoch:753, P:None, Loss:67.0725, Loss-ae:67.0725, Loss-topo:105280.6607\n",
      "Epoch:754, P:None, Loss:67.2566, Loss-ae:67.2566, Loss-topo:108609.6306\n",
      "Epoch:755, P:None, Loss:67.8900, Loss-ae:67.8900, Loss-topo:112203.3415\n",
      "Epoch:756, P:None, Loss:66.8102, Loss-ae:66.8102, Loss-topo:103258.0502\n",
      "Epoch:757, P:None, Loss:67.0729, Loss-ae:67.0729, Loss-topo:111519.6016\n",
      "Epoch:758, P:None, Loss:67.4329, Loss-ae:67.4329, Loss-topo:100330.6272\n",
      "Epoch:759, P:None, Loss:67.4758, Loss-ae:67.4758, Loss-topo:108130.9096\n",
      "Epoch:760, P:None, Loss:66.0822, Loss-ae:66.0822, Loss-topo:97343.3036\n",
      "Epoch:761, P:None, Loss:66.9330, Loss-ae:66.9330, Loss-topo:111915.1775\n",
      "Epoch:762, P:None, Loss:67.9510, Loss-ae:67.9510, Loss-topo:103760.3549\n",
      "Epoch:763, P:None, Loss:67.9343, Loss-ae:67.9343, Loss-topo:107859.6507\n",
      "Epoch:764, P:None, Loss:67.3506, Loss-ae:67.3506, Loss-topo:103649.3114\n",
      "Epoch:765, P:None, Loss:68.0039, Loss-ae:68.0039, Loss-topo:102776.5089\n",
      "Epoch:766, P:None, Loss:70.0701, Loss-ae:70.0701, Loss-topo:114181.2623\n",
      "Epoch:767, P:None, Loss:65.2655, Loss-ae:65.2655, Loss-topo:103173.9743\n",
      "Epoch:768, P:None, Loss:67.3471, Loss-ae:67.3471, Loss-topo:104201.6562\n",
      "Epoch:769, P:None, Loss:65.8921, Loss-ae:65.8921, Loss-topo:102741.9565\n",
      "Epoch:770, P:None, Loss:71.0305, Loss-ae:71.0305, Loss-topo:108195.2388\n",
      "Epoch:771, P:None, Loss:67.2748, Loss-ae:67.2748, Loss-topo:109443.7109\n",
      "Epoch:772, P:None, Loss:65.7259, Loss-ae:65.7259, Loss-topo:107513.6864\n",
      "Epoch:773, P:None, Loss:66.0371, Loss-ae:66.0371, Loss-topo:110379.8025\n",
      "Epoch:774, P:None, Loss:65.7197, Loss-ae:65.7197, Loss-topo:101666.6328\n",
      "Epoch:775, P:None, Loss:66.4012, Loss-ae:66.4012, Loss-topo:97996.6920\n",
      "Epoch:776, P:None, Loss:67.3150, Loss-ae:67.3150, Loss-topo:111535.0000\n",
      "Epoch:777, P:None, Loss:68.0741, Loss-ae:68.0741, Loss-topo:110278.6094\n",
      "Epoch:778, P:None, Loss:68.4126, Loss-ae:68.4126, Loss-topo:112909.9576\n",
      "Epoch:779, P:None, Loss:67.4975, Loss-ae:67.4975, Loss-topo:107581.6049\n",
      "Epoch:780, P:None, Loss:64.8981, Loss-ae:64.8981, Loss-topo:103298.9866\n",
      "Epoch:781, P:None, Loss:68.5177, Loss-ae:68.5177, Loss-topo:115841.0614\n",
      "Epoch:782, P:None, Loss:65.1135, Loss-ae:65.1135, Loss-topo:105240.7701\n",
      "Epoch:783, P:None, Loss:67.2175, Loss-ae:67.2175, Loss-topo:107534.4621\n",
      "Epoch:784, P:None, Loss:68.1898, Loss-ae:68.1898, Loss-topo:101586.2768\n",
      "Epoch:785, P:None, Loss:66.9338, Loss-ae:66.9338, Loss-topo:101102.2645\n",
      "Epoch:786, P:None, Loss:66.4213, Loss-ae:66.4213, Loss-topo:102984.5480\n",
      "Epoch:787, P:None, Loss:68.6627, Loss-ae:68.6627, Loss-topo:102070.7221\n",
      "Epoch:788, P:None, Loss:71.1588, Loss-ae:71.1588, Loss-topo:108541.1473\n",
      "Epoch:789, P:None, Loss:68.8346, Loss-ae:68.8346, Loss-topo:98353.3694\n",
      "Epoch:790, P:None, Loss:68.7963, Loss-ae:68.7963, Loss-topo:101432.3672\n",
      "Epoch:791, P:None, Loss:66.3506, Loss-ae:66.3506, Loss-topo:102300.9107\n",
      "Epoch:792, P:None, Loss:69.6306, Loss-ae:69.6306, Loss-topo:101378.7740\n",
      "Epoch:793, P:None, Loss:64.7685, Loss-ae:64.7685, Loss-topo:100078.6551\n",
      "Epoch:794, P:None, Loss:68.7520, Loss-ae:68.7520, Loss-topo:109334.3661\n",
      "Epoch:795, P:None, Loss:67.1624, Loss-ae:67.1624, Loss-topo:106726.8326\n",
      "Epoch:796, P:None, Loss:68.8299, Loss-ae:68.8299, Loss-topo:110965.7801\n",
      "Epoch:797, P:None, Loss:65.2418, Loss-ae:65.2418, Loss-topo:95580.0067\n",
      "Epoch:798, P:None, Loss:69.3437, Loss-ae:69.3437, Loss-topo:106672.7098\n",
      "Epoch:799, P:None, Loss:67.2340, Loss-ae:67.2340, Loss-topo:104124.2679\n",
      "Epoch:800, P:None, Loss:72.7759, Loss-ae:72.7759, Loss-topo:107554.9185\n",
      "Epoch:801, P:None, Loss:67.8510, Loss-ae:67.8510, Loss-topo:99740.1451\n",
      "Epoch:802, P:None, Loss:69.6697, Loss-ae:69.6697, Loss-topo:110486.0804\n",
      "Epoch:803, P:None, Loss:67.6544, Loss-ae:67.6544, Loss-topo:112039.2645\n",
      "Epoch:804, P:None, Loss:66.9046, Loss-ae:66.9046, Loss-topo:101423.2132\n",
      "Epoch:805, P:None, Loss:68.8695, Loss-ae:68.8695, Loss-topo:109353.5346\n",
      "Epoch:806, P:None, Loss:65.3355, Loss-ae:65.3355, Loss-topo:102307.8259\n",
      "Epoch:807, P:None, Loss:67.2492, Loss-ae:67.2492, Loss-topo:102767.8259\n",
      "Epoch:808, P:None, Loss:69.4683, Loss-ae:69.4683, Loss-topo:108081.9855\n",
      "Epoch:809, P:None, Loss:67.1977, Loss-ae:67.1977, Loss-topo:103772.0145\n",
      "Epoch:810, P:None, Loss:67.6481, Loss-ae:67.6481, Loss-topo:104763.4888\n",
      "Epoch:811, P:None, Loss:67.4782, Loss-ae:67.4782, Loss-topo:102797.6127\n",
      "Epoch:812, P:None, Loss:70.9500, Loss-ae:70.9500, Loss-topo:114235.5011\n",
      "Epoch:813, P:None, Loss:69.2258, Loss-ae:69.2258, Loss-topo:112036.8002\n",
      "Epoch:814, P:None, Loss:65.7418, Loss-ae:65.7418, Loss-topo:93797.6908\n",
      "Epoch:815, P:None, Loss:66.9065, Loss-ae:66.9065, Loss-topo:102442.3806\n",
      "Epoch:816, P:None, Loss:68.2589, Loss-ae:68.2589, Loss-topo:107068.1518\n",
      "Epoch:817, P:None, Loss:66.9133, Loss-ae:66.9133, Loss-topo:96646.9275\n",
      "Epoch:818, P:None, Loss:66.1748, Loss-ae:66.1748, Loss-topo:103119.4632\n",
      "Epoch:819, P:None, Loss:67.7015, Loss-ae:67.7015, Loss-topo:102164.5893\n",
      "Epoch:820, P:None, Loss:68.1074, Loss-ae:68.1074, Loss-topo:105288.5134\n",
      "Epoch:821, P:None, Loss:67.0431, Loss-ae:67.0431, Loss-topo:108733.9609\n",
      "Epoch:822, P:None, Loss:67.2713, Loss-ae:67.2713, Loss-topo:99627.1641\n",
      "Epoch:823, P:None, Loss:68.5068, Loss-ae:68.5068, Loss-topo:106167.2098\n",
      "Epoch:824, P:None, Loss:68.1417, Loss-ae:68.1417, Loss-topo:96973.8460\n",
      "Epoch:825, P:None, Loss:66.2143, Loss-ae:66.2143, Loss-topo:97136.1462\n",
      "Epoch:826, P:None, Loss:66.7597, Loss-ae:66.7597, Loss-topo:100986.2712\n",
      "Epoch:827, P:None, Loss:66.7159, Loss-ae:66.7159, Loss-topo:100815.0982\n",
      "Epoch:828, P:None, Loss:69.9168, Loss-ae:69.9168, Loss-topo:109435.1920\n",
      "Epoch:829, P:None, Loss:68.7912, Loss-ae:68.7912, Loss-topo:101518.3901\n",
      "Epoch:830, P:None, Loss:68.3405, Loss-ae:68.3405, Loss-topo:100819.8438\n",
      "Epoch:831, P:None, Loss:68.9404, Loss-ae:68.9404, Loss-topo:108404.1350\n",
      "Epoch:832, P:None, Loss:67.6708, Loss-ae:67.6708, Loss-topo:110017.3415\n",
      "Epoch:833, P:None, Loss:66.1931, Loss-ae:66.1931, Loss-topo:97235.1674\n",
      "Epoch:834, P:None, Loss:67.3515, Loss-ae:67.3515, Loss-topo:96304.7500\n",
      "Epoch:835, P:None, Loss:69.5513, Loss-ae:69.5513, Loss-topo:102228.2723\n",
      "Epoch:836, P:None, Loss:66.1567, Loss-ae:66.1567, Loss-topo:99720.2455\n",
      "Epoch:837, P:None, Loss:66.2479, Loss-ae:66.2479, Loss-topo:92433.1696\n",
      "Epoch:838, P:None, Loss:67.5376, Loss-ae:67.5376, Loss-topo:105107.6741\n",
      "Epoch:839, P:None, Loss:65.8220, Loss-ae:65.8220, Loss-topo:98692.3884\n",
      "Epoch:840, P:None, Loss:66.9022, Loss-ae:66.9022, Loss-topo:101083.6127\n",
      "Epoch:841, P:None, Loss:68.0742, Loss-ae:68.0742, Loss-topo:100917.8348\n",
      "Epoch:842, P:None, Loss:65.7526, Loss-ae:65.7526, Loss-topo:93294.0234\n",
      "Epoch:843, P:None, Loss:68.5022, Loss-ae:68.5022, Loss-topo:111374.0246\n",
      "Epoch:844, P:None, Loss:68.7011, Loss-ae:68.7011, Loss-topo:109760.4442\n",
      "Epoch:845, P:None, Loss:69.0200, Loss-ae:69.0200, Loss-topo:109157.5212\n",
      "Epoch:846, P:None, Loss:67.2254, Loss-ae:67.2254, Loss-topo:104763.6496\n",
      "Epoch:847, P:None, Loss:66.0786, Loss-ae:66.0786, Loss-topo:98033.6317\n",
      "Epoch:848, P:None, Loss:66.9911, Loss-ae:66.9911, Loss-topo:106339.7087\n",
      "Epoch:849, P:None, Loss:67.9657, Loss-ae:67.9657, Loss-topo:99910.1853\n",
      "Epoch:850, P:None, Loss:67.3267, Loss-ae:67.3267, Loss-topo:95943.7522\n",
      "Epoch:851, P:None, Loss:66.3771, Loss-ae:66.3771, Loss-topo:106775.4643\n",
      "Epoch:852, P:None, Loss:64.7991, Loss-ae:64.7991, Loss-topo:97759.9431\n",
      "Epoch:853, P:None, Loss:66.1881, Loss-ae:66.1881, Loss-topo:94267.2723\n",
      "Epoch:854, P:None, Loss:66.5369, Loss-ae:66.5369, Loss-topo:107190.2009\n",
      "Epoch:855, P:None, Loss:68.0573, Loss-ae:68.0573, Loss-topo:92770.1205\n",
      "Epoch:856, P:None, Loss:68.3313, Loss-ae:68.3313, Loss-topo:100949.7913\n",
      "Epoch:857, P:None, Loss:67.4393, Loss-ae:67.4393, Loss-topo:98654.7935\n",
      "Epoch:858, P:None, Loss:68.8884, Loss-ae:68.8884, Loss-topo:109925.5837\n",
      "Epoch:859, P:None, Loss:66.9669, Loss-ae:66.9669, Loss-topo:90930.1719\n",
      "Epoch:860, P:None, Loss:67.1131, Loss-ae:67.1131, Loss-topo:92994.4386\n",
      "Epoch:861, P:None, Loss:67.8910, Loss-ae:67.8910, Loss-topo:101279.1306\n",
      "Epoch:862, P:None, Loss:67.7632, Loss-ae:67.7632, Loss-topo:96057.9777\n",
      "Epoch:863, P:None, Loss:68.2715, Loss-ae:68.2715, Loss-topo:101174.4074\n",
      "Epoch:864, P:None, Loss:67.0760, Loss-ae:67.0760, Loss-topo:103294.8181\n",
      "Epoch:865, P:None, Loss:67.6883, Loss-ae:67.6883, Loss-topo:106219.7243\n",
      "Epoch:866, P:None, Loss:69.6237, Loss-ae:69.6237, Loss-topo:107989.0558\n",
      "Epoch:867, P:None, Loss:67.0205, Loss-ae:67.0205, Loss-topo:102800.6518\n",
      "Epoch:868, P:None, Loss:68.2394, Loss-ae:68.2394, Loss-topo:104702.7344\n",
      "Epoch:869, P:None, Loss:67.1416, Loss-ae:67.1416, Loss-topo:107417.7690\n",
      "Epoch:870, P:None, Loss:66.7553, Loss-ae:66.7553, Loss-topo:93281.7121\n",
      "Epoch:871, P:None, Loss:67.8504, Loss-ae:67.8504, Loss-topo:94765.4699\n",
      "Epoch:872, P:None, Loss:66.9241, Loss-ae:66.9241, Loss-topo:99667.5926\n",
      "Epoch:873, P:None, Loss:67.5879, Loss-ae:67.5879, Loss-topo:103712.7489\n",
      "Epoch:874, P:None, Loss:66.7471, Loss-ae:66.7471, Loss-topo:100574.9554\n",
      "Epoch:875, P:None, Loss:67.2704, Loss-ae:67.2704, Loss-topo:91815.1473\n",
      "Epoch:876, P:None, Loss:69.2014, Loss-ae:69.2014, Loss-topo:100444.4621\n",
      "Epoch:877, P:None, Loss:70.9577, Loss-ae:70.9577, Loss-topo:107254.3125\n",
      "Epoch:878, P:None, Loss:65.7951, Loss-ae:65.7951, Loss-topo:100940.8069\n",
      "Epoch:879, P:None, Loss:68.4533, Loss-ae:68.4533, Loss-topo:96388.8739\n",
      "Epoch:880, P:None, Loss:66.6175, Loss-ae:66.6175, Loss-topo:95043.4196\n",
      "Epoch:881, P:None, Loss:67.4984, Loss-ae:67.4984, Loss-topo:98972.0335\n",
      "Epoch:882, P:None, Loss:66.3568, Loss-ae:66.3568, Loss-topo:100088.7656\n",
      "Epoch:883, P:None, Loss:66.9856, Loss-ae:66.9856, Loss-topo:91553.1585\n",
      "Epoch:884, P:None, Loss:67.1016, Loss-ae:67.1016, Loss-topo:92244.5011\n",
      "Epoch:885, P:None, Loss:67.7390, Loss-ae:67.7390, Loss-topo:96226.8248\n",
      "Epoch:886, P:None, Loss:69.9062, Loss-ae:69.9062, Loss-topo:103044.3382\n",
      "Epoch:887, P:None, Loss:65.8523, Loss-ae:65.8523, Loss-topo:92449.4643\n",
      "Epoch:888, P:None, Loss:68.7393, Loss-ae:68.7393, Loss-topo:109789.4096\n",
      "Epoch:889, P:None, Loss:65.7365, Loss-ae:65.7365, Loss-topo:101560.4632\n",
      "Epoch:890, P:None, Loss:67.2269, Loss-ae:67.2269, Loss-topo:95999.9888\n",
      "Epoch:891, P:None, Loss:65.9146, Loss-ae:65.9146, Loss-topo:94279.5502\n",
      "Epoch:892, P:None, Loss:67.6689, Loss-ae:67.6689, Loss-topo:104217.6507\n",
      "Epoch:893, P:None, Loss:67.9010, Loss-ae:67.9010, Loss-topo:102954.0792\n",
      "Epoch:894, P:None, Loss:69.7205, Loss-ae:69.7205, Loss-topo:103638.6417\n",
      "Epoch:895, P:None, Loss:68.1803, Loss-ae:68.1803, Loss-topo:101093.5379\n",
      "Epoch:896, P:None, Loss:66.1524, Loss-ae:66.1524, Loss-topo:96527.8248\n",
      "Epoch:897, P:None, Loss:67.0275, Loss-ae:67.0275, Loss-topo:107062.0647\n",
      "Epoch:898, P:None, Loss:67.4893, Loss-ae:67.4893, Loss-topo:99102.3895\n",
      "Epoch:899, P:None, Loss:68.7142, Loss-ae:68.7142, Loss-topo:102362.7333\n",
      "Epoch:900, P:None, Loss:67.1265, Loss-ae:67.1265, Loss-topo:103033.6786\n",
      "Epoch:901, P:None, Loss:66.5973, Loss-ae:66.5973, Loss-topo:95623.9297\n",
      "Epoch:902, P:None, Loss:68.3983, Loss-ae:68.3983, Loss-topo:92837.8627\n",
      "Epoch:903, P:None, Loss:67.0775, Loss-ae:67.0775, Loss-topo:101012.6629\n",
      "Epoch:904, P:None, Loss:67.8156, Loss-ae:67.8156, Loss-topo:100047.1719\n",
      "Epoch:905, P:None, Loss:65.7720, Loss-ae:65.7720, Loss-topo:94638.4799\n",
      "Epoch:906, P:None, Loss:69.4803, Loss-ae:69.4803, Loss-topo:102203.7779\n",
      "Epoch:907, P:None, Loss:65.5415, Loss-ae:65.5415, Loss-topo:91053.1395\n",
      "Epoch:908, P:None, Loss:66.4107, Loss-ae:66.4107, Loss-topo:94563.2020\n",
      "Epoch:909, P:None, Loss:67.8408, Loss-ae:67.8408, Loss-topo:96918.9397\n",
      "Epoch:910, P:None, Loss:65.9497, Loss-ae:65.9497, Loss-topo:101396.7757\n",
      "Epoch:911, P:None, Loss:66.0422, Loss-ae:66.0422, Loss-topo:92808.5446\n",
      "Epoch:912, P:None, Loss:67.0384, Loss-ae:67.0384, Loss-topo:94712.1406\n",
      "Epoch:913, P:None, Loss:67.9185, Loss-ae:67.9185, Loss-topo:97032.2288\n",
      "Epoch:914, P:None, Loss:66.4299, Loss-ae:66.4299, Loss-topo:92992.2757\n",
      "Epoch:915, P:None, Loss:69.2380, Loss-ae:69.2380, Loss-topo:105255.6618\n",
      "Epoch:916, P:None, Loss:66.1919, Loss-ae:66.1919, Loss-topo:94289.0491\n",
      "Epoch:917, P:None, Loss:68.7473, Loss-ae:68.7473, Loss-topo:102945.9442\n",
      "Epoch:918, P:None, Loss:69.4967, Loss-ae:69.4967, Loss-topo:97328.9286\n",
      "Epoch:919, P:None, Loss:67.8889, Loss-ae:67.8889, Loss-topo:100711.0826\n",
      "Epoch:920, P:None, Loss:68.3293, Loss-ae:68.3293, Loss-topo:96513.2020\n",
      "Epoch:921, P:None, Loss:67.3031, Loss-ae:67.3031, Loss-topo:93487.2902\n",
      "Epoch:922, P:None, Loss:65.5461, Loss-ae:65.5461, Loss-topo:93804.7188\n",
      "Epoch:923, P:None, Loss:65.4728, Loss-ae:65.4728, Loss-topo:93852.9565\n",
      "Epoch:924, P:None, Loss:68.3945, Loss-ae:68.3945, Loss-topo:97277.6150\n",
      "Epoch:925, P:None, Loss:67.6225, Loss-ae:67.6225, Loss-topo:89712.3136\n",
      "Epoch:926, P:None, Loss:66.2700, Loss-ae:66.2700, Loss-topo:94620.3069\n",
      "Epoch:927, P:None, Loss:68.6824, Loss-ae:68.6824, Loss-topo:105718.3359\n",
      "Epoch:928, P:None, Loss:69.2898, Loss-ae:69.2898, Loss-topo:93683.8248\n",
      "Epoch:929, P:None, Loss:69.8268, Loss-ae:69.8268, Loss-topo:102739.2020\n",
      "Epoch:930, P:None, Loss:65.0327, Loss-ae:65.0327, Loss-topo:97299.3192\n",
      "Epoch:931, P:None, Loss:66.3409, Loss-ae:66.3409, Loss-topo:96096.5045\n",
      "Epoch:932, P:None, Loss:67.9267, Loss-ae:67.9267, Loss-topo:97889.3705\n",
      "Epoch:933, P:None, Loss:65.9273, Loss-ae:65.9273, Loss-topo:92537.5212\n",
      "Epoch:934, P:None, Loss:68.1056, Loss-ae:68.1056, Loss-topo:100202.6607\n",
      "Epoch:935, P:None, Loss:70.3453, Loss-ae:70.3453, Loss-topo:98888.3147\n",
      "Epoch:936, P:None, Loss:66.9480, Loss-ae:66.9480, Loss-topo:101898.6607\n",
      "Epoch:937, P:None, Loss:67.1532, Loss-ae:67.1532, Loss-topo:95152.4275\n",
      "Epoch:938, P:None, Loss:68.3730, Loss-ae:68.3730, Loss-topo:97292.3962\n",
      "Epoch:939, P:None, Loss:69.0226, Loss-ae:69.0226, Loss-topo:95446.5011\n",
      "Epoch:940, P:None, Loss:67.0750, Loss-ae:67.0750, Loss-topo:88861.9922\n",
      "Epoch:941, P:None, Loss:68.0184, Loss-ae:68.0184, Loss-topo:96407.9420\n",
      "Epoch:942, P:None, Loss:68.6925, Loss-ae:68.6925, Loss-topo:98746.7065\n",
      "Epoch:943, P:None, Loss:68.1727, Loss-ae:68.1727, Loss-topo:93152.9286\n",
      "Epoch:944, P:None, Loss:68.9653, Loss-ae:68.9653, Loss-topo:93546.9978\n",
      "Epoch:945, P:None, Loss:66.0993, Loss-ae:66.0993, Loss-topo:95961.5513\n",
      "Epoch:946, P:None, Loss:69.0046, Loss-ae:69.0046, Loss-topo:101627.7109\n",
      "Epoch:947, P:None, Loss:66.5856, Loss-ae:66.5856, Loss-topo:90083.9230\n",
      "Epoch:948, P:None, Loss:65.4634, Loss-ae:65.4634, Loss-topo:92746.1194\n",
      "Epoch:949, P:None, Loss:68.1523, Loss-ae:68.1523, Loss-topo:99204.8114\n",
      "Epoch:950, P:None, Loss:69.3108, Loss-ae:69.3108, Loss-topo:107901.4799\n",
      "Epoch:951, P:None, Loss:65.3427, Loss-ae:65.3427, Loss-topo:84431.8806\n",
      "Epoch:952, P:None, Loss:65.6088, Loss-ae:65.6088, Loss-topo:87409.2489\n",
      "Epoch:953, P:None, Loss:66.9409, Loss-ae:66.9409, Loss-topo:95329.5268\n",
      "Epoch:954, P:None, Loss:67.5879, Loss-ae:67.5879, Loss-topo:92699.4492\n",
      "Epoch:955, P:None, Loss:68.9073, Loss-ae:68.9073, Loss-topo:93105.2165\n",
      "Epoch:956, P:None, Loss:64.8649, Loss-ae:64.8649, Loss-topo:89928.6328\n",
      "Epoch:957, P:None, Loss:67.4970, Loss-ae:67.4970, Loss-topo:93462.3538\n",
      "Epoch:958, P:None, Loss:67.7965, Loss-ae:67.7965, Loss-topo:100351.7087\n",
      "Epoch:959, P:None, Loss:65.9036, Loss-ae:65.9036, Loss-topo:90559.6306\n",
      "Epoch:960, P:None, Loss:68.2316, Loss-ae:68.2316, Loss-topo:94783.0871\n",
      "Epoch:961, P:None, Loss:66.9119, Loss-ae:66.9119, Loss-topo:93796.7266\n",
      "Epoch:962, P:None, Loss:70.5443, Loss-ae:70.5443, Loss-topo:99024.2656\n",
      "Epoch:963, P:None, Loss:68.3804, Loss-ae:68.3804, Loss-topo:89859.7455\n",
      "Epoch:964, P:None, Loss:64.7548, Loss-ae:64.7548, Loss-topo:95039.3092\n",
      "Epoch:965, P:None, Loss:66.5927, Loss-ae:66.5927, Loss-topo:97213.6127\n",
      "Epoch:966, P:None, Loss:67.2837, Loss-ae:67.2837, Loss-topo:88170.8259\n",
      "Epoch:967, P:None, Loss:67.4591, Loss-ae:67.4591, Loss-topo:96609.8002\n",
      "Epoch:968, P:None, Loss:65.3819, Loss-ae:65.3819, Loss-topo:96698.7902\n",
      "Epoch:969, P:None, Loss:65.8661, Loss-ae:65.8661, Loss-topo:93882.0792\n",
      "Epoch:970, P:None, Loss:67.7045, Loss-ae:67.7045, Loss-topo:99477.5290\n",
      "Epoch:971, P:None, Loss:67.6330, Loss-ae:67.6330, Loss-topo:93209.6083\n",
      "Epoch:972, P:None, Loss:69.8336, Loss-ae:69.8336, Loss-topo:96930.8449\n",
      "Epoch:973, P:None, Loss:66.5488, Loss-ae:66.5488, Loss-topo:90152.3862\n",
      "Epoch:974, P:None, Loss:66.2778, Loss-ae:66.2778, Loss-topo:88516.1172\n",
      "Epoch:975, P:None, Loss:67.3302, Loss-ae:67.3302, Loss-topo:96520.8906\n",
      "Epoch:976, P:None, Loss:66.9289, Loss-ae:66.9289, Loss-topo:95052.5658\n",
      "Epoch:977, P:None, Loss:65.7047, Loss-ae:65.7047, Loss-topo:91430.0156\n",
      "Epoch:978, P:None, Loss:66.9863, Loss-ae:66.9863, Loss-topo:94243.4107\n",
      "Epoch:979, P:None, Loss:66.9821, Loss-ae:66.9821, Loss-topo:94055.7511\n",
      "Epoch:980, P:None, Loss:67.2171, Loss-ae:67.2171, Loss-topo:95696.4821\n",
      "Epoch:981, P:None, Loss:68.3343, Loss-ae:68.3343, Loss-topo:95074.4520\n",
      "Epoch:982, P:None, Loss:66.7127, Loss-ae:66.7127, Loss-topo:90994.5647\n",
      "Epoch:983, P:None, Loss:67.5745, Loss-ae:67.5745, Loss-topo:88030.0190\n",
      "Epoch:984, P:None, Loss:66.3081, Loss-ae:66.3081, Loss-topo:96996.6473\n",
      "Epoch:985, P:None, Loss:66.2540, Loss-ae:66.2540, Loss-topo:86040.7299\n",
      "Epoch:986, P:None, Loss:65.4718, Loss-ae:65.4718, Loss-topo:92731.3884\n",
      "Epoch:987, P:None, Loss:65.5558, Loss-ae:65.5558, Loss-topo:94957.1172\n",
      "Epoch:988, P:None, Loss:67.4133, Loss-ae:67.4133, Loss-topo:98924.7645\n",
      "Epoch:989, P:None, Loss:67.3728, Loss-ae:67.3728, Loss-topo:95769.5525\n",
      "Epoch:990, P:None, Loss:66.8160, Loss-ae:66.8160, Loss-topo:89514.1350\n",
      "Epoch:991, P:None, Loss:66.1685, Loss-ae:66.1685, Loss-topo:93311.4163\n",
      "Epoch:992, P:None, Loss:70.0004, Loss-ae:70.0004, Loss-topo:103563.2958\n",
      "Epoch:993, P:None, Loss:67.1251, Loss-ae:67.1251, Loss-topo:93736.0279\n",
      "Epoch:994, P:None, Loss:67.4045, Loss-ae:67.4045, Loss-topo:87458.0234\n",
      "Epoch:995, P:None, Loss:67.2942, Loss-ae:67.2942, Loss-topo:92846.3940\n",
      "Epoch:996, P:None, Loss:66.9781, Loss-ae:66.9781, Loss-topo:98239.9208\n",
      "Epoch:997, P:None, Loss:67.0271, Loss-ae:67.0271, Loss-topo:96153.0993\n",
      "Epoch:998, P:None, Loss:67.5168, Loss-ae:67.5168, Loss-topo:94364.0279\n",
      "Epoch:999, P:None, Loss:68.2772, Loss-ae:68.2772, Loss-topo:92721.3973\n",
      "Epoch:1000, P:None, Loss:68.4117, Loss-ae:68.4117, Loss-topo:105252.3616\n",
      "Epoch:1001, P:None, Loss:66.3904, Loss-ae:66.3904, Loss-topo:93016.3069\n",
      "Epoch:1002, P:None, Loss:67.2838, Loss-ae:67.2838, Loss-topo:96472.2344\n",
      "Epoch:1003, P:None, Loss:65.8426, Loss-ae:65.8426, Loss-topo:88232.4408\n",
      "Epoch:1004, P:None, Loss:70.5765, Loss-ae:70.5765, Loss-topo:98402.4810\n",
      "Epoch:1005, P:None, Loss:67.4824, Loss-ae:67.4824, Loss-topo:91912.7098\n",
      "Epoch:1006, P:None, Loss:68.7537, Loss-ae:68.7537, Loss-topo:105222.1819\n",
      "Epoch:1007, P:None, Loss:66.9470, Loss-ae:66.9470, Loss-topo:95674.3175\n",
      "Epoch:1008, P:None, Loss:67.1701, Loss-ae:67.1701, Loss-topo:94056.7154\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m title_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMotionSense 20Hz\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTopoAE lambda \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_lam)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtopo_reducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_HD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle_plot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle_plot\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/Topological_ae/MotionSense20Hz/../../../librep/transforms/topo_ae.py:71\u001b[0m, in \u001b[0;36mTopologicalDimensionalityReduction.fit\u001b[0;34m(self, X, y, title_plot)\u001b[0m\n\u001b[1;32m     69\u001b[0m reshaped_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape)\n\u001b[1;32m     70\u001b[0m in_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(reshaped_data)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 71\u001b[0m loss, loss_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     73\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/Topological_ae/MotionSense20Hz/../../../librep/estimators/ae/torch/models/topological_ae/topological_ae.py:64\u001b[0m, in \u001b[0;36mTopologicallyRegularizedAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m         latent_distances \u001b[38;5;241m=\u001b[39m latent_distances \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_norm\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# Use reconstruction loss of autoencoder\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m         ae_loss, ae_loss_comp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#         print('TEST'*20)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#         print(self.topo_sig(x_distances, latent_distances))\u001b[39;00m\n\u001b[1;32m     68\u001b[0m         topo_error, topo_error_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopo_sig(\n\u001b[1;32m     69\u001b[0m             x_distances, latent_distances)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/Topological_ae/MotionSense20Hz/../../../librep/estimators/ae/torch/models/topological_ae/model_submodules.py:321\u001b[0m, in \u001b[0;36mConvolutionalAutoencoder_custom_dim3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m\"\"\"Apply autoencoder to batch of input images.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m \n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    320\u001b[0m latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[0;32m--> 321\u001b[0m x_reconst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# print('LATENT', latent.shape, 'XRECONST', x_reconst.shape)\u001b[39;00m\n\u001b[1;32m    323\u001b[0m reconst_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconst_error(x, x_reconst)\n",
      "File \u001b[0;32m~/librep-hiaac/experiments/Topological_ae/MotionSense20Hz/../../../librep/estimators/ae/torch/models/topological_ae/model_submodules.py:308\u001b[0m, in \u001b[0;36mConvolutionalAutoencoder_custom_dim3.decode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute reconstruction using convolutional autoencoder.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:801\u001b[0m, in \u001b[0;36mConvTranspose1d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    797\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    798\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    800\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98d66c-b630-493e-a3d0-6aee364a1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456c1f2-f856-44b5-b4c3-1c1b59781951",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52697960-6cb0-4f76-a033-21828d9effbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97126520-db06-4926-9f62-b8382a68d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d394d-adf1-4b47-90dc-47d402fc389b",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a246d2-1000-4a0b-a833-e13b05bfaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 1\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd10d4c-3f50-46a1-89e7-5d03513b1908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3455110-7d0f-4ae3-981b-dfc4d60b390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a5042-2a17-405a-9050-d4db3696d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc9431-db7f-4d90-842a-5973855211e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db0e416-dedc-42bf-ae22-3de332d7f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc62e6a4-6962-47e2-84f9-29fff5c662f3",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb29ec-76b3-4f98-bdd5-404e64da5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 5\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8782b60-204a-4e91-a8fd-a56ab3d50a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a924330-75ac-40f1-9756-b7f56c2df3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a64ca-2da3-41bb-b20c-3566ae56876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8dac9c-8a14-41a9-8974-27708e65a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f9fad-ac7a-4e0c-986b-9b4488e9f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9003d3-7f20-4458-bcf2-29e3e96eeb2d",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58053da5-5f27-4cc1-b960-a1091462f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 10\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90b324-05ae-46ea-871b-cd1bf602cf57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f32e46-bfa8-4fd9-97e4-becfe3c0c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2948fd-3ff5-4055-bd5d-1ea460ece54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f48f0c-8167-4f92-b5e3-41026aeca3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a7cfa-75b3-453e-b1d9-ded339fb22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b63bc06-eb93-4755-8ee3-6add8e40f7c1",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454ea1e-b392-4867-8f6d-8d04ffcee910",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 100\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122769a9-ea11-49de-8476-cd57135a5f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bb0dd-d189-4e9b-83de-2d9371e450cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a42d610-e5ac-4c50-97cd-830abea35003",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae177f50-7f6e-4af4-85c4-94684b3557aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4c244-97bd-4d17-b263-88ec61dcfef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52efb733-3f7f-47a1-9520-2b1f7f249ebb",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288716cc-11e3-430b-84d8-b0171c9381e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 0.001\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d4038-557d-47b5-af9e-e699b5375d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eac8b7-edf5-43a0-b165-c2a3bac3fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3af3aa-cf3e-487a-a4cb-7372e2fa878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76956d30-c84a-4709-8fa7-5d40b5ea3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a10091-8295-484a-b3f6-46553c9cc6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e376a06-d032-4217-83a6-b471479895d2",
   "metadata": {},
   "source": [
    "# Reducing with Topological Autoencoders (L=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802da098-9f7d-43bc-b38a-f593ce163bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lam = 0.0001\n",
    "topo_reducer = CustomTopoDimRedTransform(\n",
    "    model_name='ConvolutionalAutoencoder_custom_dim3',\n",
    "    model_lambda=model_lam,\n",
    "    patience=None,\n",
    "    num_epochs=model_epc,\n",
    "    from_dim=180,\n",
    "    to_dim=model_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf236a2-d382-41f2-9b2d-ff2a7f2ece1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_plot = \"MotionSense 20Hz\\nTopoAE lambda {}\".format(model_lam)\n",
    "topo_reducer.fit(train_HD, train_Y, title_plot=title_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423f23b-d40a-4e92-bcea-38a5c7614781",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = topo_reducer.transform(train_HD)\n",
    "test_LD = topo_reducer.transform(test_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7680b3bf-8516-491f-8507-664ef97be97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LD = np.reshape(topo_reducer.transform(train_HD), (-1, model_dim))\n",
    "print('TRAIN LD RESHAPED', train_LD.shape)\n",
    "test_LD = np.reshape(topo_reducer.transform(test_HD), (-1, model_dim))\n",
    "print('TEST LD RESHAPED', test_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3498b7-6cb3-4323-a28f-7a32e3c71e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_result = run_experiments(train_HD, train_LD, train_Y, test_HD, test_LD, test_Y)\n",
    "experiments_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc4944-7ea5-4b73-9f88-8f4cdcf899b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "tsne = TSNE()\n",
    "test_2D = tsne.fit_transform(test_LD)\n",
    "visualize(test_2D, test_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
